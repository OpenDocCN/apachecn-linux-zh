User-Kernel Communication Pathways

考虑一下这个场景:你已经成功地为压力传感器设备开发了一个设备驱动程序(也许是通过使用内核的 I2C API 通过 I2C 协议从芯片获取压力)。所以，在驱动程序的变量中有当前的压力值，这当然意味着它在内核内存空间中。当前的问题是，您现在如何让*用户空间应用程序检索该值？*好吧，就像我们在上一章学到的，你总是可以在驾驶员的 *fops* 结构中包含一个`.read`方法。当用户空间应用发出`read(2)`系统调用时，控制权将(通过**虚拟文件系统** ( **VFS** )转移到您的驱动程序的*读取方法。*在那里，你执行`copy_to_user()`(或等效)，导致用户模式应用程序接收该值。然而，还有其他的，有时是更好的方法来做到这一点。

在本章中，您将了解各种可用的通信接口或路径，作为用户和内核地址空间之间通信或接口的一种方式。这是编写驱动程序代码的一个重要方面，因为如果没有这些知识，你将如何实现一件关键的事情——在内核空间组件(通常，这是一个设备驱动程序，但它可能是任何东西)和用户空间进程或线程之间高效地传输信息？不仅如此，我们将要学习的一些技术也经常用于调试(和/或诊断)目的。在本章中，我们将介绍实现内核和用户(虚拟)地址空间之间的通信的几种技术:通过传统的 proc 文件系统、 *procfs* ，驱动程序通过 sys 文件系统、 *sysfs* 、通过调试文件系统、 *debugfs* 、通过 *netlink sockets* 以及通过`ioctl(2)`系统调用进行通信。

本章将涵盖以下主题:

*   内核驱动程序与用户空间应用程序通信/接口的方法
*   通过 proc 文件系统(procfs)接口
*   通过系统文件系统(sysfs)接口
*   通过调试文件系统(debugfs)接口
*   通过网络连接插座接口
*   通过 ioctl 系统调用接口
*   比较接口方法-表格

我们开始吧！

# 技术要求

我假设您已经浏览了*前言*，相关部分是*为了从本书中获得最大收益，*并且已经适当地准备了一个运行 Ubuntu 18.04 LTS(或更高的稳定版本)的来宾**虚拟机** ( **VM** )并且安装了所有需要的软件包。如果没有，我建议你先做这个。

为了从这本书中获得最大的收益，我强烈建议您首先设置工作区环境，包括克隆这本书的 GitHub 存储库([https://GitHub . com/PacktPublishing/Linux-Kernel-Programming-Part-2](https://github.com/PacktPublishing/Linux-Kernel-Programming-Part-2/tree/main/ch2))获取相关代码，并以动手的方式进行操作。

# 内核驱动程序与用户空间应用程序通信/接口的方法

正如我们在介绍中提到的，在这一章中，我们希望学习如何在内核空间组件(通常，这是一个设备驱动程序，但实际上它可以是任何东西)和用户空间进程或线程之间高效地传输信息。首先，让我们简单地列举内核或驱动程序作者可以用来与用户空间 C 应用程序通信或交互的各种技术。嗯，用户空间组件可以是一个 C 应用程序、一个 shell 脚本(这两者我们通常都会在本书中展示)，甚至是其他应用程序，比如 C++/Java 应用程序、Python/Perl 脚本等等。

正如我们在配套指南 *Linux 内核编程*中看到的，在*第 4 章【编写您的第一个内核模块–LKMs 第 1 部分】*中的*库和系统调用 API*小节中，用户空间应用程序和包含设备驱动程序的内核之间的基本接口是系统调用 API*。*现在，在上一章中，您学习了为 Linux 编写字符设备驱动程序的基础知识。其中，您还学习了如何通过让用户模式应用程序打开设备文件并发出`read(2)`和`write(2)`系统调用来在用户和内核地址空间之间传输数据。这导致 VFS 调用驱动程序的读/写方法，您的驱动程序通过`copy_{from|to}_user()`API 执行数据传输。所以，这里的问题是:如果我们已经讨论过了，那么在这方面还有什么需要了解的呢？

啊，相当多！现实情况是，用户模式应用程序和内核之间还有其他几种接口技术。当然，它们都非常依赖于使用系统调用；毕竟，没有其他(同步的、编程的)方法可以从用户空间进入内核！然而，技术不同。本章的目的是向您展示各种可用的通信接口，当然，根据项目的不同，一个可能比其他更适合使用。让我们来看看本章中用于用户和内核地址空间之间接口的各种技术:

*   通过传统的 procfs 接口
*   通过 sysfs
*   Via debugfs
*   通过网络连接插座
*   通过`ioctl(2)`系统调用

在本章中，我们将通过提供驱动程序代码示例来详细讨论这些接口技术。此外，我们还将简要探讨它们如何有利于*调试的目的。*那么，让我们从使用 procfs 接口开始。

# 通过 proc 文件系统(procfs)接口

在本节中，我们将介绍什么是 proc 文件系统，以及如何利用它作为用户和内核地址空间之间的接口。proc 文件系统是一个功能强大且易于编程的界面，通常用于状态报告和调试核心内核系统。

Note that from version 2.6 Linux onward and for upstream contribution, this interface is *not* to be used by driver authors (it's strictly meant for kernel-internal usage only). Nevertheless, for completeness, we will cover it here.

## 理解 proc 文件系统

Linux 有一个名为 *proc* 的虚拟文件系统；它的默认挂载点是`/proc`。关于 proc 文件系统首先要意识到的是，它的内容是在非易失性磁盘上的*而不是*。它的内容在内存中，因此是不稳定的。在`/proc`下可以看到的文件和目录都是内核代码为 proc 设置的伪文件；内核通过(几乎)总是将文件的*大小*显示为零来暗示这个事实:

```sh
$ mount | grep -w proc
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
$ ls -l /proc/
total 0
dr-xr-xr-x  8 root  root          0 Jan 27 11:13 1/
dr-xr-xr-x  8 root  root          0 Jan 29 08:22 10/
dr-xr-xr-x  8 root  root          0 Jan 29 08:22 11/
dr-xr-xr-x  8 root  root          0 Jan 29 08:22 11550/
[...]
-r--r--r--  1 root  root          0 Jan 29 08:22 consoles
-r--r--r--  1 root  root          0 Jan 29 08:19 cpuinfo
-r--r--r--  1 root  root          0 Jan 29 08:22 crypto
-r--r--r--  1 root  root          0 Jan 29 08:20 devices
-r--r--r--  1 root  root          0 Jan 29 08:22 diskstats
[...]
-r--r--r--  1 root  root          0 Jan 29 08:22 vmstat
-r--r--r--  1 root  root          0 Jan 29 08:22 zoneinfo
$ 
```

让我们总结一下关于 Linux 强大的 proc 文件系统的几个关键点。

The objects under `/proc` (files, directories, soft links, and so on) are all pseudo objects; they live in RAM!

### /proc 下的目录

`/proc`下名称为整数值的目录代表系统上当前活动的进程。目录的名称是进程的 PID(技术上，它是进程的 TGID。我们在配套指南*第 6 章*、*内核和内存管理内部要素*中介绍了 TGID/PID。

此文件夹–`/proc/PID/`–包含有关此过程的信息。因此，例如，对于 *init* 或 *systemd* 进程(总是 PID `1`)，您可以在`/proc/1/`文件夹下检查关于该进程的详细信息(其属性、打开的文件、内存布局、子进程等)。

举个例子，在这里，我们将获得一个根壳，并执行`ls /proc/1`:

![](assets/b602d4a8-8d7b-4aca-ad53-c4f04ef4240d.png)

Figure 2.1 – Screenshot of performing ls /proc/1 on an x86_64 guest system

关于`/proc/<PID>/...`下的伪文件和文件夹的完整细节可以在`proc(5)`的手册页上找到(通过做`man 5 proc`)；一定要试一试，参考一下！

Note that the precise content under `/proc` varies from both the kernel version and the (CPU) architecture; x86_64 tends to have the richest content.

### proc 文件系统背后的目的

proc 文件系统背后的*目的*是双重的:

*   第一，对于开发人员、系统管理员和任何真正深入内核的人来说，这是一个简单的界面，这样他们就可以获得关于进程内部、内核甚至硬件的信息。使用这个界面只需要知道基本的 shell 命令，如`cd`、`cat`、`echo`、`ls`、等。
*   第二，作为*根*用户，有时也是所有者，您可以在`/proc/sys`下写入某些伪文件，从而调整各种内核参数。这个功能叫做**系统** *。*举例来说，可以在`/proc/sys/net/ipv4/`中调优各种 IPv4 组网参数。都记录在这里:[https://www . kernel . org/doc/Documentation/networking/IP-sysctl . txt](https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt)。

更改基于 proc 的可调参数的值很容易；例如，让我们更改框中任意给定时间点允许的最大线程数。以*根*的身份运行以下命令:

```sh
# cat /proc/sys/kernel/threads-max
15741
# echo 10000 > /proc/sys/kernel/threads-max
# cat /proc/sys/kernel/threads-max
10000
#
```

说完了，我们就完了。但是，应该清楚的是，前面的操作是*不稳定的*-该更改仅适用于本次会话；当然，电源循环或重新启动会导致它恢复到默认值。那么，我们如何让改变永久化*？*简短回答:使用`sysctl(8)`效用；有关更多详细信息，请参考其手册页。

你现在准备好写一些 procfs 接口代码了吗？不要太快——下一部分会告诉你为什么这可能不是一个好主意。

### 驱动程序作者不得使用 procfs

尽管我们可以使用 proc 文件系统与用户模式应用程序进行交互，但这里有一点需要注意！您必须意识到，procfs 就像内核中的许多类似设施一样，是一个**应用程序二进制接口** ( **ABI** )。内核社区并没有承诺保持稳定和现在的样子，就像内核*API*和它们的内部数据结构一样。事实上，自从 2.6 内核以来，内核们已经非常清楚地表明了这一点——设备驱动程序作者(等等)不应该为了他们自己的目的或他们的接口、调试或其他目的而使用 procfs 。早些时候，对于 2.6 Linux，使用 proc 来实现上述目的是很常见的(根据内核社区，这是滥用的，因为 proc 只用于内核内部使用！).

因此，如果作为驱动程序作者的我们认为 procfs 是越界的，或者是被弃用的，那么我们使用什么工具来与用户空间进程进行通信呢？驱动程序作者将使用 sysfs 工具*导出*他们的接口。实际上，不仅仅是 sysfs 有几种选择可供您选择，例如 sysfs、debugfs、netlink 套接字和 ioctl 系统调用。我们将在本章后面详细介绍这些内容。

不过，坚持住。同样，现实是，关于驱动程序作者不使用 procfs 的“规则”是针对社区的。这意味着，如果您打算将您的驱动程序或内核模块*上游*到主线内核，从而在 GPLv2 许可证下贡献您的代码，*那么*所有的社区规则肯定适用。如果没有，那就真的由你来决定了。当然，遵循内核社区的准则和规则只能是一件好事；我们绝对建议您这样做。在阻止驱动程序等非核心资源使用 proc 方面，不幸的是，proc API/ABI 没有可用的最新内核文档。

On the 5.4.0 kernel, there are around 70-odd callers of the `proc_create()` kernel API, several of which being (typically older) drivers and filesystems.

尽管如此(你已经被警告了！)，让我们学习如何通过 procfs 使用户空间进程与内核代码交互。

## 使用 procfs 与用户空间交互

作为内核模块或设备驱动程序开发人员，我们实际上可以在`/proc`下创建我们自己的条目，利用它作为用户空间的简单界面。我们如何做到这一点？内核提供了在 procfs 下创建目录和文件的 API。我们将在本节中学习如何使用它们。

### 基本 procfs APIs

在这里，我们不打算深究 procfs API 集的血淋淋的细节；相反，我们将覆盖刚刚足够让你能够理解和使用它们。关于更深层次的细节，请参考终极资源:内核代码库。我们将在这里介绍的例程已经导出，因此像您这样的驱动程序作者可以使用它们。此外，正如我们前面提到的，所有的 procfs 文件对象都是真正的伪对象，也就是说它们只存在于内存中。

Here, we are assuming you understand how to design and implement a simple LKM; you'll find more details in the companion guide to this book, *Linux Kernel Programming*, in the fourth and fifth chapters.

让我们从探索一些简单的 procfs APIs 开始，这些 API 允许您执行一些关键任务——分别在 proc 文件系统下创建一个目录，在那里创建(伪)文件，以及删除它们。对于所有这些任务，请确保包含相关的头文件；也就是`#include <linux/proc_fs.h>`:

1.  在`/proc`下创建一个名为`name`的目录:

```sh
struct proc_dir_entry *proc_mkdir(const char *name,
                         struct proc_dir_entry *parent);
```

第一个参数是目录的名称，而第二个参数是指向在其中创建目录的父目录的指针。这里通过`NULL`创建根目录下的目录；也就是`/proc`下的*。*保存返回值，因为您通常会在后续的 API 中将其用作参数。

The `proc_mkdir_data()` routine allows you to pass along a data item (a `void *`) as well; note that it's exported via `EXPORT_SYMBOL_GPL`.

2.  创建一个名为`/proc/parent/name`的 procfs(伪)文件:

```sh
struct proc_dir_entry *proc_create(const char *name, umode_t mode,
                         struct proc_dir_entry *parent,
                         const struct file_operations *proc_fops);
```

这里的关键参数是`struct file_operations`，我们在上一章已经介绍过了。您需要用要实现的“方法”来填充它(下面将详细介绍)。想想看:这真的是很厉害的东西；使用`fops` 结构，您可以在您的驱动程序(或内核模块)中设置内核的 proc 文件系统层将遵守的“回调”函数:当用户空间进程读取您的 proc 文件时，它(VFS)将调用驱动程序的`.read`方法或回调函数。如果用户空间 app 写了，会调用驱动的`.write`回调！

3.  删除 procfs 条目:

```sh
void remove_proc_entry(const char *name, struct proc_dir_entry *parent)
```

此 API 移除指定的`/proc/name`条目并释放它(如果未使用)；类似地(通常也更方便)，使用`remove_proc_subtree()`应用编程接口移除`/proc`内的整个子树(通常是在清理时或发生错误时)。

既然我们知道了基础，经验方法要求我们将这些 API 付诸实践！为此，让我们弄清楚在`/proc`下要创建哪些目录/文件。

### 我们将创建的四个 procfs 文件

为了帮助清楚地说明 procfs 作为接口技术的用法，我们将让内核模块在`/proc`下创建一个目录。在该目录中，它将创建四个 procfs(伪)文件。请注意，默认情况下，所有 procfs 文件的*所有者:组*属性为*根:根*。现在，创建一个名为`/proc/proc_simple_intf`的目录，并在它下面创建四个(伪)文件。`/proc/proc_simple_intf`目录下四个 procfs(伪)文件的名称和属性如下表所示:

| **proc fs‘文件’的名称** | **R:读取回调的动作，通过用户空间读取**调用 | **W:写回调操作，通过用户空间写**调用 | **Procfs‘文件’权限** |
| `llkdproc_dbg_level` | 检索(到用户空间)全局变量的当前值；也就是
`debug_level` | 将`debug_level`全局变量更新为用户空间写入的值 | `0644` |
| `llkdproc_show_pgoff` | 检索(到用户空间)内核的`PAGE_OFFSET`值 | –无写回调– | `0444` |
| `llkdproc_show_drvctx` | 检索(到用户空间)驱动程序“上下文”结构中的当前值；也就是`drv_ctx` | –无写回调– | `0440` |
| `llkdproc_config1`(也作`dbg_level`) | 检索(到用户空间)上下文变量的当前值；也就是
`drvctx->config1` | 将驱动程序上下文成员`drvctx->config1`更新为用户空间写入的值 | `0644` |

我们将查看在`/proc`下创建`proc_simple_intf`目录的 API 和实际代码，以及前面提到的四个文件。(由于篇幅不够，我们不会实际展示所有代码；只是关于“调试级”获取和设置的代码；这不是问题，代码的其余部分在概念上非常相似)。

### 尝试动态调试级 procfs 控件

首先，让我们检查一下本章将使用的“驱动程序上下文”数据结构(事实上，我们在上一章中首次使用了它):

```sh
// ch2/procfs_simple_intf/procfs_simple_intf.c
[ ... ]
/* Borrowed from ch1; the 'driver context' data structure;
 * all relevant 'state info' reg the driver and (fictional) 'device'
 * is maintained here.
 */
struct drv_ctx {
    int tx, rx, err, myword, power;
    u32 config1; /* treated as equivalent to 'debug level' of our driver */
    u32 config2;
    u64 config3;
#define MAXBYTES   128
    char oursecret[MAXBYTES];
};
static struct drv_ctx *gdrvctx;
static int debug_level; /* 'off' (0) by default ... */
```

在这里，我们还可以看到，我们有一个全局整数名为`debug_level`；这将提供对“项目”调试详细程度的动态控制。调试级别被分配了一个范围`[0-2]`，这里我们有以下内容:

*   `0`表示*没有调试消息*(默认)。
*   `1`是*中等调试*的详细程度。
*   `2`暗示*高调试*冗长。

整个模式的美妙之处——实际上也是这里的关键之处——在于我们将能够通过我们创建的 procfs 界面从用户空间查询和设置这个`debug_level`变量！这将允许最终用户(出于安全原因，需要*根*访问)在运行时动态改变调试级别(这是许多产品中常见的功能)。

在深入研究代码级细节之前，让我们先试一试，这样我们就知道会发生什么:

1.  在这里，使用我们的`lkm`便利包装脚本，我们必须构建和`insmod(8)`内核模块(本书源代码树中的`ch2/proc_simple_intf`):

```sh
$ cd <booksrc>/ch2/proc_simple_intf
$ ../../lkm procfs_simple_intf          *<-- builds the kernel module*
Version info:
[...]
[24826.234323] procfs_simple_intf:procfs_simple_intf_init():321: proc dir (/proc/procfs_simple_intf) created
[24826.240592] procfs_simple_intf:procfs_simple_intf_init():333: proc file 1 (/proc/procfs_simple_intf/llkdproc_debug_level) created
[24826.245072] procfs_simple_intf:procfs_simple_intf_init():348: proc file 2 (/proc/procfs_simple_intf/llkdproc_show_pgoff) created
[24826.248628] procfs_simple_intf:alloc_init_drvctx():218: allocated and init the driver context structure
[24826.251784] procfs_simple_intf:procfs_simple_intf_init():368: proc file 3 (/proc/procfs_simple_intf/llkdproc_show_drvctx) created
[24826.255145] procfs_simple_intf:procfs_simple_intf_init():378: proc file 4 (/proc/procfs_simple_intf/llkdproc_config1) created
[24826.259203] procfs_simple_intf initialized
$ 
```

在这里，我们构建并插入了内核模块；`dmesg(1)`显示内核 *printks* ，显示我们创建的 procfs 文件之一是属于动态调试工具的文件(此处用粗体突出显示；由于这些是伪文件，文件大小将显示为`0`字节。

2.  现在我们通过查询`debug_level`的当前值来测试一下:

```sh
$ cat /proc/procfs_simple_intf/llkdproc_debug_level
debug_level:0
$
```

3.  太好了，就像预期的那样，它是零(默认值)。现在，让我们将调试级别更改为`2`:

```sh
$ sudo sh -c "echo 2 > /proc/procfs_simple_intf/llkdproc_debug_level"
$ cat /proc/procfs_simple_intf/llkdproc_debug_level
debug_level:2
$
```

请注意我们是如何发布`echo`作为*根*的。我们可以看到，调试级别确实变了(变成了`2`的值)！试图设置超出范围的值也会被捕获(并且`debug_level`变量的值被重置为其最后一个有效值)，如下所示:

```sh
$ sudo sh -c "echo 5 > /proc/procfs_simple_intf/llkdproc_debug_level"
sh: echo: I/O error
$ dmesg
[...]
[ 6756.415727] procfs_simple_intf: trying to set invalid value for debug_level [allowed range: 0-2]; resetting to previous (2)
```

右；果然奏效。然而，问题是，所有这些是如何在代码级别工作的？请继续阅读了解详情！

### 通过 procfs 动态控制调试级别

让我们来回答前面提到的问题–*它是如何用代码完成的？*挺直白的，真的:

1.  首先，在内核模块的`init`代码中，我们必须创建我们的 procfs 目录，用我们内核模块的名称命名它:

```sh
static struct proc_dir_entry *gprocdir;
[...]
gprocdir = proc_mkdir(OURMODNAME, NULL);
```

2.  同样，在内核模块的`init`代码中，我们必须创建控制项目“调试级别”的`procfs`文件:

```sh
// ch2/procfs_simple_intf/procfs_simple_intf.c[...]
#define PROC_FILE1           "llkdproc_debug_level"
#define PROC_FILE1_PERMS     0644
[...]
static int __init procfs_simple_intf_init(void)
{
    int stat = 0;
    [...]
    /* 1\. Create the PROC_FILE1 proc entry under the parent dir OURMODNAME;
     * this will serve as the 'dynamically view/modify debug_level'
     * (pseudo) file */
    if (!proc_create(PROC_FILE1, PROC_FILE1_PERMS, gprocdir,
 &fops_rdwr_dbg_level)) {
    [...]
    pr_debug("proc file 1 (/proc/%s/%s) created\n", OURMODNAME, PROC_FILE1);
    [...]
```

这里，我们使用`proc_create()` API 创建 *procfs* 文件，并将其“链接”到所提供的`file_operations`结构。

3.  fops 结构(技术上，`struct file_operations`)是这里的关键数据结构。正如我们在[第 1 章](1.html)、*编写一个简单的杂项字符设备驱动程序*中所学的那样，我们在这里将*功能*分配给设备上的各种文件操作，或者，在本例中，procfs 文件。下面是初始化 fop 的代码:

```sh
static const struct file_operations fops_rdwr_dbg_level = {
    .owner = THIS_MODULE,
    .open = myproc_open_dbg_level,
    .read = seq_read,
    .write = myproc_write_debug_level,
    .llseek = seq_lseek,
    .release = single_release,
};
```

4.  fops 的`open`方法指向一个我们必须定义的函数:

```sh
static int myproc_open_dbg_level(struct inode *inode, struct file *file)
{
    return single_open(file, proc_show_debug_level, NULL);
}
```

使用内核的`single_open()`应用编程接口，我们记录了这样一个事实:每当读取这个文件时——最终通过来自用户空间的`read(2)`系统调用来完成 proc 文件系统将“回调”我们的`proc_show_debug_level()`例程(第二个参数为`single_open()`)。

We won't bother with the internal implementation of the `single_open()` API here; if you're curious, you can always look it up here: `fs/seq_file.c:single_open()`.

因此，概括地说，要向 procfs 注册“read”方法，我们需要执行以下操作:

*   将`fops.open`指针初始化为`foo()`功能。
*   在`foo()`函数中，调用`single_open()`，提供读取回调函数作为第二个参数。

There's some history here; without getting too deep into it, suffice it to say that the older working of procfs had issues. Notably, you couldn't transfer more than a single page of data (with read or write) without manually iterating over the content. The *sequence iterator* functionality that was introduced with 2.6.12 fixed these issues. Nowadays, using `single_open()` and its ilk (the `seq_read`, `seq_lseek`, and `seq_release` built-in kernel functions) is the simpler and correct approach to using procfs.

5.  那么，当用户空间*将*(通过`write(2)`系统调用)写入 proc 文件时会发生什么呢？简单:在前面的代码中，您可以看到我们已经将`fops_rdwr_dbg_level.write`方法注册为`myproc_write_debug_level()`函数，这意味着每当这个(伪)文件被写入时，这个函数都会被*回调*(在*第 6 步*中解释过，在*读取*回调之后)。

我们通过`single_open`注册的*读取*回调函数的代码如下:

```sh
/* Our proc file 1: displays the current value of debug_level */
static int proc_show_debug_level(struct seq_file *seq, void *v)
{
    if (mutex_lock_interruptible(&mtx))
        return -ERESTARTSYS;
    seq_printf(seq, "debug_level:%d\n", debug_level);
    mutex_unlock(&mtx);
    return 0;
}
```

`seq_printf()`在概念上类似于大家熟悉的`sprintf()` API。它会将提供给它的数据正确打印到`seq_file`对象上。当我们在这里说“打印”时，我们真正的意思是，它有效地将数据缓冲区传递给发出读取系统调用的用户空间进程或线程，该调用首先将我们带到这里，实际上*将数据传输到用户空间。*

Oh yes, what's with the `mutex_{un}lock*()` APIs? They are for something critical – *locking.* We will provide a detailed discussion on locking in [Chapter 6](6.html), *Kernel Synchronization – Part 1*, and [Chapter 7](7.html), *Kernel Synchronization – Part 2*; for now, just understand that these are required synchronization primitives.

6.  我们通过`fops_rdwr_dbg_level.write`注册的`write`回拨功能如下:

```sh
#define DEBUG_LEVEL_MIN     0
#define DEBUG_LEVEL_MAX     2
[...]
/* proc file 1 : modify the driver's debug_level global variable as per what user space writes */
static ssize_t myproc_write_debug_level(struct file *filp, 
                const char __user *ubuf, size_t count, loff_t *off)
{
   char buf[12];
   int ret = count, prev_dbglevel;
   [...]
   prev_dbglevel = debug_level;
 *// < ... validity checks (not shown here) ... >*
   /* Get the user mode buffer content into the kernel (into 'buf') */
   if (copy_from_user(buf, ubuf, count)) {
        ret = -EFAULT;
        goto out;
   }
   [...]
   ret = kstrtoint(buf, 0, &debug_level); /* update it! */
   if (ret)
        goto out;
  if (debug_level < DEBUG_LEVEL_MIN || debug_level > DEBUG_LEVEL_MAX) {
            [...]
            debug_level = prev_dbglevel;
            ret = -EFAULT; goto out;
   }
   /* just for fun, let's say that our drv ctx 'config1'
      represents the debug level */
   gdrvctx->config1 = debug_level;
   ret = count;
out:
   mutex_unlock(&mtx);
   return ret;
}
```

在我们的 write 方法的实现中(注意它在结构上与字符设备驱动程序的 write 方法有多相似)，我们执行了一些有效性检查，然后通过通常的`copy_from_user()`函数复制了用户空间进程写给我们的数据(回想一下我们是如何使用`echo`命令写入 procfs 文件的)。然后，我们使用内核内置的`kstrtoint()`应用编程接口(类似的还有几个)将字符串缓冲区转换为整数，并将结果存储在我们的全局变量中；也就是`debug_level`！我们再次验证它，如果一切正常，我们还将驱动程序上下文的`config1`成员设置为相同的值，然后返回一条成功消息(仅作为示例)。

7.  内核模块的其余代码非常相似——我们为其余三个 procfs 文件设置了功能。我让你来详细浏览代码并尝试一下。
8.  再来一个快速演示:让我们将`debug_level`设置为`1`，然后转储驱动程序上下文结构(通过我们创建的第三个 procfs 文件):

```sh
$ cat /proc/procfs_simple_intf/llkdproc_debug_level
debug_level:0
$ sudo sh -c "echo 1 > /proc/procfs_simple_intf/llkdproc_debug_level"
```

9.  好了，`debug_level`变量现在将有一个值`1`；现在，让我们转储驱动程序上下文结构:

```sh
$ cat /proc/procfs_simple_intf/llkdproc_show_drvctx 
cat: /proc/procfs_simple_intf/llkdproc_show_drvctx: Permission denied
$ sudo cat /proc/procfs_simple_intf/llkdproc_show_drvctx 
prodname:procfs_simple_intf
tx:0,rx:0,err:0,myword:0,power:1
config1:0x1,config2:0x48524a5f,config3:0x424c0a52
oursecret:AhA xxx
$ 
```

我们需要 *root* 权限才能做到这一点。一旦完成，我们可以清楚地看到我们的`drv_ctx`数据结构的所有成员。不仅如此，我们还验证了粗体突出显示的`config1`成员现在的值为`1`，从而反映了设计的“调试级别”。

此外，请注意输出是如何以高度可解析的格式(几乎类似于 JSON)故意生成到用户空间的。当然，作为一个小练习，你可以安排去做！

A large number of recent **Internet of Things** (**IoT**) products use RESTful APIs to communicate; the format that's parsed is typically JSON. Getting in the habit of designing and implementing your kernel-to-user (and vice versa) communication in easily parsable formats (such as JSON) is only going to help.

至此，您已经了解了如何创建一个 procfs 目录，以及其中的一个文件，最重要的是，如何创建和使用读写回调函数，以便当用户模式进程读取或写入您的 proc 文件时，您可以从内核深处做出适当的响应。正如我们前面提到的，由于空间不足，我们将不描述驱动我们已经创建和使用的其余三个 procfs 文件的代码。这在概念上与我们刚刚介绍的内容非常相似。我们希望您通读并试用！

## 一些杂项生产程序

让我们通过查看一些剩余的杂项 procfs APIs 来结束这一部分。您可以使用`proc_symlink()`功能在`/proc`内创建符号链接或软链接。

接下来`proc_create_single_data()` API 可以非常有用；它被用作一种“快捷方式”，您只需要将一个“读取”方法附加到 procfs 文件:

```sh
struct proc_dir_entry *proc_create_single_data(const char *name, umode_t mode, struct     
        proc_dir_entry *parent, int (*show)(struct seq_file *, void *), void *data);
```

因此，使用这个应用编程接口消除了对单独的 fops 数据结构的需要。我们可以使用这个函数来创建和处理我们的第二个 procfs 文件——文件`llkdproc_show_pgoff`:

```sh
... proc_create_single_data(PROC_FILE2, PROC_FILE2_PERMS, gprocdir, proc_show_pgoff, 0) ...
```

当从用户空间读取时，内核的 VFS 和 proc 层代码路径将调用注册的方法——我们模块的`proc_show_pgoff()`函数——在该方法中，我们简单地调用`seq_printf()`将`PAGE_OFFSET`的值发送到用户空间:

```sh
seq_printf(seq, "%s:PAGE_OFFSET:0x%px\n", OURMODNAME, PAGE_OFFSET);
```

此外，请注意以下关于`proc_create_single_data`原料药的信息:

*   您可以使用第五个参数`proc_create_single_data()`将任何数据项传递给 read 回调(在那里作为名为`private`的`seq_file`成员检索，非常类似于我们在上一章中使用`filp->private_data`的方式)。
*   内核主线中的几个典型的老驱动程序确实利用了这个函数来创建它们的 procfs 接口。其中有 RTC 驱动程序(在`/proc/driver/rtc`设置一个入口)。SCSI `megaraid`驱动程序(`drivers/scsi/megaraid`)使用该例程至少 10 次，以设置其 proc 接口(当配置选项被启用时；它是默认的)。

Be careful! I find that on an Ubuntu 18.04 LTS system running the distro (default) kernel, this API – `proc_create_single_data()` – isn't even available, so the build fails. On our custom "vanilla" 5.4 LTS kernel, it works just fine.

此外，还有一些关于我们在这里设置的 procfs API 的文档，尽管这往往是为了内部使用，而不是为了模块:[https://www . kernel . org/doc/html/latest/file systems/API-summary . html # the-proc-file system](https://www.kernel.org/doc/html/latest/filesystems/api-summary.html#the-proc-filesystem)。

所以，正如我们之前提到的，使用 procfs APIs，这是一个**您的里程可能会变化的情况** ( **YMMV** )！发布前仔细测试您的代码。最好遵循内核社区指南，简单地对 procfs 说**不**作为驱动接口技术。别担心——我们会在这一章中看到更好的！

这就完成了我们对使用 procfs 作为有用的通信接口的介绍。现在，让我们学习如何为驱动程序使用一个更合适的接口 sysfs 接口。

# 通过系统文件系统(sysfs)接口

2.6 Linux 内核版本的一个关键特征是所谓的现代*设备模型*的出现。本质上，一系列复杂的树状分层数据结构对系统中的所有设备进行建模。实际上，它远不止于此；**系统树包含以下内容(除其他外):**

*   系统上的每条总线(也可以是虚拟或伪总线)
*   每条总线上的每台设备
*   总线上绑定到设备的每个设备驱动程序

因此，在运行时创建并由设备模型维护的不仅仅是外围设备，还有底层系统总线、每条总线上的设备以及绑定或将要绑定到设备的设备驱动程序。作为一个典型的司机作者，你看不到这个模型的内部工作原理；你真的不用担心。在系统启动时，每当新设备变得可见时，*驱动核心*(内置内核机器的一部分)就会在 sysfs 树下生成所需的虚拟文件。(相反，当设备被移除或分离时，其条目会从树中消失。)

然而，回想一下*与 proc 文件系统*的接口部分，使用 procf 作为设备驱动程序的接口并不是真正正确的方法，至少对于想要向上游移动的代码来说是这样。那么，*的正确做法是什么？啊，*创建 sysfs(伪)文件被认为是设备驱动程序与用户空间交互的“正确方式”。**

 *所以，现在我们看到了！sysfs 是一个虚拟文件系统，通常安装在`/sys`目录下。实际上，sysfs 与 procfs 非常相似，是发送到用户空间的内核导出的信息树(设备和其他)。您可以认为 sysfs 在现代设备模型中具有不同的*视口*。通过 sysfs，您可以通过几种不同的方式或不同的“视口”查看系统；例如，您可以通过它支持的各种总线查看系统(T4 总线视图–PCI、USB、平台、I2C、SPI 等)，通过各种“类”设备(T6 类视图)，通过*设备*本身，通过*块*设备视口，等等。下面的截图显示了我的 Ubuntu 18.04 LTS 虚拟机上`/sys`的内容，显示了这种情况:

![](assets/bf6846b2-8f7a-47e3-98a2-be5528a87f22.png)

Figure 2.2 – Screenshot showing the content of sysfs (/sys) on an x86_64 Ubuntu VM

正如我们所看到的，使用 sysfs，您还可以使用其他几个视窗来查看系统。当然，在本节中，我们希望了解如何通过 sysfs 将设备驱动程序连接到用户空间，如何编写代码在 sysfs 下创建我们的驱动程序(伪)文件，以及如何注册来自它们的读/写回调。让我们从基本的 sysfs APIs 开始。

## 在代码中创建 sysfs(伪)文件

在 sysfs 下创建伪(或虚拟)文件的一种方法是通过`device_create_file()`应用编程接口。其签名如下:

```sh
drivers/base/core.c:int device_create_file(struct device *dev,
                         const struct device_attribute *attr);
```

让我们逐一考虑它的两个参数；首先，有一个指向`struct device`的指针。第二个参数是指向设备属性结构的指针；稍后我们将对此进行解释和处理(在*中设置设备属性并创建 sysfs 文件*部分)。现在，让我们只关注第一个参数——器件结构。这似乎很直观——一个设备由一个名为`device`的元数据结构表示(它是驱动核心的一部分；你可以在`include/linux/device.h`标题中找到它的完整定义。

请注意，当您编写(或处理)一个“真实的”设备驱动程序时，很有可能会存在或形成一个通用的*设备结构*。这通常发生在*注册*设备时；底层设备结构通常可用作该设备的专用结构的成员。例如，所有结构，如`platform_device`、`pci_device`、`net_device`、`usb_device`、`i2c_client`、`serial_port`等，都有一个`struct device`构件嵌入其中。因此，您可以使用该设备结构指针作为 API 的参数，以便在 sysfs 下创建文件。请放心，您很快就会看到这是用代码完成的！因此，让我们通过创建一个简单的“平台设备”来获得一个设备结构。在下一节中，您将学习如何做到这一点！

## 创建简单的平台设备

显然，为了在 sysfs 下创建一个(伪)文件，我们需要一个指向`struct device`的指针作为`device_create_file()`的第一个参数。然而，对于我们此时此地的演示 sysfs 驱动程序，我们实际上没有任何真正的设备，因此没有`struct device`可以工作！

那么，难道我们不能创造一个*人造*或者*伪装置*并简单地使用它吗？是的，但是如何做，更关键的是，我们为什么要这样做？理解现代的 **Linux 设备模型** ( **LDM** )是建立在三个关键组件之上的非常关键:**必须存在一个设备所依赖的底层总线，并且设备被设备驱动程序**所“绑定”和驱动。(我们已经在[第 1 章](1.html)、*编写简单的杂项字符设备驱动程序*、在*Linux 设备型号快速说明*部分提到过这一点)。

所有这些都必须注册到驱动核心。现在，不用担心公共汽车和驾驶它们的公共汽车司机；它们将由内核的驱动核心子系统在内部注册和处理。然而，当没有真正的*设备*时，我们将不得不创建一个伪设备来处理模型。同样，做这样的事情有几种方法，但是我们将创建**一个***T5】平台设备。*该设备将“活”在一条被称为 ***平台总线**的伪总线(即只存在于软件中)上。*

### 平台设备

快速但重要的一点是:*平台设备*通常用于表示嵌入式板内的**片上系统** ( **SoC** )上的各种设备。SoC 通常是一种非常复杂的芯片，它将各种组件集成到硅中。除了处理单元(中央处理器/图形处理器)之外，它还可以容纳多个外围设备，包括以太网媒体访问控制、通用串行总线、多媒体、串行通用异步收发器、时钟、I2C、串行接口、闪存芯片控制器等。我们需要将这些组件枚举为平台设备的一个原因是，SoC 内部没有物理总线；因此，使用平台总线。

Traditionally, the code that was used to instantiate these SoC platform devices was kept in a "board" file (or files) within the kernel source (`arch/<arch>/...`). Due to it becoming overloaded, it's been moved outside the pure kernel source into a useful hardware description format called the **Device Tree** (within **Device Tree Source** (**DTS**) files that are themselves with the kernel source tree).

在我们的 Ubuntu 18.04 LTS 来宾虚拟机上，让我们看看 sysfs 下的平台设备:

```sh
$ ls /sys/devices/platform/
alarmtimer  'Fixed MDIO bus.0'   intel_pmc_core.0   platform-framebuffer.0   reg-dummy   
serial8250 eisa.0  i8042  pcspkr power rtc_cmos uevent
$
```

The *Bootlin* website (previously called *Free Electrons*) offers superb materials on embedded Linux, drivers, and so on. This link on their site leads to excellent material on the LDM: [https://bootlin.com/pub/conferences/2019/elce/opdenacker-kernel-programming-device-model/](https://bootlin.com/pub/conferences/2019/elce/opdenacker-kernel-programming-device-model/).

回到驱动程序:我们通过`platform_device_register_simple()` API 将我们的(人工)平台设备注册到(已经存在的)平台总线驱动程序中，从而使其存在。当我们这样做的时候，驱动核心将*生成*所需的 sysfs 目录和一些样板 sysfs 条目(或文件)。这里，在 sysfs 演示驱动程序的初始化代码中，我们将通过向驱动程序核心注册来设置一个(尽可能简单的)*平台设备*:

```sh
// ch2/sysfs_simple_intf/sysfs_simple_intf.c
include <linux/platform_device.h>
static struct platform_device *sysfs_demo_platdev;
[...]
#define PLAT_NAME    "llkd_sysfs_simple_intf_device"
sysfs_demo_platdev =
     platform_device_register_simple(PLAT_NAME, -1, NULL, 0);
[...]
```

`platform_device_register_simple()`应用编程接口返回一个指向`struct platform_device`的指针。这个组织的成员之一是`struct device dev`。我们现在有了我们一直在追求的:一个*装置* *结构*。另外，需要注意的是，当这个注册 API 运行时，效果在 sysfs *中是可见的。*您可以很容易地看到新的平台设备，加上几个样板 sysfs 对象，正在由驱动核心在这里创建(通过 sysfs 对我们可见)；让我们构建并 *insmod* 我们的内核模块来看看这个:

```sh
$ cd <...>/ch2/sysfs_simple_intf
$ make && sudo insmod ./sysfs_simple_intf.ko
[...]
$ ls -l /sys/devices/platform/llkd_sysfs_simple_intf_device/
total 0
-rw-r--r-- 1 root root 4.0K Feb 15 20:22 driver_override
-rw-r--r-- 1 root root 4.0K Feb 15 20:22 llkdsysfs_debug_level
-r--r--r-- 1 root root 4.0K Feb 15 20:22 llkdsysfs_pgoff
-r--r--r-- 1 root root 4.0K Feb 15 20:22 llkdsysfs_pressure
-r--r--r-- 1 root root 4.0K Feb 15 20:22 modalias
drwxr-xr-x 2 root root 0 Feb 15 20:22 power/
lrwxrwxrwx 1 root root 0 Feb 15 20:22 subsystem -> ../../../bus/platform/
-rw-r--r-- 1 root root 4.0K Feb 15 20:21 uevent
$ 
```

我们可以用不同的方式创造一个`struct device`；通用的方式是设置并发布`device_create()` API。创建 sysfs 文件的另一种方法是创建一个“对象”并调用`sysfs_create_file()`应用编程接口，同时不需要设备结构。(使用这两种方法的教程链接可以在*进一步阅读*部分找到)。这里，我们更喜欢使用“平台设备”，因为它是编写(平台)驱动程序的更接近的方法。

还有另一种有效的方法。正如我们在[第 1 章](1.html)、*编写简单的杂项字符设备驱动程序*中看到的，我们构建了一个符合内核`misc`框架的简单字符驱动程序。在那里，我们实例化了一个`struct miscdevice`；一旦注册(通过`misc_register()` API)，这个结构将包含一个名为`struct device *this_device;`的成员，从而允许我们将其用作有效的设备指针！因此，我们可以简单地扩展我们早期的`misc`设备驱动程序，并在这里使用它。然而，为了了解一些关于平台驱动程序的知识，我们选择了这种方法。(我们将扩展早期`misc`设备驱动程序的方法留给您，以便它可以使用 sysfs APIs 并创建/使用 sysfs 文件作为练习)。

回到我们的驱动程序，与初始化代码相比，在*清理*代码中，我们必须取消注册我们的平台设备:

```sh
platform_device_unregister(sysfs_demo_platdev);
```

现在，让我们将所有这些知识联系在一起，实际看看生成 sysfs 文件的代码，以及它们的读写回调函数！

## 将它们结合在一起—设置设备属性并创建 sysfs 文件

正如我们在本节开始时提到的，我们将使用`device_create_file()`应用编程接口来创建 sysfs 文件:

```sh
int device_create_file(struct device *dev, const struct device_attribute *attr);
```

在前一节中，您学习了我们如何获取设备结构(我们的应用编程接口的第一个参数)。现在，让我们弄清楚如何初始化和使用第二个参数；也就是`device_attribute`结构。结构本身定义如下:

```sh
// include/linux/device.hstruct device_attribute {
    struct attribute attr;
    ssize_t (*show)(struct device *dev, struct device_attribute *attr,
                    char *buf);
    ssize_t (*store)(struct device *dev, struct device_attribute *attr,
                     const char *buf, size_t count);
};
```

第一个成员`attr`主要由 sysfs 文件的*名称*及其*模式*(权限位掩码)组成。另外两个成员是函数指针(“虚函数”)，类似于**文件操作**或 **fops** 结构中的函数指针:

*   `show`:代表*读取回调*功能
*   `store`:代表*写回调*功能

我们的工作是初始化这个`device_attribute`结构，从而建立 sysfs 文件。虽然您总是可以手动初始化它，但有一种更简单的方法:内核提供(几个)宏来初始化`struct device_attribute`；其中有`DEVICE_ATTR()`宏:

```sh
// include/linux/device.h
define DEVICE_ATTR(_name, _mode, _show, _store) \
   struct device_attribute dev_attr_##_name = __ATTR(_name, _mode, _show, _store)
```

请注意`dev_attr_##_name`执行的“字符串化”，确保结构的名称以作为第一个参数传递给`DEVICE_ATTR`的名称作为后缀。此外，名为`__ATTR()`的实际“工作者”宏实际上在预处理时在代码中实例化了一个`device_attribute`结构，该结构的名称变成了`dev_attr_<name>`:

```sh
// include/linux/sysfs.h
#define __ATTR(_name, _mode, _show, _store) { \
    .attr = {.name = __stringify(_name), \
    .mode = VERIFY_OCTAL_PERMISSIONS(_mode) }, \
    .show = _show, \
    .store = _store, \
}
```

此外，内核在这些宏上定义了额外的简单包装宏，以便指定*模式*(sysfs 文件的权限)，从而使您这个驱动程序作者更加简单。其中有`DEVICE_ATTR_RW(_name)`、`DEVICE_ATTR_RO(_name)`、`DEVICE_ATTR_WO(_name)`:

```sh
#define DEVICE_ATTR_RW(_name) \
     struct device_attribute dev_attr_##_name = __ATTR_RW(_name)
#define __ATTR_RW(_name) __ATTR(_name, 0644, _name##_show, _name##_store)
```

有了这个代码，我们可以创建一个**读写**(**RW**)**只读** ( **RO** )或者**只写** ( **WO** ) sysfs 文件。现在，我们希望建立一个可以读写的 sysfs 文件。在内部，这是一个“钩子”或回调，供我们查询或设置`debug_level`全局变量，就像我们之前在 procfs 上的示例内核模块中所做的那样！

既然我们有了足够的背景，就让我们深入研究一下代码吧！

### 实现 sysfs 文件及其回调的代码

让我们看看简单的 *sysfs 接口驱动程序*的相关代码部分，一步一步地尝试一下:

1.  设置设备属性结构(通过`DEVICE_ATTR_RW`宏；有关更多信息，请参见前面的部分)并创建我们的第一个 sysfs(伪)文件:

```sh
// ch2/sysfs_simple_intf/sysfs_simple_intf.c
#define SYSFS_FILE1 llkdsysfs_debug_level
// [... *<we show the actual read/write callback functions just a bit further down>* ...]
static DEVICE_ATTR_RW(SYSFS_FILE1);

int __init sysfs_simple_intf_init(void)
{
 [...]
*/* << 0\. The platform device is created via the platform_device_register_simple() API; code already shown above ... >> */*

 // 1\. Create our first sysfile file : llkdsysfs_debug_level
 /* The device_create_file() API creates a sysfs attribute file for
  * given device (1st parameter); the second parameter is the pointer
  * to it's struct device_attribute structure dev_attr_<name> which was
  * instantiated by our DEV_ATTR{_RW|RO} macros above ... */
  stat = device_create_file(&sysfs_demo_platdev->dev, &dev_attr_SYSFS_FILE1);
[...]
```

从这里显示的宏的定义中，我们可以推断出`static DEVICE_ATTR_RW(SYSFS_FILE1);`实例化了一个名为`llkdsysfs_debug_level`的初始化的`device_attribute`结构(因为`SYSFS_FILE1`宏就是这样评估的)和一个`0644`模式；读取回调名为`llkdsysfs_debug_level_show()`，写入回调名为`llkdsysfs_debug_level_store()`！

2.  下面是读写回调的相关代码(同样，我们不会在这里显示全部代码)。首先，让我们看看 read 回调:

```sh
/* debug_level: sysfs entry point for the 'show' (read) callback */
static ssize_t llkdsysfs_debug_level_show(struct device *dev,
                                          struct device_attribute *attr,
                                          char *buf)
{
        int n;
        if (mutex_lock_interruptible(&mtx))
                return -ERESTARTSYS;
        pr_debug("In the 'show' method: name: %s, debug_level=%d\n",   
                 dev->kobj.name, debug_level); 
        n = snprintf(buf, 25, "%d\n", debug_level);
        mutex_unlock(&mtx);
        return n;
}
```

这是如何工作的？在读取我们的 sysfs 文件时，调用前面的回调函数。在其中，简单地写入用户提供的缓冲区指针，`buf`(它的第三个参数；我们使用内核`snprintf()` API 这样做)，具有将*(【这里是 T4】、`debug_level`)提供的值转移到用户空间的效果！*

3.  让我们构建并`insmod(8)`内核模块(为了方便起见，我们将使用我们的`lkm`包装脚本来实现):

```sh
$ ../../lkm sysfs_simple_intf          // <-- build and insmod it[...]
[83907.192247] sysfs_simple_intf:sysfs_simple_intf_init():237: sysfs file [1] (/sys/devices/platform/llkd_sysfs_simple_intf_device/llkdsysfs_debug_level) created
[83907.197279] sysfs_simple_intf:sysfs_simple_intf_init():250: sysfs file [2] (/sys/devices/platform/llkd_sysfs_simple_intf_device/llkdsysfs_pgoff) created
[83907.201959] sysfs_simple_intf:sysfs_simple_intf_init():264: sysfs file [3] (/sys/devices/platform/llkd_sysfs_simple_intf_device/llkdsysfs_pressure) created
[83907.205888] sysfs_simple_intf initialized
$
```

4.  现在，让我们列出并读取与调试级别相关的 sysfs 文件:

```sh
$ ls -l /sys/devices/platform/llkd_sysfs_simple_intf_device/llkdsysfs_debug_level
-rw-r--r-- 1 root root 4096 Feb   4 17:41 /sys/devices/platform/llkd_sysfs_simple_intf_device/llkdsysfs_debug_level
$ cat /sys/devices/platform/llkd_sysfs_simple_intf_device/llkdsysfs_debug_level
0
```

这反映了调试级别目前为`0`的事实。

5.  现在，让我们来看一下调试级 sysfs 文件的*写回调*的代码:

```sh
#define DEBUG_LEVEL_MIN 0
#define DEBUG_LEVEL_MAX 2

static ssize_t llkdsysfs_debug_level_store(struct device *dev,
                                           struct device_attribute *attr,
                                           const char *buf, size_t count)
{
        int ret = (int)count, prev_dbglevel;
        if (mutex_lock_interruptible(&mtx))
                return -ERESTARTSYS;

        prev_dbglevel = debug_level;
        pr_debug("In the 'store' method:\ncount=%zu, buf=0x%px count=%zu\n"
        "Buffer contents: \"%.*s\"\n", count, buf, count, (int)count, buf);
        if (count == 0 || count > 12) {
                ret = -EINVAL;
                goto out;
        }

        ret = kstrtoint(buf, 0, &debug_level); /* update it! */
 *// < ... validity checks ... >*
        ret = count;
 out:
        mutex_unlock(&mtx);
        return ret;
}
```

同样，应该清楚的是`kstrtoint()`内核 API 用于将用户空间`buf`字符串转换为整数值，然后我们进行验证。还有，`kstrtoint`的第三个参数是要写入的整数，因此要更新！

6.  现在，让我们尝试从 sysfs 文件更新`debug_level`的值:

```sh
$ sudo sh -c "echo 2 > /sys/devices/platform/llkd_sysfs_simple_intf_device/llkdsysfs_debug_level"
$ cat /sys/devices/platform/llkd_sysfs_simple_intf_device/llkdsysfs_debug_level
2
$
```

瞧，成功了！

7.  正如我们在与 procfs 接口时所做的那样，我们在 sysfs 代码示例中提供了更多的代码。在这里，我们有另一个(只读)sysfs 界面来显示`PAGE_OFFSET`的值，外加一个新的。想象一下，这个司机的工作是获取一个“压力”值(也许是通过 I2C 驱动的压力传感器芯片)。让我们假设我们已经这样做了，并将这个压力值存储在一个名为`gpressure`的整数全局变量中。要“显示”用户空间的当前压力值，我们必须使用 sysfs 文件。这是:

Internally, for the purpose of this demo, we have randomly set the `gpressure` global variable to a value of `25`.

```sh
$ cat /sys/devices/platform/llkd_sysfs_simple_intf_device/llkdsysfs_pressure
25$
```

仔细看输出；`25`后为什么立即出现提示？因为我们只是按原样打印了值——没有换行符，什么都没有；这是人们所期待的。显示“压力”值的代码确实很简单:

```sh
/* show 'pressure' value: sysfs entry point for the 'show' (read) callback */
static ssize_t llkdsysfs_pressure_show(struct device *dev,
                       struct device_attribute *attr, char *buf)
{
        int n;
        if (mutex_lock_interruptible(&mtx))
                return -ERESTARTSYS;
        pr_debug("In the 'show' method: pressure=%u\n", gpressure);
        n = snprintf(buf, 25, "%u", gpressure);
        mutex_unlock(&mtx);
        return n;
}
/* The DEVICE_ATTR{_RW|RO|WO}() macro instantiates a struct device_attribute dev_attr_<name> here...   */
static DEVICE_ATTR_RO(llkdsysfs_pressure); 
```

至此，您已经学会了如何通过 sysfs 与用户空间进行交互！像往常一样，我敦促您实际编写代码，并亲自尝试这些技能；看一下本章末尾的*问题*部分，自己尝试一下(相关的)作业。现在，让我们继续 sysfs，了解一个关于其 ABI 的重要*规则*。

## “每个 sysfs 文件一个值”规则

到目前为止，您已经理解了如何为用户空间内核接口目的创建和使用 sysfs，但是有一个关键点我们一直忽略。有一个关于使用 sysfs 文件的“规则”，它规定你只能读写一个值！将此视为*每个文件一个值*规则。

因此，就像我们使用“压力”值的例子一样，我们只返回压力的当前值，仅此而已。因此，与其他接口技术不同，sysfs 不太适合那些您可能想要向用户空间返回任意冗长的信息包(比如，驱动程序上下文结构的内容)的情况；换句话说，它不适合纯粹的“调试”目的。

The kernel documents and "rules" regarding the usage of sysfs can be found here: [https://www.kernel.org/doc/html/latest/admin-guide/sysfs-rules.html#rules-on-how-to-access-information-in-sysfs](https://www.kernel.org/doc/html/latest/admin-guide/sysfs-rules.html#rules-on-how-to-access-information-in-sysfs).

此外，这里还有 sysfs API 的文档:[https://www . kernel . org/doc/html/latest/file systems/API-summary . html #用于导出内核对象的文件系统](https://www.kernel.org/doc/html/latest/filesystems/api-summary.html#the-filesystem-for-exporting-kernel-objects)。

内核通常提供几种不同的方法来创建 sysfs 对象；例如，使用`sysfs_create_files()` API，可以一次创建多个 sysfs 文件:`int __must_check sysfs_create_files(struct kobject *kobj, const struct attribute * const *attr);`。这里，您需要提供一个指向`kobject`的指针和一个指向属性结构列表的指针。

我们对 sysfs 作为接口技术的讨论到此结束；总之，sysfs 确实被认为是驱动程序作者在用户空间显示和/或设置特定驱动程序值的正确方式。由于“每个 sysfs 文件一个值”的约定，sysfs 确实不太适合调试信息分配。这巧妙地将我们带到了下一个话题——首次亮相！

# 通过调试文件系统(debugfs)接口

想象一下，作为一名驱动程序开发人员，你在 Linux 上面临的困境:你想要实现一种简单而优雅的方式来提供从驱动程序到用户空间的调试“钩子”。例如，用户简单地对一个(伪)文件执行`cat(1)`会导致你的驱动程序的“调试回调”函数被调用。然后，它将继续向用户模式进程转储一些状态信息(可能是“驱动程序上下文”结构)，用户模式进程将忠实地将其转储到 stdout。

好的，没问题:在 2.6 版本发布之前的几天里，我们可以(正如您在*通过 proc 文件系统(procfs)* 进行接口)愉快地使用 procfs 层将我们的驱动程序与用户空间进行接口。然后，从 2.6 Linux 开始，内核社区否决了这种方法。我们被告知严格停止使用 procfs，而是使用 sysfs 层作为驱动程序与用户空间接口的手段。然而，正如我们在*通过 sys 文件系统(sysfs)* 与 *接口部分看到的，它有一个严格的*每个文件一个值*规则。这对于向驱动程序报告单个值或向驱动程序发送单个值(通常是环境传感器值等)来说非常有用，但是很快就排除了除了最琐碎的调试接口之外的所有接口。我们可以使用 ioctl 方法(正如我们将看到的)来设置一个调试接口，但是这样做要困难得多。*

那么，你能做什么？幸运的是，从 2.6.12 Linux 开始，有一个优雅的解决方案叫做 debugfs。“调试文件系统”非常容易使用，并且在传达驱动程序作者(事实上，任何人)可以为他们选择的任何目的使用它的事实时非常明确！没有每文件一个值的规则——忘了吧，没有规则。

当然，就像我们处理过的其他基于文件系统的方法一样——procfs、sysfs 和现在的 debugfs——内核社区明确声称所有这些接口都是 ABI，因此，它们的稳定性和寿命是*而不是*保证的。虽然这是采用的正式立场，但现实是这些接口已经成为现实世界中事实上的接口；在一个晴朗的日子里，不加掩饰地把它们剥掉，对任何人都没有好处。

下面的截图显示了我们的 x86-64 Ubuntu 18.04.3 LTS 来宾上的 debugfs 的内容(运行我们在配套书籍*中构建的“自定义”5.4.0 内核 Linux 内核 Programmin* g、*第 3 章*、*从源代码构建 5.0 Linux 内核，第 2 部分*！):

![](assets/d01e778c-ea1a-4934-8283-30d35557238d.png)

Figure 2.3 – Screenshot revealing the content of the debugfs filesystem on an x86_64 Linux VM

与 procfs 和 sysfs 一样，由于 debugfs 是一个内核特性(毕竟它是一个虚拟文件系统！)，其中的精确内容高度依赖于内核版本和 CPU 架构。正如我们之前提到的，通过查看这个截图，现在应该很明显，有很多真实世界的 debugfs“用户”。

## 检查 debugfs 的存在

首先，为了使用强大的*调试*界面，必须在内核配置中启用它。相关的 Kconfig 宏为`CONFIG_DEBUG_FS`。让我们检查一下它是否在我们的 5.4 定制内核上启用:

Here, we are assuming you have the `CONFIG_IKCONFIG` and `CONFIG_IKCONFIG_PROC` options set to `y`, thus allowing us to use the `/proc/config.gz` pseudo file to access the current kernel's configuration.

```sh
$ zcat /proc/config.gz | grep -w CONFIG_DEBUG_FS
CONFIG_DEBUG_FS=y
```

的确如此；它通常在发行版中默认启用。

接下来，debugfs 的默认挂载点是`/sys/kernel/debug`。因此，我们可以看到它在内部依赖于 sysfs 内核特性的存在和装载，这是默认的。让我们检查一下 debugfs 在我们的 Ubuntu 18.04 x86_64 虚拟机上的安装位置:

```sh
$ mount | grep -w debugfs
debugfs on /sys/kernel/debug type debugfs (rw,relatime)
```

它可用并安装在预期位置；也就是`/sys/kernel/debug`。

Of course, it's always a best practice to never assume that this will always be the location where it's mounted; in your script or user mode C program, take the trouble to check and verify it. In fact, allow me to rephrase this: *it's always a good practice to never assume anything; making assumptions is a really good source of bugs*.

顺便说一下，一个有趣的 Linux 特性是文件系统可以安装在不同的位置，甚至多个位置；还有，有些人更喜欢创建一个符号链接到`/sys/kernel/debug`作为`/debug`；这取决于你，真的。

像往常一样，我们在这里的意图是在 debugfs 保护伞下创建我们的(伪)文件，然后注册并利用来自它们的读/写回调，目的是将我们的驱动程序与用户空间接口。为此，我们需要了解 debugfs API 的基本用法。我们将在下一节中向您介绍这方面的文档。

## 查找调试应用编程接口文档

内核在这里提供了关于使用 debugfs API 的简洁而优秀的文档(由 LWN Jonathan Corbet 提供):https://www . kernel . org/doc/Documentation/file systems/debugfs . txt(当然，您也可以直接在内核代码库中查找)。

我敦促您参考这个文档来学习如何使用 debugfs APIs，因为它很容易阅读和理解；这样，您可以避免不必要地在这里重复相同的信息。除了前面提到的文档之外，现代内核文档系统(基于“狮身人面像”的系统)也提供了相当详细的 debugfs API 页面:[https://www . kernel . org/doc/html/latest/file systems/API-summary . html？highlight = debugfs # the-debugfs-file system](https://www.kernel.org/doc/html/latest/filesystems/api-summary.html?highlight=debugfs#the-debugfs-filesystem)。

Note that all debugfs APIs are exported as GPL-only to kernel modules (thus necessitating the module being released under the "GPL" license (this can be dual licensed, but one must be "GPL")).

## 与 debugfs 的接口示例

Debugfs 被刻意设计成“没有特定规则”的心态，这使得它成为使用*进行调试的理想界面*。为什么呢？它允许你构建任意字节流并将其发送到用户空间，包括带有`debugfs_create_blob()`应用编程接口的二进制“blob”。

我们前面的示例内核模块使用 procfs 和 sysfs 构建并使用了三到四个(伪)文件。对于 debugfs 的快速演示，我们将只关注两个“文件”:

*   `llkd_dbgfs_show_drvctx`:你肯定猜到了，读的时候会导致我们(现在已经很熟悉的)“驱动上下文”数据结构的当前内容被转储到控制台；我们将确保伪文件的模式是只读的(按根)。
*   `llkd_dbgfs_debug_level`:该文件的模式应为读写(仅限根用户)；读取时会显示`debug_level`的当前值；当一个整数被写入其中时，我们将把内核模块中`debug_level`的值更新为传递的值。

这里，在我们内核模块的初始化代码中，我们将首先在`debugfs`下创建一个目录:

```sh
// ch2/debugfs_simple_intf/debugfs_simple_intf.c

static struct dentry *gparent;
[...]
static int debugfs_simple_intf_init(void)
{
    int stat = 0;
    struct dentry *file1, *file2;
    [...]
    gparent = debugfs_create_dir(OURMODNAME, NULL);
```

现在我们有了一个起点——一个目录——让我们继续前进，并在它下面创建 debugfs(伪)文件。

### 创建和使用第一个 debugfs 文件

For readability and to save space, we won't show the error handling code sections here.

就像在 procfs 的例子中一样，我们必须分配和初始化我们的“驱动程序上下文”数据结构的一个实例(我们没有在这里显示代码，因为它是重复的，所以请参考 GitHub 源代码)。

然后，通过通用的`debugfs_create_file()`应用编程接口，我们必须创建一个`debugfs`文件，将其与一个`file_operations`结构相关联。这实际上只是注册了一个 read 回调:

```sh
static const struct file_operations dbgfs_drvctx_fops = {
    .read = dbgfs_show_drvctx,
};
[...]
*// < ... init function ... >*
   /* Generic debugfs file + passing a pointer to a data structure as a
    * demo.. the 4th param is a generic void * ptr; it's contents will be
    * stored into the i_private field of the file's inode.
    */
#define DBGFS_FILE1 "llkd_dbgfs_show_drvctx"
    file1 = debugfs_create_file(DBGFS_FILE1, 0440, gparent,
                (void *)gdrvctx, &dbgfs_drvctx_fops);
    [...]
```

From 5.8 Linux onward (recall that we're working with the 5.4 LTS kernel), the return value of several of the debugfs creation APIs have been removed (they will return `void`); Greg Kroah-Hartman's patch mentions that this was done as no one was using them. This is quite typical of Linux – unneeded features are stripped off, and kernel evolution continues...

显然，“读取”回调是我们的`dbgfs_show_drvctx()`功能。提醒一下，每当读取`debugfs`文件(`llkd_dbgfs_show_drvctx`)时，该函数都会被 debugfs 层自动调用；下面是我们的 debugfs read 回调函数的代码:

```sh
static ssize_t dbgfs_show_drvctx(struct file *filp, char __user * ubuf,
                                 size_t count, loff_t * fpos)
{
    struct drv_ctx *data = (struct drv_ctx *)filp->f_inode->i_private;
                       // retrieve the "data" from the inode
#define MAXUPASS 256   // careful- the kernel stack is small!
    char locbuf[MAXUPASS];

    if (mutex_lock_interruptible(&mtx))
        return -ERESTARTSYS;

   /* As an experiment, we set our 'config3' member of the drv ctx stucture
    * to the current 'jiffies' value (# of timer interrupts since boot);
    * so, every time we 'cat' this file, the 'config3' value should change!
    */
   data->config3 = jiffies;
   snprintf(locbuf, MAXUPASS - 1,
            "prodname:%s\n"
            "tx:%d,rx:%d,err:%d,myword:%d,power:%d\n"
            "config1:0x%x,config2:0x%x,config3:0x%llx (%llu)\n"
            "oursecret:%s\n",
            OURMODNAME,
            data->tx, data->rx, data->err, data->myword, data->power,
            data->config1, data->config2, data->config3, data->config3,
            data->oursecret);

    mutex_unlock(&mtx);
    return simple_read_from_buffer(ubuf, MAXUPASS, fpos, locbuf,
                                   strlen(locbuf));
}
```

请注意，我们如何通过取消引用 debugfs 文件的 inode 成员来检索“数据”指针(我们的驱动程序上下文结构)，这被称为`i_private`。

As we mentioned in [Chapter 1](1.html), *Writing a Simple misc Character Device Driver*, using the `data` pointer to dereference the driver context structure from the file's inode is one of a number of similar, common techniques employed by driver authors to avoid the use of globals. Here, `gdrvctx` *is* a global, so it's a moot point; we are simply using it to demonstrate the typical use case.

使用`snprintf()`应用编程接口，我们可以用驱动程序“上下文”结构的当前内容填充一个本地缓冲区，然后通过`simple_read_from_buffer()`应用编程接口将其传递给发出读取的用户空间应用程序，这通常会导致它显示在终端/控制台窗口上。这个`simple_read_from_buffer()`原料药是`copy_to_user()`的包装。

让我们转一转:

```sh
$ ../../lkm debugfs_simple_intf
[...]
[200221.725752] dbgfs_simple_intf: allocated and init the driver context structure
[200221.728158] dbgfs_simple_intf: debugfs file 1 <debugfs_mountpt>/dbgfs_simple_intf/llkd_dbgfs_show_drvctx created
[200221.732167] dbgfs_simple_intf: debugfs file 2 <debugfs_mountpt>/dbgfs_simple_intf/llkd_dbgfs_debug_level created
[200221.735723] dbgfs_simple_intf initialized
```

正如我们所看到的，两个 debugfs 文件是按照预期创建的；让我们验证这一点(这里要小心；你只能把 debugfs 看作*根*):

```sh
$ ls -l /sys/kernel/debug/dbgfs_simple_intf
ls: cannot access '/sys/kernel/debug/dbgfs_simple_intf': Permission denied
$ sudo ls -l /sys/kernel/debug/dbgfs_simple_intf
total 0
-rw-r--r-- 1 root root 0 Feb  7 15:58 llkd_dbgfs_debug_level
-r--r----- 1 root root 0 Feb  7 15:58 llkd_dbgfs_show_drvctx
$
```

伪文件已经创建并具有正确的权限。现在，让我们从`llkd_dbgfs_show_drvctx`文件中读取(作为根用户):

```sh
$ sudo cat /sys/kernel/debug/dbgfs_simple_intf/llkd_dbgfs_show_drvctx
prodname:dbgfs_simple_intf
tx:0,rx:0,err:0,myword:0,power:1
config1:0x0,config2:0x48524a5f,config3:0x102fbcbc2 (4345023426)
oursecret:AhA yyy
$
```

它有效；几秒钟后再次执行读取。注意`config3`的值是如何变化的。为什么呢？回想一下，我们将其设置为`jiffies`值，即自系统启动以来发生的定时器“滴答”/中断的次数:

```sh
$ sudo cat /sys/kernel/debug/dbgfs_simple_intf/llkd_dbgfs_show_drvctx | grep config3
config1:0x0,config2:0x48524a5f,config3:0x102fbe828 (4345030696)
$
```

创建并使用了我们的第一个 debugfs 文件后，让我们了解第二个 debugfs 文件。

### 创建和使用第二个 debugfs 文件

让我们进入第二个 debugfs 文件。我们将使用一个有趣的快捷助手 debugfs API`debugfs_create_u32()`来创建它。这个应用编程接口*自动*设置内部回调，允许你在驱动程序中读/写指定的无符号 32 位全局变量。这个“助手”例程的主要优点是不需要显式提供`file_operations`结构，甚至不需要任何回调例程。debugfs 层“理解”并在内部进行设置，以便读取或写入数字(全局)变量将始终正常工作！看看 *init* 代码路径中的以下代码，它创建并设置了我们的第二个调试文件:

```sh
static int debug_level;    /* 'off' (0) by default ... */ 
[...]
 /* 3\. Create the debugfs file for the debug_level global; we use the
    * helper routine to make it simple! There is a downside: we have no
    * chance to perform a validity check on the value being written.. */
#define DBGFS_FILE2     "llkd_dbgfs_debug_level"
   file2 = debugfs_create_u32(DBGFS_FILE2, 0644, gparent, &debug_level);
   [...]
   pr_debug("%s: debugfs file 2 <debugfs_mountpt>/%s/%s created\n",
             OURMODNAME, OURMODNAME, DBGFS_FILE2);
```

就这么简单！现在，读取这个文件会产生`debug_level`的当前值；写入它会将其设置为写入的值。让我们这样做:

```sh
$ sudo cat /sys/kernel/debug/dbgfs_simple_intf/llkd_dbgfs_debug_level
0
$ sudo sh -c "echo 5 > /sys/kernel/debug/dbgfs_simple_intf/llkd_dbgfs_debug_level"
$ sudo cat /sys/kernel/debug/dbgfs_simple_intf/llkd_dbgfs_debug_level
5
$ 
```

这是可行的，但是这种“捷径”方法有一个缺点:因为这都是在内部完成的，所以我们没有办法*验证*正在写入的值。因此，在这里，我们将值`5`写到`debug_level`；它起作用了，但它是一个无效的值(至少让我们假设是这样的)！那么，如何才能纠正这种情况呢？简单:不要使用这个助手方法；取而代之的是，通过通用的`debugfs_create_file()`应用编程接口以“通常”的方式进行(就像我们对第一个 debugfs 文件所做的那样)。这样做的好处是，当我们为读取和写入设置显式回调例程时，通过在 fops 结构中指定它们，我们可以控制正在写入的值(我将此留给您，作为练习)。就像生活一样，这是一种权衡；你赢了一些，你输了一些。

## 用于处理数字全局的助手调试 API

您刚刚学习了如何使用`debugfs_create_u32()`助手 API 来设置一个 debugfs 文件来读取/写入一个无符号的 32 位整数全局。事实上，debugfs 层提供了一堆类似的“助手”API，用于隐式读取/写入模块中的数字(整数)全局变量。

下面是创建 debugfs 条目的助手例程，这些条目可以读取/写入不同位大小的无符号整数(8 位、16 位、32 位和 64 位)全局变量。最后一个参数是关键参数——内核/模块中全局整数的地址:

```sh
// include/linux/debugfs.h
struct dentry *debugfs_create_u8(const char *name, umode_t mode,
                 struct dentry *parent, u8 *value);
struct dentry *debugfs_create_u16(const char *name, umode_t mode,
                 struct dentry *parent, u16 *value);
struct dentry *debugfs_create_u32(const char *name, umode_t mode,
                 struct dentry *parent, u32 *value);
struct dentry *debugfs_create_u64(const char *name, umode_t mode,
                 struct dentry *parent, u64 *value);
```

前面的 API 使用十进制基数；为了使使用*十六进制基数*变得容易，我们有以下助手:

```sh
struct dentry *debugfs_create_x8(const char *name, umode_t mode,
                 struct dentry *parent, u8 *value);
struct dentry *debugfs_create_x16(const char *name, umode_t mode,
                 struct dentry *parent, u16 *value);
struct dentry *debugfs_create_x32(const char *name, umode_t mode,
                 struct dentry *parent, u32 *value);
struct dentry *debugfs_create_x64(const char *name, umode_t mode,
                 struct dentry *parent, u64 *value);
```

As an aside, the kernel also provides a helper API for those cases where the precise *size* of the variable varies; hence, using the `debugfs_create_size_t()` helper creates a debugfs file appropriate for a variable of size `size_t`.

对于只需要查看数值全局的驱动程序，或者更新它而不用担心无效值的驱动程序来说，这些 debugfs 助手 API 非常有用，并且确实被主线内核中的几个驱动程序常用(我们将很快在 MMC 驱动程序中查看一个示例)。为了规避“有效性检查”问题，通常我们可以安排*用户空间*应用(或脚本)进行有效性检查；事实上，这通常是做事的“正确方式”。

The UNIX paradigm has a saying: *provide mechanism, not policy.*

当使用属于*布尔*类型的全局时，debugfs 提供了以下助手应用编程接口:

```sh
struct dentry *debugfs_create_bool(const char *name, umode_t mode,
                  struct dentry *parent, bool *value);
```

读取“文件”将只返回`Y`或`N`(以换行符结尾)；显然，`Y`如果第四个`value`参数的当前值为非零，`N`则不然。写的时候可以写`Y`或者`N`或者`1`或者`0`；其他值将不被接受。

Think about it: you can control your "robot" device via your robot device driver by writing `1` to a boolean variable called, say, `power` to turn it on, and use `0` to turn it off! The possibilities are endless.

debugfs 上的内核文档提供了一些其他的 APIs 我把它留给你看。既然我们已经介绍了如何创建和使用我们的演示 debugfs 伪文件，让我们学习如何删除它们。

### 正在删除 debugfs 伪文件

当一个模块被移除时(例如，通过`rmmod(8)`，我们必须删除我们的调试文件。更老的方法是通过`debugfs_remove()`应用编程接口，其中每个调试文件都必须单独移除(至少可以说是痛苦的)。现代方法使这变得非常简单:

```sh
void debugfs_remove_recursive(struct dentry *dentry);
```

将指针传递到整个“父”目录(我们首先创建的目录)，整个分支被递归移除；太好了。

此时不删除您的 debugfs 文件，从而使它们在文件系统中处于孤立状态，这是自找麻烦！试想一下:当以后有人(试图)读或写它们中的任何一个时，会发生什么？**一个内核 bug，或者一个*哎呀*** ，就是这样。

#### 看到一个内核错误–哎呀！

让我们实现它——一个内核错误！令人兴奋，是的！？

好吧，要创建一个内核 bug，我们必须确保当我们移除(卸载)内核模块时，清理(删除)所有 debugfs 文件的 API`debugfs_remove_recursive()`是*而不是*调用的。因此，在每个模块被移除之后，我们的 debugfs 目录和文件似乎就出现了！但是，如果您尝试对它们中的任何一个进行读/写操作，它们将处于*孤立状态*，因此，在尝试取消引用其元数据时，内部 debugfs 代码路径将执行无效的内存引用，从而导致(内核级)错误。

在内核空间中，bug 确实是一件非常严重的事情；从理论上讲，它永远不会发生！这叫做*哎呀；*作为处理这种情况的一部分，调用内部内核函数，该函数通过`printk`将有用的诊断信息转储到内存内核日志缓冲区，以及控制台设备(在生产系统上，它也可能被定向到其他地方，以便以后可以检索和调查；例如，通过内核的 *kdump* 机制。

让我们引入一个模块参数，该参数控制我们(相当故意地)是否导致*哎呀*发生:

```sh
// ch2/debugfs_simple_intf/debugfs_simple_intf.c
[...]
/* Module parameters */
static int cause_an_oops;
module_param(cause_an_oops, int, 0644);
MODULE_PARM_DESC(cause_an_oops,
"Setting this to 1 can cause a kernel bug, an Oops; if 1, we do NOT perform required cleanup! so, after removal, any op on the debugfs files will cause an Oops! (default is 0, no bug)");
```

在我们的驱动程序的清理代码路径中，我们检查`cause_an_oops`变量是否非零，并故意做*而不是*(递归地)删除我们的 debugfs 文件，因此设置了错误:

```sh
static void debugfs_simple_intf_cleanup(void)
{
        kfree(gdrvctx);
        if (!cause_an_oops)
 debugfs_remove_recursive(gparent);
        pr_info("%s removed\n", OURMODNAME);
}
```

当我们“正常”使用`insmod(8)`时，吓人的`cause_an_oops`模块参数默认为`0`，从而保证一切正常。但是让我们开始冒险吧！我们正在构建内核模块，当我们插入它时，我们必须传递参数，同时将其设置为`1`(请注意，在这里，我们在我们的 x86_64 Ubuntu 18.04 LTS 来宾系统上以*根*的身份在我们的自定义`5.4.0-llkd01`内核上运行):

```sh
# id
uid=0(root) gid=0(root) groups=0(root)
# insmod ./debugfs_simple_intf.ko cause_an_oops=1
# cat /sys/kernel/debug/dbgfs_simple_intf/llkd_dbgfs_debug_level
0
# dmesg 
[ 2061.048140] dbgfs_simple_intf: allocated and init the driver context structure
[ 2061.050690] dbgfs_simple_intf: debugfs file 1 <debugfs_mountpt>/dbgfs_simple_intf/llkd_dbgfs_show_drvctx created
[ 2061.053638] dbgfs_simple_intf: debugfs file 2 <debugfs_mountpt>/dbgfs_simple_intf/llkd_dbgfs_debug_level created
[ 2061.057089] dbgfs_simple_intf initialized (fyi, our 'cause an Oops' setting is currently On)
# 
```

现在，让我们移除内核模块——在内部，用于清理(递归删除)我们的 debugfs 文件的代码不会运行。在这里，我们实际上是通过试图读取我们的一个 debugfs 文件来触发内核错误*哎呀，*:

```sh
# rmmod debugfs_simple_intf
# cat /sys/kernel/debug/dbgfs_simple_intf/llkd_dbgfs_debug_level 
Killed
```

控制台上的`Killed`信息是不祥之兆！这是一个线索，表明出了问题。查看内核日志确认我们确实得到了*哎呀！*以下(部分裁剪)截图显示了这一点:

![](assets/1b9849ec-98d3-4fa8-a772-87b4d6fa656b.png)

Figure 2.4 – A partial screenshot of a kernel Oops, a kernel-level bug

由于提供的内核调试细节超出了本书的范围，我们在此不再赘述。尽管如此，弄清楚一点还是很直观的。仔细看前面的截图:在`BUG:`语句中，可以看到**内核虚拟地址** ( **kva** )其查找导致了这个 bug，称为 Oops(我们在配套指南 *Linux 内核编程–第 7 章，内存内部管理要领*中覆盖了 kva 空间；这确实是驱动程序作者的关键信息):

```sh
CPU: 1 PID: 4673 Comm: cat Tainted: G OE 5.4.0-llkd01 #2
```

这显示了运行进程上下文(`cat`)的 CPU ( `1`)、被污染的标志和内核版本。输出的一个真正关键的部分如下:

```sh
RIP: 0010:debugfs_u32_get+0x5/0x20
```

这告诉你 CPU 指令指针(x86_64 上名为 RIP 的寄存器)在`debugfs_u32_get()`函数中，从函数的机器码开始偏移`0x5`字节(此外，内核算出函数的长度是`0x20`字节)！

Combining this information with powerful tools such as `objdump(1)` and `addr2line(1)` can help to literally pinpoint the location of the bug in code!

CPU 寄存器被转储；更好的是，*调用跟踪*或*调用栈*–进程上下文的内核模式栈的*内容(关于内核栈的详细信息，请参考 *Linux 内核编程*、在*第 6 章*、*内核内部本质、进程和线程、*–向您展示了导致这一点的代码；也就是崩溃(自下而上读取堆栈跟踪)。另一个快速提示:如果调用跟踪输出中的一个内核函数前面有一个`?`符号，就忽略它(可能是之前留下的一个“回光返照”)。*

Realistically, a kernel bug on a production system *must* cause the entire system to panic (halt). On non-production systems (like what we're running on), a kernel panic may or may not occur; here, it doesn't. Nevertheless, a kernel bug must be treated with the highest level of severity, it's indeed a show-stopper and must be fixed. The procfs file, `/proc/sys/kernel/panic_on_oops`, is set to `0` by most distros, but on production systems, it will typically be set to the value `1`.

这里的寓意很清楚:debugfs 没有执行自动清理；我们必须这么做。好的，让我们通过查看内核中的一些实际使用情况来结束关于 debugfs 的讨论。

## debugfs–实际用户

正如我们之前提到的，debugfs API 有几个“真实世界”的用户；我们能认出其中一些吗？嗯，这里有一个方法:简单地在内核源码树的`drivers/`目录下搜索名为`*debugfs*.c`的文件；你可能会感到惊讶(我在 5.4.0 内核树中发现了 114 个这样的文件！).我们来看几个:

```sh
$ cd <kernel-source-tree> ; find drivers/ -iname "*debugfs*.c" 
drivers/block/drbd/drbd_debugfs.c
drivers/mmc/core/debugfs.c
drivers/platform/x86/intel_telemetry_debugfs.c
[...]
drivers/infiniband/hw/qib/qib_debugfs.c
drivers/infiniband/hw/hfi1/debugfs.c
[...]
drivers/media/usb/uvc/uvc_debugfs.c
drivers/acpi/debugfs.c
drivers/net/wireless/mediatek/mt76/debugfs.c
[...]
drivers/net/wireless/intel/iwlwifi/mvm/debugfs-vif.c
drivers/net/wimax/i2400m/debugfs.c
drivers/net/ethernet/broadcom/bnxt/bnxt_debugfs.c
drivers/net/ethernet/marvell/mvpp2/mvpp2_debugfs.c
drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
[...]
drivers/misc/genwqe/card_debugfs.c
drivers/misc/mei/debugfs.c
drivers/misc/cxl/debugfs.c
[...]
drivers/usb/mtu3/mtu3_debugfs.c
drivers/sh/intc/virq-debugfs.c
drivers/soundwire/debugfs.c
[...]
drivers/crypto/ccree/cc_debugfs.c
```

看一看(其中的)一些；他们的代码公开了 debugfs 接口。这并不总是仅仅出于调试的目的；许多 debugfs 文件是为实际生产使用的！例如，MMC 驱动程序包含以下代码行，它利用 debugfs“helper”API 来获取 x32 全局:

```sh
drivers/mmc/core/debugfs.c:mmc_add_card_debugfs():
debugfs_create_x32("state", S_IRUSR, root, &card->state);
```

这会创建一个名为`state`的 debugfs 文件，当读取该文件时，会显示卡的“状态”。

好了，这就完成了我们对如何通过强大的 debugfs 框架与用户空间交互的介绍。我们的演示 debugfs 驱动程序在其中创建了一个 debugfs 目录和两个 debugfs 伪文件；然后，您学习了如何为它们设置和使用读写回调处理程序。“捷径”API(如`debugfs_create_u32()`和好友)也很强大。不仅如此，我们甚至设法生成了一个内核错误——哎呀！现在，让我们学习如何通过一种特殊类型的套接字进行通信，这种套接字被称为 netlink socket。

# 通过网络连接插座接口

在这里，你将学会用一个熟悉的、实际上无处不在的网络抽象——套接字来连接内核和用户空间。熟悉网络应用程序编程的程序员都相信它的优点。

Familiarity with network programming in C/C++ with socket APIs helps here. Do see the *Further reading* section for a couple of good tutorials on this topic.

## 使用插座的优点

其中，套接字技术为我们提供了几个优势(优于其他典型的用户模式 IPC 机制，如管道、SysV IPC/POSIX IPC 机制(消息队列、共享内存、信号量等))，如下所示:

*   双向同时数据传输(全双工)。
*   在互联网上是无损的，至少使用一些传输层协议，比如 TCP，当然，在本地主机上也是如此。
*   高速数据传输，尤其是在本地主机上！
*   流控制语义总是有效的。
*   异步通信；消息可以排队，因此发送者不必等待接收者。
*   特别是关于我们的主题，在其他用户内核通信路径(如 procfs、sysfs、debugfs、ioctl)中，用户空间 app 必须发起到内核空间的转移；使用 netlink 套接字，*内核可以发起传输。*
*   此外，对于我们到目前为止看到的所有其他机制(procfs、sysfs 和 debugfs)，散布在文件系统中的各种接口文件会导致内核名称空间污染；对于 netlink 套接字(顺便说一句，对于 ioctl)，情况并非如此，因为没有文件。

这些优势可能会有所帮助，具体取决于您正在开发的产品类型。现在，让我们理解什么是网络连接套接字。

## 了解什么是网络连接套接字

那么，什么是网络连接插座呢？我们将保持简单——一个*网络连接插座*是一个“特殊”的插座家族，从 2.2 版本开始只存在于 Linux 操作系统上。使用它，您可以在用户模式进程(或线程)和内核中的组件之间建立**进程间通信**(**IPC**)；在我们的例子中，内核模块，通常是驱动程序。

它在许多方面类似于 UNIX 域数据报套接字；它只用于在*本地主机* *上进行通信，而不是跨系统通信。尽管 UNIX 域套接字使用路径名作为其命名空间(一个特殊的“套接字”文件)，但 netlink 套接字使用 PID。迂腐地说，这是一个端口标识，而不是进程标识，尽管实际上，进程标识经常被用作名称空间。现代内核核心(除了驱动程序之外)在许多情况下使用 netlink 套接字，例如，iproute2 网络实用程序使用它来配置无线驱动程序。作为另一个有趣的例子，udev 特性使用 netlink 套接字来实现内核 udev 实现和用户空间守护进程(udevd 或 systemd-udevd，用于设备发现、设备节点供应等)之间的通信。*

在这里，我们将使用 netlink 套接字设计并实现一个简单的用户内核消息传递演示。为此，我们必须(至少)编写两个程序——一个作为发出基于套接字的系统调用的用户空间应用程序，另一个用于内核空间组件(这里是内核模块)。我们将让用户空间进程向内核模块发送一条“消息”；内核模块应该接收并打印它(到内核日志缓冲区中)。内核模块然后将回复用户空间进程，这个进程正在阻止这个事件。

因此，不再赘述，让我们开始使用 netlink 套接字编写一些代码；我们将从用户空间应用程序开始。继续读！

## 编写用户空间 netlink 套接字应用程序

按照以下步骤运行*用户空间*应用程序:

1.  我们必须做的第一件事就是给自己买一个*插座*。传统上，套接字被定义为通信的端点；因此，一对插座形成连接。我们将使用`socket(2)`系统调用来做到这一点。它的签名是
    `int socket(int domain, int type, int protocol);`。

在不太详细的情况下，我们这样做:

2.  下一步是通过通常的`bind(2)`系统调用语义绑定套接字。首先，我们必须为此目的初始化一个网络链接源`socketaddr`结构(其中我们将系列指定为网络链接，将 PID 值指定为调用进程的 PID(仅适用于单播))。下面的代码用于这里提到的前两个步骤(为了清楚起见，我们不会在这里显示错误检查代码):

```sh
// ch2/netlink_simple_intf/userapp_netlink/netlink_userapp.c
#define NETLINK_MY_UNIT_PROTO        31
    // kernel netlink protocol # (registered by our kernel module)
#define NLSPACE 1024

[...] 
 /* 1\. Get ourselves an endpoint - a netlink socket! */
sd = socket(PF_NETLINK, SOCK_RAW, NETLINK_MY_UNIT_PROTO);
printf("%s:PID %d: netlink socket created\n", argv[0], getpid());

/* 2\. Setup the netlink source addr structure and bind it */
memset(&src_nl, 0, sizeof(src_nl));
src_nl.nl_family = AF_NETLINK;
/* Note carefully: nl_pid is NOT necessarily the PID of the sender process; it's actually 'port id' and can be any unique number */
src_nl.nl_pid = getpid();
src_nl.nl_groups = 0x0; // no multicast
bind(sd, (struct sockaddr *)&src_nl, sizeof(src_nl))
```

3.  接下来，我们必须初始化一个网络链接“目的地址”结构。这里，我们将 PID 成员设置为`0`，这是一个特殊的值，表示目的地是内核:

```sh
/* 3\. Setup the netlink destination addr structure */
memset(&dest_nl, 0, sizeof(dest_nl));
dest_nl.nl_family = AF_NETLINK;
dest_nl.nl_groups = 0x0; // no multicast
dest_nl.nl_pid = 0;      // destined for the kernel
```

4.  接下来，我们必须分配并初始化一个网络链接“头”数据结构。其中，它指定了源 PID，更重要的是，我们将交付给内核组件的数据“有效载荷”。在这里，我们使用辅助宏，如`NLMSG_DATA()`来指定网络链接头结构中的正确数据位置:

```sh
/* 4\. Allocate and setup the netlink header (including the payload) */
nlhdr = (struct nlmsghdr *)malloc(NLMSG_SPACE(NLSPACE));
memset(nlhdr, 0, NLMSG_SPACE(NLSPACE));
nlhdr->nlmsg_len = NLMSG_SPACE(NLSPACE);
nlhdr->nlmsg_pid = getpid();
/* Setup the payload to transmit */
strncpy(NLMSG_DATA(nlhdr), thedata, strlen(thedata)+1);
```

5.  接下来，必须初始化一个`iovec`结构来引用网络链接头，并且必须初始化一个`msghdr`数据结构来指向目的地址和`iovec`:

```sh
/* 5\. Setup the iovec and ... */
memset(&iov, 0, sizeof(struct iovec));
iov.iov_base = (void *)nlhdr;
iov.iov_len = nlhdr->nlmsg_len;
[...]
/* ... now setup the message header structure */
memset(&msg, 0, sizeof(struct msghdr));
msg.msg_name = (void *)&dest_nl;   // dest addr
msg.msg_namelen = sizeof(dest_nl); // size of dest addr
msg.msg_iov = &iov;
msg.msg_iovlen = 1; // # elements in msg_iov
```

6.  最后，通过`sendmsg(2)`系统调用(以套接字描述符和前述`msghdr`结构为参数)发送(传输)消息:

```sh
/* 6\. Actually (finally!) send the message via sendmsg(2) */
nsent = sendmsg(sd, &msg, 0);
```

7.  内核组件——内核模块，我们稍后将讨论——现在应该通过它的 netlink 套接字接收消息并显示消息的内容；我们安排它然后礼貌地回答。要获取回复，我们的用户空间应用程序现在必须在套接字上执行阻塞读取:

```sh
/* 7\. Block on incoming msg from the kernel-space netlink component */
printf("%s: now blocking on kernel netlink msg via recvmsg() ...\n", argv[0]);
nrecv = recvmsg(sd, &msg, 0);
```

我们必须使用`recvmsg(2)`系统调用来做到这一点。当它被解除阻止时，它表明消息已被接收。

Why so much abstraction and wrapping for data structures? Well, it's how things often evolve – the `msghdr` structure was created so that the `sendmsg(2)` API can use fewer parameters. But that implies the parameters have to go somewhere; they go deep inside `msghdr`, which points to the destination address and `iovec`, whose `base` member points to the netlink header structure, which contains the payload! Whew.

作为一个实验，如果我们过早地构建和运行用户模式 netlink 应用程序–*而没有*内核端代码，会怎么样？当然，它会失败...但是具体怎么做呢？好吧，用经验方法。通过古老的`strace(1)`实用程序尝试这一点，我们可以看到`socket(2)`系统调用返回了一个失败，原因是`Protocol not supported`:

```sh
$ strace -e trace=network ./netlink_userapp
socket(AF_NETLINK, SOCK_RAW, 0x1f /* NETLINK_??? */) = -1 EPROTONOSUPPORT (Protocol not supported)
netlink_u: netlink socket creation failed: Protocol not supported
+++ exited with 1 +++
$
```

这是正确的；内核中还没有这样的`protocol # 31` ( `31` = `0x1f`，我们正在使用的协议号)到位*！我们还没有这么做。这就是用户空间的一面。现在，让我们完成这个难题，让它真正发挥作用！我们将通过查看内核组件(模块/驱动程序)是如何编写的来实现这一点。*

 *## 将内核空间 netlink 套接字代码编写为内核模块

内核为 netlink 提供基础架构，包括 API 和数据结构；所有必需的都被导出，因此作为模块作者，您可以使用它们。我们使用其中的几种；这里概述了对内核 netlink 组件(内核模块)进行编程的步骤:

1.  就像用户空间应用程序一样，我们必须做的第一件事就是给自己买一个 netlink 套接字。内核 API 为`netlink_kernel_create()`，其签名如下:

```sh
struct sock * netlink_kernel_create(struct net *, int , struct netlink_kernel_cfg *);
```

第一个参数是通用网络结构；我们在这里传递内核现有且有效的`init_net`结构。第二个参数是*要使用的协议号(单位)*；我们将指定与用户空间应用程序相同的编号(`31`)。第三个参数是指向(可选的)netlink 配置结构的指针；这里，我们只将输入成员设置为我们的函数，而不考虑其余的。当用户空间进程(或线程)向内核 netlink 组件提供任何输入(即传输某些东西)时，这个函数被回调。因此，在内核模块的`init`例程中，我们有以下内容:

```sh
// ch2/netlink_simple_intf/kernelspace_netlink/netlink_simple_intf.c
#define OURMODNAME               "netlink_simple_intf"
#define NETLINK_MY_UNIT_PROTO    31 
    // kernel netlink protocol # that we're registering
static struct sock *nlsock;
[...]
static struct netlink_kernel_cfg nl_kernel_cfg = { 
    .input = netlink_recv_and_reply,
};
[...]
nlsock = netlink_kernel_create(&init_net, NETLINK_MY_UNIT_PROTO,
            &nl_kernel_cfg);
```

2.  正如我们前面提到的，当用户空间进程(或线程)向我们的内核(netlink)模块或驱动程序提供任何输入(即传输某些东西)时，回调函数就会被调用。重要的是要理解它运行在进程上下文中，而不是任何类型的中断上下文中；我们使用`convenient.h:PRINT_CTX()`宏来验证这一点(我们将在[第 4 章](4.html)、*处理硬件中断*、在*完全理解上下文*部分中讨论这一点)。在这里，我们只需显示收到的消息，然后通过向我们的用户空间对等进程发送示例消息来进行回复。从我们的用户空间对等进程传输的数据有效负载可以从套接字缓冲区结构中检索，该缓冲区结构作为参数传递给我们的回调函数，或者从其中的 netlink 头结构中检索。您可以在此处看到数据和发送方 PID 是如何检索的:

```sh
static void netlink_recv_and_reply(struct sk_buff *skb)
{
    struct nlmsghdr *nlh;
    struct sk_buff *skb_tx;
    char *reply = "Reply from kernel netlink";
    int pid, msgsz, stat;

    /* Find that this code runs in process context, the process
     * (or thread) being the one that issued the sendmsg(2) */
    PRINT_CTX();

    nlh = (struct nlmsghdr *)skb->data;
    pid = nlh->nlmsg_pid; /*pid of sending process */
    pr_info("%s: received from PID %d:\n"
        "\"%s\"\n", OURMODNAME, pid, (char *)NLMSG_DATA(nlh));
```

The *socket buffer* data structure – `struct sk_buff` – is considered the critical data structure within the Linux kernel's network protocol stack. It holds all metadata concerning the network packet, including dynamic pointers to it. It has to be quickly allocated and freed (especially when network code runs in interrupt contexts); this is indeed possible because it's on the kernel's slab (SLUB) cache (see details on the kernel slab allocator in the companion guide *Linux Kernel Programming,* *Chapters 7*, *Memory Management Internals - Essentials*, *Chapter 8*, *Kernel Memory Allocation for Module Authors – Part 1*, and *Chapter 9*, *Kernel Memory Allocation for Module Authors – Part 2*).

现在，我们需要理解，我们可以通过首先取消对传递给我们回调例程的套接字缓冲区(`skb`)结构的`data`成员的引用来从网络数据包中检索有效负载！接下来，这个`data`成员实际上是指向我们的用户空间对等体建立的网络链接消息头结构的指针。然后我们解引用它来获得实际的有效载荷。

3.  我们现在想“回复”我们的用户空间对等进程；这样做需要执行一些操作。首先，我们必须使用`nlmsg_new()` API 分配一个新的网络链接消息，这实际上是对`alloc_skb()`的一个薄包装，通过`nlmsg_put()` API 向刚刚分配的套接字缓冲区添加一个网络链接消息，然后使用适当的宏(`nlmsg_data()`)将数据(有效负载)复制到网络链接头中:

```sh
    //--- Let's be polite and reply
    msgsz = strlen(reply);
    skb_tx = nlmsg_new(msgsz, 0);
    [...]
    // Setup the payload
    nlh = nlmsg_put(skb_tx, 0, 0, NLMSG_DONE, msgsz, 0);
    NETLINK_CB(skb_tx).dst_group = 0; /* unicast only (cb is the
        * skb's control buffer), dest group 0 => unicast */
    strncpy(nlmsg_data(nlh), reply, msgsz);
```

4.  我们通过`nlmsg_unicast()`应用编程接口将回复发送给我们的用户空间对等进程(甚至多播网络链接消息也是可能的):

```sh
    // Send it
    stat = nlmsg_unicast(nlsock, skb_tx, pid);
```

5.  只剩下清理(当内核模块被移除时调用)；`netlink_kernel_release()` API 实际上是`netlink_kernel_create()`的反义词，因为它清理了网络链接套接字，并将其关闭:

```sh
static void __exit netlink_simple_intf_exit(void)
{
    netlink_kernel_release(nlsock);
    pr_info("%s: removed\n", OURMODNAME);
}
```

现在，我们已经编写了用户空间应用程序和内核模块，通过 netlink 套接字进行接口，让我们实际尝试一下！

## 尝试我们的网络连接项目

是时候验证它是否如广告中所说的那样工作了。让我们开始吧:

1.  首先，构建内核模块并将其插入内核内存:

Our `lkm` convenience script makes short work of this; this session was carried out on our familiar x86_64 guest VM running Ubuntu 18.04 LTS and a custom 5.4.0 Linux kernel.

```sh
$ cd <booksrc>/ch2/netlink_simple_intf/kernelspace_netlink $ ../../../lkm netlink_simple_intf
Version info:
Distro:     Ubuntu 18.04.4 LTS
Kernel: 5.4.0-llkd01
[...]
make || exit 1
[...] Building for: KREL=5.4.0-llkd01 ARCH=x86 CROSS_COMPILE= EXTRA_CFLAGS= -DDEBUG
  CC [M]  /home/llkd/booksrc/ch13/netlink_simple_intf/kernelspace_netlink/netlink_simple_intf.o
[...]
sudo insmod ./netlink_simple_intf.ko && lsmod|grep netlink_simple_intf
------------------------------
netlink_simple_intf    16384  0
[...]
[58155.082713] netlink_simple_intf: creating kernel netlink socket
[58155.084445] netlink_simple_intf: inserted
$ 
```

2.  有了它，它就装载好了。接下来，我们将构建并试用我们的用户空间应用程序:

```sh
$ cd ../userapp_netlink/
$ make netlink_userapp
[...] 
```

这将产生以下输出:

![](assets/ce358d89-c70a-4d5b-8804-df86245ce2b1.png)

Figure 2.5 – Screenshot showing user<->kernel communication via our sample netlink socket code

它有效；内核网络链接模块接收并显示从用户空间进程(`PID 7813`)发送给它的消息。内核模块然后用自己的消息回复给它的用户空间对等体，后者成功地接收并显示它(通过`printf()`)。自己试一试。完成后，别忘了用`sudo rmmod netlink_simple_intf`移除内核模块。

An aside: a connector driver exists within the kernel. Its purpose is to ease the development of netlink-based communication, making it simpler for both kernel and user space developers set up and use a netlink-based communication interface. We will not delve into this here; please refer to the documentation within the kernel ([https://elixir.bootlin.com/linux/v5.4/source/Documentation/driver-api/connector.rst](https://elixir.bootlin.com/linux/v5.4/source/Documentation/driver-api/connector.rst)). Some sample code is also provided within the kernel source tree (at `samples/connector`).

至此，您已经学会了如何通过强大的 netlink socket 机制在用户模式应用程序和内核组件之间进行交互。正如我们前面提到的，它在内核树中有几个实际的用例。现在，让我们继续，通过流行的`ioctl(2)`系统调用，介绍另一种用户内核接口方法。

# 通过 ioctl 系统调用接口

**ioctl** 是系统调用；为什么这个有趣的名字叫 *ioctl* ？是**输入输出控制**的缩写。而读写系统调用(以及其他)用于有效地将*数据*从设备(或文件；记住 UNIX 范式*如果不是进程，就是文件！*)*ioctl*系统调用用于*向设备发出* *命令*(通过其驱动程序)。例如，更改控制台设备的终端特征、格式化磁盘时将磁道写入磁盘、向步进电机发送控制命令、控制摄像机或音频设备等，都是命令发送到设备的实例。

让我们考虑一个虚构的例子。我们有一个设备，正在为它开发(字符)设备驱动程序。该设备有各种*寄存器*，设备上的小型硬件存储器通常为 8 位、16 位或 32 位，其中一些是控制寄存器。通过在它们上适当地执行 I/O(读和写)，我们控制了设备(嗯，这确实是关键，不是吗；关于使用包括设备寄存器在内的硬件存储器的细节的实际主题将在下一章中涉及)。那么，作为驱动程序作者，您将如何与想要在该设备上执行各种控制操作的用户空间程序进行通信或交互？我们通常设计用户空间 C(或 C++)程序来打开设备，通常是对其设备文件执行`open(2)`，然后发出读写系统调用。

但是，正如我们刚才提到的，*在传输* *数据*时，`read(2)`和`write(2)`系统调用 API 是合适的，而在这里，我们打算执行**控制操作**。因此，我们需要另一个系统调用来实现...那么我们需要创建和编码一个新的系统调用(或多个调用)吗？不，比这简单得多:我们通过 *ioctl 系统调用进行*多路复用，*利用它在我们的设备上执行任何所需的控制操作！怎么做？啊，回想一下前一章至关重要的`file_operations` (fops)数据结构；我们现在将另一个成员`.ioctl`初始化为我们的 ioctl 方法函数，从而允许我们的设备驱动程序连接到这个系统调用:*

```sh
static struct file_operations ioct_intf_fops = { 
    .llseek = no_llseek,
    .ioctl = ioct_intf_ioctl,
    [...]
};
```

实际上，我们必须弄清楚我们是应该使用`ioctl`还是`file_operations`结构的`unlocked_ioctl`成员，这取决于该模块是运行在 Linux 内核版本 2.6.36 还是更高版本上；关于这一点的更多内容如下。

In fact, adding new system calls to the kernel is not something you should do lightly! The kernel chaps are *not* open to arbitrarily adding syscalls – it's a security-sensitive interface, after all. More on this is documented here: [https://www.kernel.org/doc/html/latest/kernel-hacking/hacking.html#ioctls-not-writing-a-new-system-call](https://www.kernel.org/doc/html/latest/kernel-hacking/hacking.html#ioctls-not-writing-a-new-system-call).

关于使用 ioctl 进行接口的更多信息如下。

## 在用户和内核空间中使用 ioctl

`ioctl(2)`系统调用的签名如下:

```sh
#include <sys/ioctl.h>
int ioctl(int fd, unsigned long request, ...);
```

参数列表是*varargs–变量参数–*一个。实际上，通常我们会传递两个或三个参数:

*   第一个参数是显而易见的——打开的(在我们的例子中)设备文件的文件描述符。
*   第二个参数叫做`request`，是有趣的一个:它是要传递给司机的命令。实际上，它是一个编码，封装了一个所谓的 ioctl 幻数:一个数字和一个类型(读/写)。
*   (可选)第三个参数，常称为`arg`，也是`unsigned long`量；我们使用它或者以通常的方式将一些数据传递给底层驱动程序，或者通常通过传递其(虚拟)地址并让内核写入其中，利用 C 所谓的**值-结果**或**进-出**参数样式，将数据返回给用户空间。

现在，正确使用 ioctl 并不像使用许多其他 API 那样微不足道。想一想:你很容易会有这样一个场景:几个用户空间应用程序正在向它们的底层设备驱动程序发出`ioctl(2)`系统调用(发出各种命令)。一个问题变得显而易见:内核 VFS 层将如何将 ioctl 请求导向正确的驱动程序？ioctl 通常在具有唯一*(大调，小调)*号的 char 设备文件上执行；因此，另一个驱动程序如何接收您的 ioctl 命令(除非您有意地，也许是恶意地，以这种方式设置设备文件)？

然而，存在一个协议来实现 ioctl 的安全和正确使用；每个应用程序和驱动程序都定义了一个幻数，这个幻数将被编码到它的所有 ioctl 请求中。首先，驱动程序会验证它收到的每个 ioctl 请求都包含*它的*幻数；只有这样，它才会继续处理它；否则，它将干脆放弃它。当然，这就需要一个*ABI*——我们需要给每个“注册”的司机分配唯一的魔法号码(可能是一个范围)。因为这创建了一个 ABI，所以内核文档将是相同的；你可以在这里找到谁在使用哪个神奇数字(或代码)的详细信息:[https://www . kernel . org/doc/Documentation/ioctl/ioctl-number . txt](https://www.kernel.org/doc/Documentation/ioctl/ioctl-number.txt)。

接下来，对底层驱动程序的 ioctl 请求可以是以下四种情况之一:向设备“写入”的命令、从设备“读取”(或查询)的命令、同时执行读/写传输的命令，或者两者都不执行。该信息通过定义某些位被(再次)*编码为请求，以传达以下含义:为了使这项工作更容易，我们有四个帮助宏，允许我们构造 ioctl 命令:*

*   `_IO(type,nr)`:编码一个没有参数的 ioctl 命令
*   `_IO**R**(type,nr,datatype)`:编码 ioctl 命令，用于从内核/驱动程序读取数据
*   `_IO**W**(type,nr,datatype)`:编码 ioctl 命令，用于向内核/驱动程序写入数据
*   `_IO**WR**(type,nr,datatype)`:为读/写传输编码 ioctl 命令

这些宏在用户空间`<sys/ioctl.h>`头和内核`include/uapi/asm-generic/ioctl.h`中定义。典型的(也是非常明显的)最佳实践是创建一个*公共头*文件，该文件为应用程序/驱动程序定义 ioctl 命令，并将该文件包含在用户模式应用程序和设备驱动程序中。

在这里，作为演示，我们将设计并实现一个用户空间应用程序和一个内核空间设备驱动程序来驱动一个虚构的设备，该设备通过`ioctl(2)`系统调用进行通信。因此，我们必须定义一些通过 *ioctl* 接口发出的命令。我们将在一个公共头文件中这样做，如下所示:

```sh
// ch2/ioctl_intf/ioctl_llkd.h

/* The 'magic' number for our driver; see Documentation/ioctl/ioctl-number.rst 
 * Of course, we don't know for _sure_ if the magic # we choose here this
 * will remain free; it really doesn't matter, this is just for demo purposes;
 * don't try and upstream this without further investigation :-)
 */
#define IOCTL_LLKD_MAGIC        0xA8

#define IOCTL_LLKD_MAXIOCTL        3
/* our dummy ioctl (IOC) RESET command */
#define IOCTL_LLKD_IOCRESET     _IO(IOCTL_LLKD_MAGIC, 0)
/* our dummy ioctl (IOC) Query POWER command */
#define IOCTL_LLKD_IOCQPOWER    _IOR(IOCTL_LLKD_MAGIC, 1, int)
/* our dummy ioctl (IOC) Set POWER command */
#define IOCTL_LLKD_IOCSPOWER    _IOW(IOCTL_LLKD_MAGIC, 2, int)
```

我们必须努力使我们在宏中使用的名称有意义。我们的三个命令(以粗体突出显示)都以`IOCTL_LLKD_`为前缀，表示它们都是我们虚构的`LLKD`项目的 ioctl 命令；接下来，它们以`IOC{Q|S}`为后缀，`IOC`表示这是 ioctl 命令，`Q`表示这是查询操作，`S`表示这是 set 操作。

现在，让我们学习如何从用户空间和内核空间(驱动程序)两个方面在代码级别进行设置。

### 用户空间–使用 ioctl 系统调用

*系统调用的*用户空间签名如下:

```sh
#include <sys/ioctl.h>
int ioctl(int fd, unsigned long request, ...);
```

在这里，我们可以看到它需要一个变量参数列表；ioctl 的参数如下:

*   **第一个参数**:要对其执行 ioctl 操作的文件或设备的文件描述符(我们通过对设备文件执行*打开*得到`fd`)。
*   **第二个参数**:向底层设备驱动程序(或文件系统或`fd`代表的任何东西)发出的请求或命令。
*   **可选的第三个(或多个)参数**:通常，第三个参数是整数(或指向整数或数据结构的指针)；我们使用这种方法或者在发出 *set* 类命令时向驱动程序传递一些额外的信息，或者通过众所周知的*传递引用* C 范式从驱动程序中检索一些信息，在该范式中，我们传递指针并让驱动程序“戳”它，从而实际上将参数视为返回值。

In effect, ioctl is often used as a *generic* system call. The use of ioctl to perform command operations on both hardware and software is almost embarrassingly large! Please refer to the kernel documentation (`Documentation/ioctl/<...>`) to see many actual real-world examples. For example, you will find details on who is using which magic number (or code) within ioctl here: [https://www.kernel.org/doc/Documentation/ioctl/ioctl-number.txt](https://www.kernel.org/doc/Documentation/ioctl/ioctl-number.txt).
(Similarly, the `ioctl_list(2)` man page reveals the complete list of ioctl calls in the x86 kernel; these documentation files seem to be pretty old, though. The docs now seem to be here: [https://github.com/torvalds/linux/tree/master/Documentation/userspace-api/ioctl](https://github.com/torvalds/linux/tree/master/Documentation/userspace-api/ioctl).)

让我们来看看用户空间 C 应用的一些片段，特别是在发布`ioctl(2)`系统调用时(为了简洁和可读性，我们省略了错误检查代码；完整的代码可在本书的 GitHub 存储库中获得):

```sh
// ch2/ioctl_intf/user space_ioctl/ioctl_llkd_userspace.c
#include "../ioctl_llkd.h"
[...]
ioctl(fd, IOCTL_LLKD_IOCRESET, 0);   // 1\. reset the device
ioctl(fd, IOCTL_LLKD_IOCQPOWER, &power); // 2\. query the 'power status'

// 3\. Toggle it's power status
if (0 == power) {
        printf("%s: Device OFF, powering it On now ...\n", argv[0]);
        if (ioctl(fd, IOCTL_LLKD_IOCSPOWER, 1) == -1) { [...]
        printf("%s: power is ON now.\n", argv[0]);
    } else if (1 == power) {
        printf("%s: Device ON, powering it OFF in 3s ...\n", argv[0]);
        sleep(3); /* yes, careful here of sleep & signals! */
        if (ioctl(fd, IOCTL_LLKD_IOCSPOWER, 0) == -1) { [...]
        printf("%s: power OFF ok, exiting..\n", argv[0]);
    }
[...]
```

我们的驱动程序如何处理这些用户空间发布的 ioctls？我们来看看。

### 内核空间–使用 ioctl 系统调用

在前一节中，我们看到内核驱动程序必须初始化其`file_operations`结构，以包含`ioctl`方法。不过，这还不止于此:Linux 内核一直在进化；在早期的内核版本中，开发人员使用了一种非常粗粒度的锁，尽管它有效，但却严重损害了它的性能(我们将在[第 6 章](6.html)、*内核同步-第 1 部分*和[第 7 章](7.html)、*内核同步-第 2 部分*中详细讨论锁定)。太差了，被戏称为**大仁锁** ( **BKL** )！好消息是，到了内核版本 2.6.36，开发人员摆脱了这个臭名昭著的锁。然而，这样做也有一些副作用:其中之一是，在内核中发送到 ioctl 方法的参数数量，也就是在我们的`file_operations`数据结构中，随着更新的方法(命名为`unlocked_ioctl`)从 4 个变成了 3 个。因此，对于我们的演示驱动程序，在初始化驱动程序的`file_operations`结构时，我们将使用以下内容初始化 *ioctl* 方法:

```sh
// ch2/ioctl_intf/kerneldrv_ioctl/ioctl_llkd_kdrv.c
#include "../ioctl_llkd.h"
#include <linux/version.h>
[...]
static struct file_operations ioctl_intf_fops = { 
    .llseek = no_llseek,
#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 36)
    .unlocked_ioctl = ioctl_intf_ioctl, // use the 'unlocked' version
#else
    .ioctl = ioctl_intf_ioctl, // 'old' way
#endif
};
```

显然，正如它在 fops 驱动程序中定义的那样，ioctl 被认为是一个私有驱动程序接口(`driver-private`)。此外，在驱动程序代码的函数定义中，必须考虑到与较新的“解锁”版本相关的相同事实；我们的司机是这样做的:

```sh
#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 36)
static long ioctl_intf_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
#else
static int ioctl_intf_ioctl(struct inode *ino, struct file *filp, unsigned int cmd, unsigned long arg)
#endif
{
[...]
```

这里的关键代码是驱动程序的 ioctl 方法。想想看:一旦完成了基本的有效性检查，驱动程序真正要做的就是对用户空间应用程序发出的所有可能有效的 ioctl 命令执行*开关盒*。让我们看一下下面的代码(为了可读性，我们将跳过`#if LINUX_VERSION_CODE >= ...`宏指令，只显示现代 ioctl 函数签名，以及一些有效性检查；您可以在本书的 GitHub 存储库中查看完整的代码):

```sh
static long ioctl_intf_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
    int retval = 0;
    pr_debug("In ioctl method, cmd=%d\n", _IOC_NR(cmd));

    /* Verify stuff: is the ioctl's for us? etc.. */
    [...]

    switch (cmd) {
    case IOCTL_LLKD_IOCRESET:
        pr_debug("In ioctl cmd option: IOCTL_LLKD_IOCRESET\n");
        /* ... Insert the code here to write to a control register to reset  
           the device ... */
        break;
    case IOCTL_LLKD_IOCQPOWER:  /* Get: arg is pointer to result */
        pr_debug("In ioctl cmd option: IOCTL_LLKD_IOCQPOWER\n"
            "arg=0x%x (drv) power=%d\n", (unsigned int)arg, power);
        if (!capable(CAP_SYS_ADMIN))
            return -EPERM;
        /* ... Insert the code here to read a status register to query the
         * power state of the device ... * here, imagine we've done that 
         * and placed it into a variable 'power'
         */
        retval = __put_user(power, (int __user *)arg);
        break;
    case IOCTL_LLKD_IOCSPOWER:  /* Set: arg is the value to set */
        if (!capable(CAP_SYS_ADMIN))
            return -EPERM;
        power = arg;
        /* ... Insert the code here to write a control register to set the
         * power state of the device ... */
        pr_debug("In ioctl cmd option: IOCTL_LLKD_IOCSPOWER\n"
            "power=%d now.\n", power);
        break;
    default:
        return -ENOTTY;
    }
[...]
```

`_IOC_NR`宏用于从`cmd`参数中提取命令号。在这里，我们可以看到驾驶员对通过用户空间过程发出的`ioctl`的三种有效情况做出“反应”:

*   接收到`IOCTL_LLKD_IOC**RESET**`命令后，执行设备复位。
*   当接收到`IOCTL_LLKD_IOC**Q**POWER`命令时，它查询(`Q`进行查询)并返回当前电源状态(通过使用*值-结果* C 编程方法将其值戳入第三个参数`arg`)。
*   当接收到`IOCTL_LLKD_IOC**S**POWER`命令时，它设置(`S`用于设置)电源状态(为第三个参数`arg`中传递的值)。

当然，由于我们使用的是一个纯粹虚构的设备，我们的驱动程序实际上并不执行任何注册(或其他硬件)工作。这个驱动程序只是一个你可以利用的模板。

如果黑客试图在一次(相当笨拙的)黑客攻击中发布一个我们的驱动程序不知道的命令会怎么样？最初的有效性检查会抓住它；即使他们不这样做，我们也会在我们的 *ioctl* 方法中遇到`default`的情况，导致驾驶员将`-ENOTTY`返回到用户空间。这将通过 glibc“glue”代码，将用户空间进程(或线程)的`errno`值设置为`ENOTTY`，通知它 ioctl 方法不能被服务。我们的用户空间`perror(3)`应用编程接口将显示`Inappropriate ioctl for device`错误信息。事实上，如果驱动程序没有*ioctl 方法(也就是说，如果`file_operations`结构内的 ioctl 成员设置为`NULL`)，并且用户空间应用程序对其发出`ioctl`方法，就会出现这种情况。*

 *我留给你来尝试这个用户空间/驱动程序项目的例子；为了方便起见，一旦加载了驱动程序(通过 insmod)，就可以使用`ch2/userspace_ioctl/cr8devnode.sh`方便脚本生成设备文件。设置好之后，运行用户空间应用程序；你会发现连续运行它会使我们虚构的设备的“电源状态”反复切换。

## ioctl 作为调试接口

正如我们在本章开头提到的，使用 *ioctl* 接口进行调试怎么样？它可以用于此目的。您可以随时将“调试”命令插入到*开关盒*块中；它可以用来向用户空间应用程序提供关于驱动程序状态、关键变量的值(健康监控)等有用的信息。

不仅如此，除非明确记录给最终用户或客户，否则通过 ioctl 接口使用的精确命令是未知的；因此，您需要记录界面，同时为其他团队或客户提供足够的细节，以充分利用它们。这就引出了一个有趣的问题:您可能会选择故意不记录某个 ioctl 命令；现在这是一个“隐藏”命令，例如，现场工程师可以使用它来检查设备。(我把做这件事作为一项任务留给你。)

The kernel documentation on ioctl includes this file: [https://www.kernel.org/doc/Documentation/ioctl/botching-up-ioctls.txt](https://www.kernel.org/doc/Documentation/ioctl/botching-up-ioctls.txt). Though biased toward kernel graphics stack devs, it describes typical design mistakes, trade-offs, and more.

太棒了——你快完成了！您已经学习了如何通过各种技术将内核模块或驱动程序与用户模式进程或线程(在用户空间应用程序中)接口。我们从 procfs 开始，然后继续使用 sysfs 和 debugfs。netlink 套接字和 ioctl 系统调用完成了我们对这些接口方法的研究。

但是有了这些选择，你应该在项目中使用哪一个呢？下一节将通过快速比较这些不同的接口方法来帮助您做出决定。

# 比较接口方法-表格

在本节中，我们根据几个参数创建了本章中描述的各种用户内核接口方法的快速对照表:

| **参数/接口方法** | procfs | **sysfs** | 调试 | **网联插座** | **ioctl** |
| **易开发性** | 易于学习和使用。 | (相对)易学易用。 | (非常)易学易用。 | 更难；得写用户空间 C +驱动代码+懂套接字 API。 | 公平/更难；得写用户空间 c++驱动代码。 |
| **适合什么用途** | 核心内核*只有*(少数老一点的驱动可能还在用)；司机最好避开。 | 设备驱动接口。 | 用于生产和调试目的的驱动程序(和其他)接口。 | 用户包括设备驱动程序、核心网络代码、udev 系统等等。 | 设备驱动程序接口主要(包括许多)。 |
| **界面可见性** | 对所有人可见；使用权限来控制访问。 | 对所有人可见；使用权限来控制访问。 | 对所有人可见；使用权限来控制访问。 | 对文件系统隐藏；不会污染内核命名空间。 | 对文件系统隐藏；不会污染内核命名空间。 |
| **驱动程序/模块作者的上游内核 ABI *** | 主线不推荐在驱动程序中使用。 | “正确的方式”；将驱动程序与用户空间接口的正式认可的方法。 | 驱动程序和其他产品很好地支持并在主线中大量使用。 | 支持良好(从 2.2 开始)。 | 支持得很好。 |
| **用于(驾驶员)调试目的**  | 是的(虽然不应该在主线中)。 | 不/不理想。 | 是的，非常有用！“无规则”的设计。 | 不/不理想。 | 有；(甚至)通过隐藏命令。 |

*正如我们前面提到的，内核社区文档表明 procfs、sysfs 和 debugfs 都是*ABIs；*它们的稳定性和寿命没有保证。虽然这是社区采取的正式立场，但现实是，许多使用这些文件系统的实际接口已经成为现实世界中产品使用的实际接口。然而，我们应该遵循内核社区的“规则”和关于它们使用的指导方针。

# 摘要

在本章中，我们介绍了设备驱动程序作者的一个重要方面——如何在用户和内核(驱动程序)空间之间建立*接口。我们向您介绍了几种接口方法；我们从一个旧的开始，它是通过古老的 proc 文件系统接口的(然后提到为什么它不是驱动程序作者的首选方法)。然后我们继续通过更新的基于 2.6 的*系统接口。*这是*用户空间的首选界面，至少对于设备驱动程序来说是这样。不过，Sysfs 也有局限性(回想一下每个 sysfs 文件一个值的规则)。因此，使用完全自由格式的 *debugfs* 接口技术使得编写调试(和其他)接口变得非常简单和强大。netlink socket 是一种强大的接口技术，由网络子系统 udev 和一些驱动程序使用；不过，它确实需要一些关于套接字编程和内核套接字缓冲区的知识。为了在设备驱动程序上执行通用命令操作，ioctl 系统调用被证明是一个巨大的多路复用器，并且经常被设备驱动程序作者(和其他组件)用来与用户空间接口。**

 *有了这些知识，您现在可以将您的驱动级代码与用户空间应用程序(或脚本)进行实际集成；通常，用户模式**图形用户界面** ( **图形用户界面**)会想要显示从内核或设备驱动程序接收的一些值。您现在知道如何从内核空间设备驱动程序传递这些值了！

在下一章中，您将了解作者必须执行的一个典型任务驱动程序:使用硬件芯片内存！确保你清楚本章的材料，做提供的练习，复习*进一步阅读*资源，然后进入下一章。那里见！

# 问题

1.  `sysfs_on_misc` : *sysfs 任务#1* :扩展我们在[第 1 章](1.html)、*中编写的一个`misc`设备驱动，编写一个简单的杂项字符设备驱动*；设置两个 sysfs 文件及其读/写回调；从用户空间测试它们。

2.  `sysfs_addrxlate` : *sysfs 赋值#2(高级一点)* : *地址转换:*利用从本章和 *Linux 内核编程*一书中获得的知识，*第 7 章，内存管理内部组件-要点，**直接映射内存和地址转换*部分，编写一个简单的平台驱动程序，提供两个 sysfs 接口文件，分别称为`addrxlate_kva2pa`和`addrxlate_pa2kva`。将 kva 写入 sysfs 文件`addrxlate_kva2pa`时，应由驱动程序读取并翻译 *kva* 为其对应的**物理地址**(**pa**)；然后，从同一个文件中读取应该会导致显示 *pa* 。对`addrxlate_pa2kva` sysfs 文件执行同样的操作。
3.  `dbgfs_disp_pgoff` : *debugfs 赋值#1* :在这里写一个设置 debugfs 文件的内核模块:`<debugfs_mount_point>/dbgfs_disp_pgoff`。当读取时，它应该显示`PAGE_OFFSET`内核宏的当前值(到用户空间)。
4.  `dbgfs_showall_threads` : *调试文件分配#2* :在这里写一个设置调试文件的内核模块:`<debugfs_mount_point>/dbgfs_showall_threads/dbgfs_showall_threads`。读取时，它应该显示每个活动线程的一些属性。(这类似于我们在 *Linux 内核编程*一书中的代码:这里:[https://github . com/PacktPublishing/Linux-Kernel-Programming/tree/master/ch6/foreach/thrd _ showall](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/ch6/foreach/thrd_showall)。请注意，在 insmod 时间，线程仅显示*；有了 debugfs 文件，你可以随时显示所有线程的信息！
    *建议输出为 CSV 格式:* `TGID,PID,current,stack-start,name,#threads`。方括号中的`[name]`字段= >内核线程*；* `#threads`字段应只显示正整数*；*这里没有输出意味着单线程进程；例如:`130,130,0xffff9f8b3cd38000,0xffffc13280420000,[watchdogd]`)*

 *5.  *ioctl 赋值#1* :使用提供的`ch2/ioctl_intf/`代码作为模板，编写实现`ioctl`方法的用户空间 C 应用和内核空间(char)设备驱动。添加一个名为`IOCTL_LLKD_IOCQPGOFF`的 ioctl 命令，将`PAGE_OFFSET`的值(在内核中)返回给用户空间。
6.  `ioctl_undoc` : *ioctl 赋值#2* :使用提供的`ch2/ioctl_intf/`代码作为模板，编写实现`ioctl`方法的用户空间 C 应用和内核空间(char)设备驱动。添加一个驱动程序上下文数据结构(我们在几个例子中使用了这些)，然后分配并初始化它。现在，除了我们之前使用的三个 ioctl 命令之外，设置第四个未记录的命令(可以称之为`IOCTL_LLKD_IOCQDRVSTAT`)。当通过`ioctl(2)`从用户空间查询时，必须将驱动上下文数据结构的内容返回到用户空间；用户空间 C 应用程序必须打印出该结构中每个成员的当前内容。

You will find some of the questions answered in the book's GitHub repo: [https://github.com/PacktPublishing/Linux-Kernel-Programming-Part-2/tree/main/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming-Part-2/tree/main/solutions_to_assgn).

# 进一步阅读

您可以参考以下链接，了解本章所涵盖主题的更多信息。关于在 Linux 设备驱动程序中使用非常常见的 I2C 协议的更多信息，可以在这里找到:

*   一篇关于 I2C 协议基础的文章:*如何在 STM32F103C8T6 中使用 I2C？STM32 I2C 教程*，2020 年 3 月:[https://www . electronics hub . org/how-I2C-in-STM 32 f 103 c 8t 6/](https://www.electronicshub.org/how-to-use-i2c-in-stm32f103c8t6/)

*   内核文档:实现 I2C 设备驱动程序*****