# 六、内核同步——第一部分

任何熟悉多线程环境(或者甚至是单线程环境，其中多个进程在共享内存上工作，或者中断是可能的)中编程的开发人员都很清楚，每当两个或更多线程(通常是代码路径)可能竞争时，就需要**同步**；也就是说，他们的结局无法预测。纯代码本身从来不是问题，因为它的权限是读/执行(`r-x`)；在多个 CPU 内核上同时读取和执行代码不仅非常好和安全，而且受到鼓励(这导致了更好的吞吐量，这也是多线程是一个好主意的原因)。但是，当您处理共享可写数据时，您需要开始非常小心！

围绕并发及其控制(同步)的讨论是多种多样的，尤其是在复杂软件的背景下，如 Linux 内核(其子系统和相关区域，如设备驱动程序)，这就是我们在本书中讨论的。因此，为了方便起见，我们将把这个大主题分成两章，这一章和下一章。

在本章中，我们将涵盖以下主题:

*   关键部分、排他执行和原子性
*   Linux 内核中的并发问题
*   互斥还是自旋锁？什么时候使用哪个
*   使用互斥锁
*   使用自旋锁
*   锁定和中断

我们开始吧！

# 关键部分、排他执行和原子性

想象一下，你正在为多核系统编写软件(嗯，现在，你通常会在多核系统上工作，甚至在大多数嵌入式项目上)。正如我们在介绍中提到的，并行运行多个代码路径不仅安全，而且是可取的(为什么要花那些钱呢，对吗？).另一方面，以任何方式访问**共享可写数据**(也称为**共享状态** ) **的并发(并行和同时)代码路径是要求您保证在任何给定时间点，一次只有一个线程可以处理该数据的地方！这真的很关键；为什么呢？想想看:如果您允许多个并发代码路径在共享的可写数据上并行工作，您实际上是在自找麻烦:**数据损坏**(一种“竞争”)可能因此而发生。**

## 什么是关键部分？

可以并行执行并处理(读取和/或写入)共享可写数据(共享状态)的代码路径称为关键部分。它们需要避免并行性。识别和保护关键部分不被同时执行是正确软件的隐含要求，你——设计者/架构师/开发人员——必须处理。

关键部分是一段必须以独占方式运行的代码；也就是说，单独的(序列化的)，或者原子的；也就是说，不可分割地、不间断地完成。

通过排他，我们暗示在任何给定的时间点，一个线程正在运行关键部分的代码；出于数据安全的原因，这显然是必需的。

这个概念也提出了原子性的重要概念:单个原子操作是不可分割的。在任何现代处理器上，两个操作被认为永远是**原子**；也就是说，它们不能被中断，并将一直运行到完成:

*   单一机器语言指令的执行。
*   读取或写入处理器字长(通常为 32 或 64 位)内的对齐原始数据类型；例如，在 64 位系统上读取或写入 64 位整数保证是原子的。读取该变量的线程将永远看不到介于中间、撕裂或脏的结果；他们要么看到旧的价值，要么看到新的价值。

因此，如果您有一些处理共享(全局或静态)可写数据的代码行，在没有任何显式同步机制的情况下，它不能保证以独占方式运行。请注意，有时需要原子地运行关键部分的代码*、*以及排他地运行，但不是一直都需要。

当关键部分的代码运行在安全到睡眠的进程上下文中时(例如通过用户应用对驱动程序进行的典型文件操作(open、read、write、ioctl、mmap 等)，或者内核线程或工作队列的执行路径)，不将关键部分真正原子化可能是可以接受的。然而，当它的代码在非阻塞原子上下文中运行时(如 hardirq、小任务或 softirq)，*它必须以原子方式运行，也必须以独占方式运行*(我们将在*互斥体或自旋锁中更详细地讨论这些问题？*节时使用哪个)。

一个概念性的例子将有助于澄清事情。假设三个线程(来自用户空间应用)试图在多核系统上或多或少地同时打开和读取驱动程序。如果没有任何干预，他们很可能会并行运行关键部分的代码，从而并行处理共享的可写数据，因此很可能会破坏它！现在，让我们看一个概念图，看看关键部分代码路径中的非独占执行是如何出错的(我们甚至不会在这里讨论原子性):

![](assets/4daabc10-0ddf-4879-96c2-49eeb6aa96e3.png)

Figure 6.1 – A conceptual diagram showing how a critical section code path is violated by having >1 thread running within it simultaneously

如上图所示，在您的设备驱动程序中，在它的(比方说)read 方法中，您让它运行一些代码来执行它的工作(从硬件中读取一些数据)。让我们更深入地看一下这个图表*在不同时间点进行的数据访问*:

*   从`t0`到`t1`时间:无或仅访问局部变量数据。这是并发安全的，不需要保护，并且可以并行运行(因为每个线程都有自己的私有堆栈)。
*   从`t1`到`t2`:访问全局/静态共享可写数据。这是*不是*并发-安全；这是**的一个关键部分**，因此必须**保护**不被并发访问。它应该只包含专门运行的代码(单独运行，一次只运行一个线程，序列化)，并且可能是原子性的。
*   从`t2`到`t3`时间:无或仅访问局部变量数据。这是并发安全的，不需要保护，并且可以并行运行(因为每个线程都有自己的私有堆栈)。

In this book, we assume that you are already aware of the need to synchronize critical sections; we will not discuss this particular topic any further. Those of you who are interested may refer to my earlier book, *Hands-On System Programming with Linux (Packt, October 2018)*, which covers these points in detail (especially *Chapter 15*, *Multithreading with Pthreads Part II – Synchronization*).

因此，知道了这一点，我们现在可以重申一个关键部分的概念，同时也提到当情况出现时(显示在方括号中，斜体在项目符号中)。关键部分是必须按如下方式运行的代码:

*   **(始终)独占**:单独(连载)
*   **(当处于原子上下文中时)原子地**:不可分割地，不间断地完成

在下一节中，我们将看一个经典的场景——全局整数的增量。

## 一个经典案例——全球 i ++

想一想这个经典的例子:一个全局`i`整数在一个并发代码路径中递增，在这个路径中多个执行线程可以同时执行。对计算机硬件和软件的天真理解会让你相信这个操作显然是原子的。然而，现实是，现代硬件和软件(编译器和操作系统)比你想象的要复杂得多，从而导致了各种(对应用开发者来说)看不见的性能驱动的优化。

We won't attempt to delve into too much detail here, but the reality is that modern processors are extremely complex: among the many technologies they employ toward better performance, a few are superscalar and super-pipelined execution in order to execute multiple independent instructions and several parts of various instructions in parallel (respectively), performing on-the-fly instruction and/or memory reordering, caching memory in complex hierarchical on-CPU caches, false sharing, and so on! We will delve into some of these details in [Chapter 7](7.html), *Kernel Synchronization – Part 2*, in the *Cache effects – false sharing* and *Memory barriers* sections.

The paper *What every systems programmer should know about concurrency* by *Matt Kline, April 2020*, ([https://assets.bitbashing.io/papers/concurrency-primer.pdf](https://assets.bitbashing.io/papers/concurrency-primer.pdf)) is superb and a must-read on this subject; do read it!

所有这些使得情况比乍看起来更复杂。让我们继续经典的`i ++`:

```sh
static int i = 5;
[ ... ]
foo()
{
    [ ... ]
    i ++;     // is this safe? yes, if truly atomic... but is it truly atomic??
}
```

这个增量本身安全吗？简短的回答是不，你必须保护它。为什么呢？这是一个关键部分—我们正在访问共享的可写数据以进行读和/或写操作。更长的答案是，它真的取决于增量操作是否真的是原子的(不可分割的)；如果是，那么`i ++`在平行性存在的情况下不会构成危险——如果不是，它会！那么，我们如何知道`i ++`是否真的是原子的呢？有两件事决定了这一点:

*   处理器的**指令集架构** ( **ISA** )，它确定(在与低级别处理器相关的几件事情中)运行时执行的机器指令。
*   编译器。

如果 ISA 能够使用单个机器指令来执行整数增量，*和*编译器能够智能地使用它，*那么*就是真正的原子指令——它是安全的，不需要锁定。否则不安全，需要上锁！

**试试这个**:把你的浏览器导航到这个奇妙的编译器浏览器网站:[https://godbolt.org/](https://godbolt.org/)。选择 C 作为编程语言，然后在左窗格中，声明函数内的全局`i`整数和增量。使用适当的编译器和编译器选项在右窗格中编译。您将看到为 C 高级`i ++;`语句生成的实际机器代码。如果确实是单机指令，那么就安全了；如果没有，您将需要锁定。总的来说，你会发现你真的分不清:实际上，你*无法*承担起假设的事情——你将不得不默认它是不安全的，并保护它！这可以在下面的截图中看到:

![](assets/a6f1659c-346b-40c8-b5e0-f0e4033381ef.png)

Figure 6.2 – Even with the latest stable gcc version but no optimization, the x86_64 gcc produces multiple instructions for the i ++

前面的截图清楚地显示了这一点:左侧和右侧窗格中的黄色背景区域分别是 C 源代码和编译器生成的相应程序集(基于 x86_64 ISA 和编译器的优化级别)。默认情况下，在没有优化的情况下，`i ++`变成三条机器指令。这正是我们所期望的:它对应于*获取*(内存注册)*增量**存储*(内存注册)！现在，这是*不是*原子；完全有可能的是，在其中一条机器指令执行后，控制单元会干涉并将指令流切换到不同的点。这甚至可能导致另一个进程或线程被上下文切换！

好消息是，在`Compiler options...`窗口中快速点击`-O2`，T2 就变成了一条机器指令——真正的原子指令！然而，我们无法提前预测这些事情；总有一天，你的代码可能会在一个相当低端的 ARM (RISC)系统上执行，增加`i ++`需要多条机器指令的机会。(不要担心–我们将在*中使用原子整数运算符*部分介绍专门针对整数的优化锁定技术)。

Modern languages provide native atomic operators; for C/C++, it's fairly recent (from 2011); the ISO C++11 and the ISO C11 standards provide ready-made and built-in atomic variables for this. A little googling will quickly reveal them to you. Modern glibc also makes use of them. As an example, if you've worked with signaling in user space, you will know to use the `volatile sig_atomic_t` data type to safely access and/or update an atomic integer within signal handlers. What about the kernel? In the next chapter, you'll learn about the Linux kernel's solution to this key issue. We'll cover this in the *Using the atomic integer operators* and *Using the atomic bit operators* sections.

当然，Linux 内核是一个并发环境:多个执行线程在多个 CPU 内核上并行运行。不仅如此，即使在单处理器系统中，硬件中断、陷阱、故障、异常和软件信号的存在也会导致数据完整性问题。不用说，在代码路径的要求点上防止并发是说起来容易做起来难；使用锁定等技术以及其他同步原语和技术来识别和保护关键部分是绝对必要的，这就是为什么这是本章和下一章的核心主题。

## 概念–锁

我们需要同步，因为在没有任何干预的情况下，线程可以同时执行正在处理共享可写数据(共享状态)的关键部分。为了战胜并发性，我们需要消除并行性，我们需要*序列化*关键部分中的代码——共享数据被处理的地方(用于读取和/或写入)。

要强制代码路径序列化，一种常见的技术是使用**锁**。本质上，锁的工作原理是保证恰好一个执行线程可以在任何给定的时间点“获取”或拥有锁。因此，使用锁来保护代码中的关键部分会给你我们想要的东西——专门运行关键部分的代码(也许是原子的；更多关于这方面的信息):

![](assets/5ccf6307-e970-4b7f-bcaa-566fb4acfb80.png)

Figure 6.3 – A conceptual diagram showing how a critical section code path is honored, given exclusivity, by using a lock

上图显示了一种修复前面提到的情况的方法:使用锁来保护关键部分！从概念上讲，锁(和解锁)是如何工作的？

锁的基本前提是，每当存在争用时——也就是说，当多个竞争线程(比如说，`n`线程)试图获取锁时(`LOCK`操作)——只有一个线程会成功。这被称为锁的“赢家”或“所有者”。它将*锁定* API 视为非阻塞调用，因此在执行关键部分的代码时继续愉快地——并且独占地——运行(关键部分实际上是*锁定*和*解锁*操作之间的代码！).`n-1`“失败者”的线索会怎么样？他们(也许)将锁应用编程接口视为阻塞调用；实际上，他们在等待。侍候什么？*解锁*操作，当然是由锁的主人(“赢家”线程)来执行！一旦解锁，剩余的`n-1`线程现在将争夺下一个“赢家”位置；当然，他们中正好有一个会“赢”并继续前进；在此期间，`n-2`输家将等待(新)赢家的*解锁*；这样重复，直到所有`n`线程(最终顺序)获得锁。

现在，锁定当然有效，但是——这应该是非常直观的——它导致(相当陡峭！)**开销，因为它击败了并行性，序列化了**执行流！为了帮助你想象这种情况，想象一个漏斗，狭窄的茎是关键部分，一次只能装一根线。所有其他线程都会阻塞；锁定会产生瓶颈:

![](assets/4f476235-4b35-4d76-8d49-694b0095c1be.png)

Figure 6.4 – A lock creates a bottleneck, analogous to a physical funnel

另一个经常被提及的物理模拟是一条高速公路，几条车道合并成一条非常繁忙的车道，交通拥堵(也许是一个设计糟糕的收费站)。同样，并行性——汽车(线程)与不同车道上的其他汽车(CPU)并行行驶——丢失，需要序列化行为——汽车被迫一辆接一辆排队。

因此，作为软件架构师，我们必须尝试和设计我们的产品/项目，以便最少需要锁定。虽然在大多数实际项目中完全消除全局变量实际上是不可能的，但是需要优化和最小化它们的使用。我们将在后面介绍更多这方面的内容，包括一些非常有趣的无锁编程技术。

另一个真正的关键点是，新手程序员可能天真地认为对共享可写数据对象执行读取是完全安全的，因此不需要显式保护(处理器总线大小内的对齐原语数据类型除外)；这是不真实的。这种情况会导致所谓的**脏读或撕裂读**，这种情况下，当另一个写线程正在同时写入时，可能会读取陈旧的数据，而您正在错误地、没有锁定地读取完全相同的数据项。

因为我们讨论的是原子性，正如我们刚刚了解到的，在典型的现代微处理器上，唯一保证是原子性的是一条机器语言指令或对处理器总线宽度内对齐的原始数据类型的读/写。那么，我们如何标记几行“C”代码，使它们成为真正的原子代码呢？在用户空间，这甚至是不可能的(我们可以接近，但不能保证原子性)。

How do you "come close" to atomicity in user space apps? You can always construct a user thread to employ a `SCHED_FIFO` policy and a real-time priority of `99`. This way, when it wants to run, pretty much nothing besides hardware interrupts/exceptions can preempt it. (The old audio subsystem implementation heavily relied on this.)

在内核空间中，我们可以编写真正原子化的代码。具体怎么做？简单地说，我们可以使用自旋锁！我们将很快详细了解自旋锁。

### 要点总结

让我们总结一些关于关键部分的要点。仔细阅读这些内容，将它们放在手边，并确保在实践中使用它们，这一点非常重要:

*   一个**关键部分**是一个可以并行执行的代码路径，它处理(读取和/或写入)共享的可写数据(也称为“共享状态”)。
*   因为它对共享的可写数据起作用，所以关键部分需要以下保护:
    *   并行性(也就是说，它必须单独运行/序列化/以互斥方式运行)
    *   当在原子(中断)非阻塞上下文中运行时——原子地:不可分割地，直到完成，没有中断。一旦受到保护，您可以安全地访问您的共享状态，直到您“解锁”。
*   必须识别和保护代码库中的每个关键部分:
    *   确定关键部分至关重要！仔细检查你的代码，确保你不会错过它们。
    *   保护它们可以通过各种技术来实现；一种非常常见的技术是*锁定*(还有无锁编程，我们将在下一章中讨论)。
    *   一个常见的错误是只保护*将*写入全局可写数据的关键部分；您还必须保护*读取*全局可写数据的关键部分；否则，你就冒着被**撕破或者弄脏的风险去读！**为了帮助明确这一关键点，可视化一个在 32 位系统上读写的无符号 64 位数据项；在这种情况下，操作不能是原子的(需要两个加载/存储操作)。因此，如果当您在一个线程中读取数据项的值时，另一个线程正在同时写入它，那会怎样呢！？写线程获取某种“锁”,但是因为您认为读取是安全的，所以锁不会被读线程获取；由于不幸的时间巧合，您最终可能会执行部分/撕裂/脏读！在接下来的章节和下一章中，我们将学习如何通过使用各种技术来克服这些问题。
    *   另一个致命的错误是没有使用相同的锁来保护给定的数据项。
    *   未能保护关键部分会导致**数据竞争**，这种情况下，结果(正在读取/写入的数据的实际值)是“活跃的”，这意味着它会因运行时环境和时间而异。这就是所谓的 bug。(一旦进入“领域”，就极难看到、重现、确定其根本原因并修复的 bug。我们将在下一章的*内核*中介绍一些非常强大的东西来帮助您进行锁定调试；一定要看！)
*   **异常**:在以下情况下，您是安全的(隐式，无显式保护):
    *   当你处理局部变量时。它们被分配在线程的私有堆栈上(或者，在中断上下文中，在本地 IRQ 堆栈上)，因此，根据定义，它们是安全的。
    *   当您在无法在另一个上下文中运行的代码中处理共享可写数据时；也就是说，它是按自然顺序连载的。在我们的上下文中，LKM 的*初始化*和*清理*方法是合格的(它们只在`insmod`和`rmmod`上连续运行一次)。
    *   当你处理真正恒定且只读的共享数据时(不过，不要让 C 的`const`关键字愚弄你！).
*   锁定本质上是复杂的；您必须仔细思考、设计和实现这一点，以避免*死锁。*我们将在*锁定指南和死锁*部分对此进行更详细的介绍。

# Linux 内核中的并发问题

识别一段内核代码中的关键部分至关重要；你连看都看不到怎么保护它？作为一名初露头角的内核/驱动程序开发人员，以下是一些指导原则，可以帮助您认识到并发问题(以及关键部分)可能出现的位置:

*   **对称多处理器** ( **SMP** )系统的存在(`CONFIG_SMP`)
*   可抢占内核的存在
*   阻塞输入输出
*   硬件中断(在 SMP 或 UP 系统上)

这些是需要理解的关键点，我们将在本节中逐一讨论。

## 多核 SMP 系统和数据竞赛

第一点非常明显；看看下面截图中显示的伪代码:

![](assets/79357d73-c814-478c-b463-1951621f15e2.png)

Figure 6.5 – Pseudocode – a critical section within a (fictional) driver's read method; it's wrong as there's no locking

这与我们在*图 6.1* 和 6 *图 3* 中显示的情况相似；只是在这里，我们用伪代码来展示并发性。显然，从时间`t2`到时间`t3`，驱动程序正在处理一些全局共享的可写数据，因此这是一个关键部分。

现在，想象一个有四个 CPU 核心的系统(一个 SMP 系统)；两个用户空间进程，P1(在比如 CPU 0 上运行)和 P2(在比如 CPU 2 上运行)，可以并发打开设备文件并同时发出`read(2)`系统调用。现在，两个进程将同时执行驱动程序读取“方法”，从而同时处理共享的可写数据！这(在`t2`和`t3`之间的代码)是一个关键部分，由于我们违反了基本的排他规则——关键部分必须在任何时间点仅由一个线程执行——我们很可能最终破坏数据、应用，甚至更糟。

换句话说，这现在是一场**数据竞赛**；根据微妙的时间巧合，我们可能会也可能不会产生错误(bug)。正是这种不确定性——微妙的时间巧合——使得发现和修复这样的错误变得极其困难(它可以逃避您的测试工作)。

This aphorism is all too unfortunately true: *Testing can detect the presence of errors, not their absence.* Adding to this, you're worse off if your testing fails to catch races (and bugs), allowing them free rein in the field.

您可能会觉得，由于您的产品是一个运行在一个 CPU 内核(UP)上的小型嵌入式系统，所以关于控制并发性(通常是通过锁定)的讨论并不适用于您。我们不敢苟同:几乎所有现代产品，如果还没有的话，都将转向多核(也许在它们的下一代阶段)。更重要的是，正如我们将要探讨的，即使是 UP 系统也有并发问题。

## 可抢占内核、阻塞输入/输出和数据竞争

假设您正在一个配置为可抢占的 Linux 内核上运行您的内核模块或驱动程序(也就是说，`CONFIG_PREEMPT`打开；我们在配套指南 *Linux 内核编程、* *第 10 章*、*中央处理器调度器–第 1 部分*中讨论了这个主题。考虑一个进程，P1，正在进程上下文中运行驱动程序的 read 方法代码，处理全局数组。现在，当它处于关键部分(在时间`t2`和`t3`之间)时，如果内核*抢占了*进程 P1，并且上下文切换到另一个进程 P2，它正在等待执行这个代码路径，会怎么样？这很危险，同样，这是一场数据竞赛。这很可能发生在甚至一个 UP 系统上！

另一种情况有些类似(同样，可能发生在单核(UP)或多核系统上):进程 P1 正在运行驱动程序方法的关键部分(再次在时间`t2`和`t3;`之间，参见*图 6.5* )。这一次，如果在关键部分，它遇到了阻塞调用呢？

一个**阻塞调用**是一个导致调用进程上下文进入休眠状态的函数，等待一个事件；当该事件发生时，内核将“唤醒”任务，并从它停止的地方继续执行。这也称为输入/输出阻塞，非常常见；许多 API(包括几个用户空间库和系统调用，以及几个内核 API)本质上都是阻塞的。在这种情况下，进程 P1 实际上是上下文关闭中央处理器并进入睡眠，这意味着`schedule()`的代码运行并将其排入等待队列。

在此期间，在 P1 回归之前，如果另一个进程 P2 计划运行会怎么样？如果该进程也在运行这个特定的代码路径呢？想想看——当 P1 回来的时候，共享数据可能已经“在它下面”发生了变化，导致了各种各样的错误；再次，一场数据竞赛，一个 bug！

## 硬件中断和数据竞争

最后，想象一下这个场景:流程 P1 再次无辜地运行驱动程序的读取方法代码；进入临界段(时间`t2`至`t3`之间；再次参见*图 6.5* )。它取得了一些进展，但是，唉，一个硬件中断触发(在同一个中央处理器上)！在 Linux 操作系统上，硬件(外设)中断优先级最高；默认情况下，它们会抢占任何代码(包括内核代码)。这样，进程(或线程)P1 至少会被暂时搁置，从而失去处理器；中断处理代码将抢占它并运行。

你可能会想，那又怎样？的确，这完全是家常便饭！硬件中断在现代系统中非常频繁，有效地(字面上)中断各种任务上下文(在你的Shell上快速`vmstat 3`；标有`in`的`system`下面的列显示了最近 1 秒内系统上触发的硬件中断的数量！).要问的关键问题是:中断处理代码(或者是 hardirq 上半部分，或者是所谓的 tasklet 或 softirq 下半部分，以发生的为准)*是否共享和处理刚刚中断的进程上下文的相同共享可写数据？*

如果这是真的，那么，休斯顿，我们有一个问题-数据竞赛！如果没有，那么中断的代码就不是中断代码路径的关键部分，这没关系。事实是，大多数设备驱动程序确实处理中断；由此可见，它是司机作者的(你的！)确保没有全局或静态数据(实际上，没有关键部分)在进程上下文和中断代码路径之间共享的责任。如果是这样(这种情况确实发生了)，您必须以某种方式保护这些数据免受数据竞争和可能的损坏。

这些场景可能会让您觉得防范这些并发问题是一项非常艰巨的任务；面对现有的关键部分以及各种可能的并发问题，您究竟如何实现数据安全？有意思的是，实际的 API 并不难学会使用；再次强调**识别关键路段**是关键要做的事情。

Again, the basics regarding how a lock (conceptually) works, locking guidelines (very important; we'll recap on them shortly), and the types of and how to prevent deadlocks, are all dealt with in my earlier book, *Hands-On System Programming with Linux (Packt, Oct 2018)*. This books covers these points in detail in *Chapter 15*, *Multithreading with Pthreads Part II – Synchronization*.

不用多说，让我们深入探讨一下主要的同步技术，它将用于保护我们的关键部分——锁定。

## 锁定指南和死锁

锁定，就其本质而言，是一种复杂的野兽；它往往会产生复杂的连锁场景。对它了解不够会导致性能问题和错误——死锁、循环依赖、中断不安全锁定等等。使用锁定时，以下锁定准则是确保正确编写代码的关键:

*   **锁定粒度**:锁定和解锁的“距离”(实际上是临界段的长度)不要粗(过长的临界段)要“足够细”；这是什么意思？以下几点解释了这一点:
    *   你在这里需要小心。当你在处理大型项目时，锁太少是个问题，锁太多也是个问题！太少的锁会导致性能问题(因为相同的锁被重复使用，因此往往竞争激烈)。
    *   拥有大量锁实际上有利于性能，但不利于复杂性控制。这也引出了另一个需要理解的关键点:代码库中有许多锁，您应该非常清楚哪个锁保护哪个共享数据对象。如果你用，比如说，`lockA`来保护`mystructX`，这是完全没有意义的，但是在很远的代码路径(可能是一个中断处理程序)中，你忘记了这一点，当在同一个结构上工作时，试着用一些其他的锁，`lockB`来保护！现在，这些事情听起来可能很明显，但是(正如有经验的开发人员所知)，在足够大的压力下，即使是显而易见的事情也不总是显而易见的！
    *   试着平衡一下。在大型项目中，通常使用一个锁来保护一个全局(共享)数据结构。(*将*命名为锁变量井本身就可能成为一个大问题！这就是为什么我们将保护数据结构的锁作为成员放在其中的原因。)

*   **锁定排序**很关键；**在整个**中，锁必须以相同的顺序获取，并且它们的顺序应该被记录下来，并且被所有从事该项目的开发人员所遵循(注释锁也是有用的；在下一章关于*锁定*的章节中有更多的相关内容。不正确的锁排序通常会导致死锁。
*   尽可能避免递归锁定。
*   注意防止饥饿；验证锁一旦被拿走，是否确实“足够快地”被释放。
*   **简单是关键**:尽量避免复杂或过度设计，尤其是涉及锁的复杂场景。

关于锁定的话题，出现了(危险的)死锁问题。一**僵局**是无法取得任何进展；换句话说，应用和/或内核组件似乎无限期挂起。虽然我们不打算在这里深究死锁的血淋淋的细节，但我将很快提到一些可能发生的更常见类型的死锁场景:

*   简单案例，单锁，流程上下文:
    *   我们尝试两次获取同一个锁；这会导致**自死锁**。

*   简单案例、多个(两个或更多)锁、流程上下文–示例:
    *   在 CPU `0`上，线程 A 获取锁 A，然后想要锁 b。
    *   同时，在 CPU `1`上，线程 B 获取锁 B，然后想要锁 A。
    *   结果是一个僵局，通常被称为 **AB-BA** **僵局**。
    *   可以扩展；例如，AB-BC-CA **循环依赖** (A-B-C 锁链)导致死锁。
*   复杂情况、单锁、进程和中断上下文:
    *   锁 A 接受中断上下文。
    *   如果中断发生(在另一个内核上)并且处理程序试图获取锁 A 怎么办？结果就是僵局！因此，在中断上下文中获取的锁必须始终在中断禁用的情况下使用。(怎么做？当我们讨论自旋锁的时候，我们会更详细地讨论这个问题。)
*   更复杂的情况、多个锁以及进程和中断(hardirq 和 softirq)上下文

在更简单的情况下，始终遵循*锁排序准则*就足够了:始终以一种记录良好的顺序获取和释放锁(我们将在*使用互斥锁*一节的内核代码中提供一个例子)。然而，这可能会变得非常复杂；复杂的死锁场景甚至会让有经验的开发人员出错。幸运的是，***lock dep***——Linux 内核的运行时锁依赖验证器——可以捕捉每一个死锁情况！(别担心，我们会到达那里的:我们将在下一章中详细介绍 lockdep)。当我们讨论自旋锁(使用自旋锁的*部分)时，我们会遇到与前面提到的类似的进程和/或中断上下文场景；这里明确了要使用的自旋锁的类型。*

*With regard to deadlocks, a pretty detailed presentation on lockdep was given by Steve Rostedt at a Linux Plumber's Conference (back in 2011); the relevant slides are informative and explore both simple and complex deadlock scenarios, as well as how lockdep can detect them ([https://blog.linuxplumbersconf.org/2011/ocw/sessions/153](https://blog.linuxplumbersconf.org/2011/ocw/sessions/153)).Also, the reality is that not just deadlock, but even **livelock** situations, can be just as deadly! Livelock is essentially a situation similar to deadlock; it's just that the state of the participating task is running and not waiting. An example, an interrupt "storm" can cause a livelock; modern network drivers mitigate this effect by switching off interrupts (under interrupt load) and resorting to a polling technique called **New API; Switching Interrupts** (**NAPI**) (switching interrupts back on when appropriate; well, it's more complex than that, but we leave it at that here).

对于那些生活在岩石下的人来说，你会知道 Linux 内核有两种主要类型的锁:互斥锁和自旋锁。实际上，还有其他几种类型，包括其他同步(和“无锁”编程)技术，所有这些都将在本章和下一章中介绍。

# 互斥还是自旋锁？什么时候使用哪个

学习使用互斥锁和自旋锁的确切语义非常简单(内核 API 集内有适当的抽象，这使得典型的驱动程序开发人员或模块作者更加容易)。这种情况下的关键问题是一个概念性的问题:这两个锁到底有什么区别？更切题的是，在什么情况下应该使用哪个锁？在本节中，您将学习这些问题的答案。

以我们之前的驱动读取方法的伪代码(*图 6.5* )为基础示例，假设三个线程–**tA**、 **tB** 和**tC**–通过该代码并行运行(在 SMP 系统上)。我们将通过在临界段开始之前(时间 **t2** )获取或获取锁，并在临界段代码路径结束之后(时间 **t3** )释放锁(解锁)，来解决这个并发问题，同时避免任何数据竞争。让我们再看一遍伪代码，这次使用锁定来确保它是正确的:

![](assets/a0db53d6-0c64-4377-90a2-bdb95a2fab16.png)

Figure 6.6 – Pseudocode – a critical section within a (fictional) driver's read method; correct, with locking

当三个线程试图同时获取锁时，系统保证只有其中一个线程会获得锁。假设 **tB** (线程 B)获得锁:现在是“赢家”或“拥有者”线程。这意味着线程 **tA** 和 **tC** 是“输家”；他们是做什么的？他们等待开锁！“胜者”( **tB** )完成关键段并解锁锁的瞬间，之前的败者之间的战斗重新开始；他们中的一个将成为下一个赢家，这个过程重复进行。

互斥锁和自旋锁这两种锁的主要区别在于失败者如何等待解锁。有了互斥锁，失败线程就进入休眠状态；也就是说，他们通过睡觉来等待。赢家执行解锁的那一刻，内核唤醒了输家(他们所有人)，他们跑了，再次争夺锁。(事实上，互斥体和信号量有时被称为睡眠锁。)

然而，有了**自旋锁**，就没有睡觉的问题了；失败者等待锁旋转，直到锁被打开。从概念上看，这看起来如下:

```sh
while (locked) ;
```

注意这只是*概念上的*。想一想——这实际上是民意测验。然而，作为一名优秀的程序员，你会明白，轮询通常被认为是一个坏主意。那么，为什么自旋锁是这样工作的呢？嗯，它没有；它只是出于概念目的才以这种方式提出。您很快就会明白，自旋锁只有在多核(SMP)系统上才真正有意义。在这样的系统上，当赢家线程离开并运行关键部分代码时，输家通过在其他中央处理器内核上旋转来等待！实际上，在实现层面上，用于实现现代自旋锁的代码是高度优化的(并且是特定于 arch 的)，并且不会通过简单的“旋转”来工作(例如，许多 ARM 的自旋锁实现使用**等待事件** ( **WFE** )机器语言指令，这使得 CPU 在低功率状态下最佳地等待；请参阅*进一步阅读*部分，了解有关自旋锁内部实现的一些资源。

## 从理论上确定使用哪个锁

spinlock 是如何实现的，这真的不是我们关心的问题；我们感兴趣的是，自旋锁的开销比互斥锁低。为什么很简单，真的:要让互斥锁工作，失败线程必须进入睡眠状态。为此，在内部，调用`schedule()`函数，这意味着失败者将互斥锁 API 视为阻塞调用！对调度程序的调用将最终导致处理器被上下文关闭。相反，当所有者线程解锁锁时，失败者线程必须被唤醒；同样，它将被上下文切换回处理器。因此，互斥锁/解锁操作的最小“成本”是在给定机器上执行两次上下文切换所需的时间。(参见下一节*信息框*。)通过再次查看前面的截图，我们可以确定一些事情，包括花费在关键部分的时间(“锁定”的代码路径)；也就是`t_locked = t3 - t2`。

假设`t_ctxsw`代表上下文切换的时间。据我们所知，互斥锁/解锁操作的最小成本是`2 * t_ctxsw`。现在，假设下面的表达式是正确的:

```sh
t_locked < 2 * t_ctxsw
```

换句话说，如果在关键部分花费的时间少于两次上下文切换所花费的时间，会怎样？在这种情况下，使用互斥锁是错误的，因为这是太多的开销；执行元工作比实际工作花费的时间更多——这种现象被称为**抖动**。正是这种精确的用例——非常短的关键部分的存在——在现代操作系统(如 Linux)上经常如此。因此，总之，对于短的非阻塞关键部分，使用自旋锁(远)优于使用互斥锁。

## 确定使用哪个锁–实际上

所以，在`t_locked < 2 * t_ctxsw`“规则”下操作在理论上可能很棒，但是等等:你真的被期望精确地测量上下文切换时间和在每个存在一个(关键部分)的情况下在关键部分花费的时间吗？不，当然不是——那是非常不现实和迂腐的。

实际上，这样想:互斥锁的工作原理是让失败线程在解锁时休眠；自旋锁没有(失败者“自旋”)。让我们回忆一下 Linux 内核的一个黄金规则:在任何一种原子上下文中，内核都不能休眠(调用`schedule()`)。因此，我们永远不能在中断上下文中使用互斥锁，或者在睡眠不安全的任何上下文中使用互斥锁；然而，使用自旋锁就可以了。(记住，阻塞 API 是通过调用`schedule()`使调用上下文进入睡眠状态的 API。)让我们总结一下:

*   **临界区是在原子(中断)上下文中运行，还是在进程上下文中无法休眠？**使用自旋锁。
*   **临界区是否在进程上下文中运行，临界区的休眠是否必要？**使用互斥锁。

当然，使用自旋锁被认为比使用互斥锁开销更低；因此，您甚至可以在流程上下文中使用 spinlock(比如我们虚构的驱动程序的 read 方法)，只要关键部分没有阻塞(sleep)。

**[1]** The time taken for a context switch is varied; it largely depends on the hardware and the OS quality. Recent (September 2018) measurements show that context switching time is in the region of 1.2 to 1.5 **us** (**microseconds**) on a pinned-down CPU, and around 2.2 us without pinning ([https://eli.thegreenplace.net/2018/measuring-context-switching-and-memory-overheads-for-linux-threads/](https://eli.thegreenplace.net/2018/measuring-context-switching-and-memory-overheads-for-linux-threads/)).

硬件和 Linux 操作系统都有了巨大的改进，正因为如此，平均上下文切换时间也有了很大的提高。一篇旧的(1998 年 12 月)Linux Journal 文章确定，在 x86 类系统上，平均上下文切换时间为 19 us(微秒)，最坏的情况是 30 us。

这就提出了一个问题，我们如何知道代码当前是在进程还是中断上下文中运行？简单:我们的`PRINT_CTX()`宏(在我们的`convenient.h`头中)向我们展示了这一点:

```sh
if (in_task())
    /* we're in process context (usually safe to sleep / block) */
else
    /* we're in an atomic or interrupt context (cannot sleep / block) */
```

现在，您已经了解了使用哪个互斥体或自旋锁以及何时使用，让我们进入实际使用。我们将从如何使用互斥锁开始！

# 使用互斥锁

互斥锁也被称为可休眠锁或阻塞互斥锁。如您所知，如果关键部分可以休眠(阻塞)，则在流程上下文中使用它们。它们不能用于任何种类的原子或中断上下文(上半部分、下半部分，如小任务或软 IRQ 等)、内核定时器，甚至是不允许阻塞的进程上下文。

## 初始化互斥锁

互斥锁“对象”在内核中表示为`struct mutex`数据结构。考虑以下代码:

```sh
#include <linux/mutex.h>
struct mutex mymtx;
```

要使用互斥锁，它*必须*被显式初始化为解锁状态。初始化可以通过`DEFINE_MUTEX()`宏静态执行(声明和初始化对象)，也可以通过`mutex_init()`函数动态执行(这实际上是`__mutex_init()`函数的宏包装)。

例如，要声明和初始化一个名为`mymtx`的互斥对象，我们可以使用`DEFINE_MUTEX(mymtx);`。

我们也可以动态地这样做。为什么是动态的？通常，互斥锁是它保护的(全局)数据结构的成员(聪明！).例如，假设我们的驱动程序代码中有以下全局上下文结构(请注意，该代码是虚构的):

```sh
struct mydrv_priv {
    <member 1>
    <member 2>
    [...]
    struct mutex mymtx; /* protects access to mydrv_priv */
    [...]
};
```

然后，在您的驾驶员(或 LKM)方法中，执行以下操作:

```sh
static int init_mydrv(struct mydrv_priv *drvctx)
{
    [...]
    mutex_init(drvctx-mymtx);
    [...]
}
```

保持锁变量作为它所保护的(父)数据结构的成员是 Linux 中使用的一种常见(也是聪明的)模式；这种方法还有一个额外的好处，那就是避免了名称空间污染，并且对于哪个互斥体保护哪个共享数据项是明确的(这是一个比最初看起来更大的问题，尤其是在像 Linux 内核这样的大型项目中！).

Keep the lock protecting a global or shared data structure as a member within that data structure.

## 正确使用互斥锁

通常，您可以在内核源代码树中找到非常有见地的评论。这里有一个很棒的例子，它简洁地总结了正确使用互斥锁必须遵循的规则；请仔细阅读:

```sh
// include/linux/mutex.h
/*
 * Simple, straightforward mutexes with strict semantics:
 *
 * - only one task can hold the mutex at a time
 * - only the owner can unlock the mutex
 * - multiple unlocks are not permitted
 * - recursive locking is not permitted
 * - a mutex object must be initialized via the API
 * - a mutex object must not be initialized via memset or copying
 * - task may not exit with mutex held
 * - memory areas where held locks reside must not be freed
 * - held mutexes must not be reinitialized
 * - mutexes may not be used in hardware or software interrupt
 * contexts such as tasklets and timers
 *
 * These semantics are fully enforced when DEBUG_MUTEXES is
 * enabled. Furthermore, besides enforcing the above rules, the mutex
 * [ ... ]
```

作为内核开发人员，您必须了解以下内容:

*   一个关键部分导致代码路径*被序列化，破坏了并行性*。因此，你必须尽可能缩短关键部分。一个推论是**锁定数据，而不是编码**。
*   试图重新获取已经获取(锁定)的互斥锁(这实际上是递归锁定)是不支持的*，并将导致自死锁。*
**   **锁排序**:这是防止危险死锁情况的一个非常重要的经验法则。在存在多个线程和多个锁的情况下，*记录获取锁的顺序并由所有从事该项目的开发人员严格遵守是至关重要的。*实际的锁排序本身并不是神圣不可侵犯的，但是一旦决定了就必须遵循的事实是。在浏览内核源代码树时，您会遇到内核开发人员确保做到这一点的许多地方，他们(通常)会就此写一个注释，供其他开发人员查看和遵循。下面是来自 slab 分配器代码(`mm/slub.c`)的示例注释:*

```sh
/*
 * Lock order:
 * 1\. slab_mutex (Global Mutex)
 * 2\. node-list_lock
 * 3\. slab_lock(page) (Only on some arches and for debugging)
```

既然我们从概念的角度理解了互斥体是如何工作的(并且理解了它们的初始化)，那么让我们学习如何利用锁定/解锁 API。

## 互斥锁和解锁应用编程接口及其使用

互斥锁的实际锁定和解锁 API 如下。下面的代码分别展示了如何锁定和解锁互斥体:

```sh
void __sched mutex_lock(struct mutex *lock);
void __sched mutex_unlock(struct mutex *lock);
```

(这里忽略`__sched`；只是一个编译器属性让这个函数消失在`WCHAN`输出中，这个输出出现在 procfs 中，并且带有某些选项切换到`ps(1)`(比如`-l`)。

再次，`kernel/locking/mutex.c`中源代码内的注释非常详细，描述性很强；我鼓励你更详细地看一下这个文件。我们在这里只展示了它的一些代码，这些代码直接取自 5.4 Linux 内核源代码树:

```sh
// kernel/locking/mutex.c
[ ... ]
/**
 * mutex_lock - acquire the mutex
 * @lock: the mutex to be acquired
 *
 * Lock the mutex exclusively for this task. If the mutex is not
 * available right now, it will sleep until it can get it.
 *
 * The mutex must later on be released by the same task that
 * acquired it. Recursive locking is not allowed. The task
 * may not exit without first unlocking the mutex. Also, kernel
 * memory where the mutex resides must not be freed with
 * the mutex still locked. The mutex must first be initialized
 * (or statically defined) before it can be locked. memset()-ing
 * the mutex to 0 is not allowed.
 *
 * (The CONFIG_DEBUG_MUTEXES .config option turns on debugging
 * checks that will enforce the restrictions and will also do
 * deadlock debugging)
 *
 * This function is similar to (but not equivalent to) down().
 */
void __sched mutex_lock(struct mutex *lock)
{
    might_sleep();

    if (!__mutex_trylock_fast(lock))
        __mutex_lock_slowpath(lock);
}
EXPORT_SYMBOL(mutex_lock);
```

`might_sleep()`是一个具有有趣调试属性的宏；它捕获应该在原子上下文中执行但没有执行的代码！所以，考虑一下:`mutex_lock()`中的第一行代码:`might_sleep()`，意味着这个代码路径不应该被原子上下文中的任何东西执行，因为它可能会休眠。这意味着您应该只在进程上下文中使用互斥体，当它可以安全睡眠的时候！

**A quick and important reminder**: The Linux kernel can be configured with a large number of debug options; in this context, the `CONFIG_DEBUG_MUTEXES` config option will help you catch possible mutex-related bugs, including deadlocks. Similarly, under the Kernel Hackingmenu, you will find a large number of debug-related kernel config options. We discussed this in the companion guide *Linux Kernel Programming -* *Chapter 5*, *Writing Your First Kernel Module – LKMs Part 2*. There are several very useful kernel configs with regard to lock debugging; we shall cover these in the next chapter, in the *Lock debugging within the kernel* section.

### 互斥锁–通过[不]可中断睡眠？

像往常一样，互斥体的内容比我们目前看到的要多。您已经知道，一个 Linux 进程(或线程)在状态机的各种状态中循环。在 Linux 上，睡眠有两种独立的状态——可中断睡眠和不间断睡眠。可中断睡眠中的进程(或线程)是敏感的，这意味着它将响应用户空间信号，而不间断睡眠中的任务对用户信号不敏感。

在具有底层驱动程序的人机交互应用中，作为一般的经验法则，您通常应该将一个进程置于可中断的睡眠状态(当它在锁定时被阻塞)，从而由最终用户决定是否通过按下 *Ctrl* + *C* (或一些涉及信号的机制)来中止应用。在类似 Unix 的系统上，有一个设计规则经常被遵循:**提供机制，而不是策略**。说到这里，在非交互代码路径上，通常情况下您必须等待锁无限期等待，语义是已经传递给任务的信号不应该中止阻塞等待。在 Linux 上，不间断电源是最常见的。

所以，事情是这样的:`mutex_lock()` API 总是让调用任务进入不间断睡眠。如果这不是您想要的，请使用`mutex_lock_interruptible()`应用编程接口将调用任务置于可中断睡眠状态。语法上有一个区别；后者在成功时返回一个整数值`0`，在失败时(由于信号中断)返回一个整数值`-EINTR`(记住`0` / `-E`返回惯例)。

总的来说，使用`mutex_lock()`比使用`mutex_lock_interruptible()`要快；当关键部分很短时使用它(这样就可以保证锁保持很短的时间，这是一个非常理想的特性)。

The 5.4.0 kernel contains over 18,500 and just over 800 instances of calling the `mutex_lock()` and `mutex_lock_interruptible()` APIs, respectively; you can check this out via the powerful `cscope(1)` utility on the kernel source tree.

理论上，内核也提供了一个`mutex_destroy()`应用编程接口。这与`mutex_init()`相反；它的工作是将互斥体标记为不可用。它只能在互斥体处于解锁状态时调用，一旦被调用，互斥体就不能使用。这有点理论化，因为在常规系统中，它只是简化为一个空函数；只有在启用了`CONFIG_DEBUG_MUTEXES`的内核上，它才成为真正的(简单的)代码。因此，当使用互斥体时，我们应该使用这种模式，如下面的伪代码所示:

```sh
DEFINE_MUTEX(...);        // init: initialize the mutex object
/* or */ mutex_init();
[ ... ]
    /* critical section: perform the (mutex) locking, unlocking */
    mutex_lock[_interruptible]();
    << ... critical section ... >>
    mutex_unlock();
    mutex_destroy();      // cleanup: destroy the mutex object
```

既然您已经学习了如何使用互斥锁 APIs，让我们来运用这些知识。在下一节中，我们将建立在我们早期的一个之上(写得不好-没有保护！)“杂项”驱动程序，使用互斥对象根据需要锁定关键部分。

## 互斥锁——一个示例驱动程序

我们在*第 1 章* - *中创建了一个简单的设备驱动程序代码示例，编写一个简单的杂项字符设备驱动程序*；也就是`ch1/miscdrv_rdwr`。在那里，我们编写了一个简单的`misc`类字符设备驱动程序，并使用了一个用户空间实用程序(`ch12/miscdrv_rdwr/rdwr_drv_secret.c`)来读写设备驱动程序内存中的一个(所谓的)秘密。

然而，我们突出地(过分地)是正确的词在这里！)未能做到，因为代码是受保护的共享(全局)可写数据！这将使我们在现实世界中付出高昂的代价。我建议您花点时间考虑一下:两个(或三个或更多)用户模式进程打开这个驱动程序的设备文件，然后并发发出各种 I/O 读写是不可行的。在这里，全局共享可写数据(在这种特殊情况下，两个全局整数和驱动程序上下文数据结构)很容易被破坏。

因此，让我们通过复制这个驱动程序(我们现在称之为`ch12/1_miscdrv_rdwr_mutexlock/1_miscdrv_rdwr_mutexlock.c`)并重写它的一些部分来学习和纠正我们的错误。关键是我们必须使用互斥锁来保护所有的关键部分。与其在这里显示代码(毕竟是在本书的[https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming)的 GitHub 库中，请做`git clone`吧！)，让我们做一些有趣的事情:让我们看看旧的未受保护版本和新的受保护代码版本之间的“差异”(差异–由`diff(1)`生成的增量)。此处的输出已被截断:

```sh
$ pwd
<.../ch12/1_miscdrv_rdwr_mutexlock
$ diff -u ../../ch12/miscdrv_rdwr/miscdrv_rdwr.c miscdrv_rdwr_mutexlock.c>> miscdrv_rdwr.patch
$ cat miscdrv_rdwr.patch
[ ... ]
+#include <linux/mutex.h> // mutex lock, unlock, etc
 #include "../../convenient.h"
[ ... ] 
-#define OURMODNAME "miscdrv_rdwr"
+#define OURMODNAME "miscdrv_rdwr_mutexlock"

+DEFINE_MUTEX(lock1); // this mutex lock is meant to protect the integers ga and gb
[ ... ]
+     struct mutex lock; // this mutex protects this data structure
 };
[ ... ]
```

在这里，我们可以看到，在驱动程序的较新安全版本中，我们已经声明并初始化了一个名为`lock1`的互斥变量；我们将使用它来保护驱动程序中的两个全局整数`ga`和`gb`(仅用于演示目的)。接下来，重要的是，我们在“驱动上下文”数据结构中声明了一个名为`lock`的互斥锁；也就是`drv_ctx`。这将用于保护对该数据结构成员的任何和所有访问。在`init`代码内初始化:

```sh
+     mutex_init(&ctx->lock);
+
+     /* Initialize the "secret" value :-) */
      strscpy(ctx->oursecret, "initmsg", 8);
-     dev_dbg(ctx->dev, "A sample print via the dev_dbg(): driver initialized\n");
+     /* Why don't we protect the above strscpy() with the mutex lock?
+      * It's working on shared writable data, yes?
+      * Yes, BUT this is the init code; it's guaranteed to run in exactly
+      * one context (typically the insmod(8) process), thus there is
+      * no concurrency possible here. The same goes for the cleanup
+      * code path.
+      */
```

这个详细的评论清楚地解释了为什么我们不需要在`strscpy()`左右锁定/解锁。同样，这应该是显而易见的，但是局部变量对于每个进程上下文都是隐式私有的(因为它们驻留在该进程或线程的内核模式堆栈中)，因此不需要保护(每个线程/进程都有一个单独的变量*实例*，所以没有人会踩到任何人的脚！).在我们忘记之前，*清理*代码路径(通过`rmmod(8)`进程上下文调用)必须销毁互斥体:

```sh
-static void __exit miscdrv_rdwr_exit(void)
+static void __exit miscdrv_exit_mutexlock(void)
 {
+     mutex_destroy(&lock1);
+     mutex_destroy(&ctx->lock);
      misc_deregister(&llkd_miscdev);
 }
```

现在，让我们看看驱动程序打开方法的不同之处:

```sh
+
+     mutex_lock(&lock1);
+     ga++; gb--;
+     mutex_unlock(&lock1);
+
+     dev_info(dev, " filename: \"%s\"\n"
      [ ... ]
```

这就是我们操纵全局整数的地方，*使其成为关键部分*；与这个程序的前一个版本不同，在这里，我们*用`lock1`互斥体保护这个关键部分*。就是这样:这里的关键部分是代码`ga++; gb--;`:在(互斥)锁和解锁操作之间的代码。

但是(总有但是，不是吗？)，一切都不顺利！看看`mutex_unlock()`行代码后面的`printk`功能(`dev_info()`):

```sh
+ dev_info(dev, " filename: \"%s\"\n"
+         " wrt open file: f_flags = 0x%x\n"
+         " ga = %d, gb = %d\n",
+         filp->f_path.dentry->d_iname, filp->f_flags, ga, gb);
```

你觉得这样可以吗？不，仔细看:我们是*在读*全局整数的值，`ga`和`gb`。回想一下基本原理:在并发的情况下(这在这个驱动程序的 *open* 方法中肯定是可能的)，*即使在没有锁的情况下读取共享的可写数据也是潜在不安全的*。如果这对你没有意义，请想一想:如果当一个线程正在读取整数时，另一个线程正在同时更新(写入)它们，会怎么样；然后呢？这种情况称为一**脏读**(或一**撕读)**；我们最终可能会读取陈旧的数据，因此必须加以防范。(事实是，这并不是一个真正的脏读的好例子，因为在大多数处理器上，读写单个整数项确实倾向于原子操作。然而，我们绝不能假设这样的事情——我们必须只是做好我们的工作并保护它。)

事实上，还有另一个类似的 bug 在等待:我们已经从打开的文件结构(即`filp`指针)中读取了数据，却没有费心去保护它(确实，打开的文件结构有锁；我们应该用它！我们将稍后这样做)。

The precise semantics of how and when things such as *dirty reads* occur does tend to be very arch (machine)-dependent; nevertheless, our job as module or driver authors is clear: we must ensure that we protect all critical sections. This includes reads upon shared writable data.

现在，我们将把这些标记为潜在的错误(bug)。我们将在*中使用原子整数运算符*部分以更有利于性能的方式来解决这个问题。查看驱动程序读取方法的差异会发现一些有趣的东西(忽略这里显示的行号；他们可能会改变):

![](assets/ad26b085-7d4a-4090-96b8-44aef98664ce.png)

Figure 6.7 – The diff of the driver's read() method; see the usage of the mutex lock in the newer version

我们现在已经使用了驱动程序上下文结构的互斥锁来保护关键部分。设备驱动程序的*写*和*关闭*(释放)方法也是如此(自己生成补丁看看)。

注意，用户模式 app 保持不变，这意味着对于我们测试新的更安全版本，我们必须在`ch12/miscdrv_rdwr/rdwr_drv_secret.c`继续使用用户模式 app。在包含各种锁定错误和死锁检测功能的调试内核上运行和测试这样的驱动程序代码至关重要(我们将在下一章的*内核内的锁定调试*一节中返回这些“调试”功能)。

在前面的代码中，我们在`copy_to_user()`例程之前获取了互斥锁；没关系。但是，我们只在`dev_info()`之后发布。为什么不在此之前发布`printk`，从而缩短关键部分？

仔细观察`dev_info()`会发现为什么它在临界区内*。我们在这里打印三个变量的值:`secret_len`读取的字节数和`ctx->tx`和`ctx->rx`分别“发送”和“接收”的字节数。`secret_len`是一个局部变量，不需要保护，但是另外两个变量在全局驱动程序上下文结构中，因此需要保护，即使是来自(可能是脏的)读取。*

## 互斥锁——剩下的几点

在这一节中，我们将介绍一些关于互斥体的附加要点。

### 互斥锁应用编程接口变体

首先，让我们看一看互斥锁 API 的几个变体；除了可中断变量(在*互斥锁中描述–通过【不】可中断睡眠？*部分)，我们有 *trylock、可杀*和 *io* 变体。

#### 互斥 trylock 变量

如果你想实现一个**忙-等**语义呢；也就是说，测试(互斥)锁的可用性，如果可用(意味着它当前未锁定)，获取/锁定它并继续关键部分代码路径？如果该选项不可用(当前处于锁定状态)，请不要等待锁定；相反，执行一些其他工作，然后重试。实际上，这是非阻塞互斥锁变体，称为 trylock 以下流程图显示了它的工作原理:

![](assets/421daaad-97a1-4acc-8cfc-e4d33751eb84.png)

Figure 6.8 – The "busy wait" semantic, a non-blocking trylock operation

互斥锁的 trylock 变体的 API 如下:

```sh
int mutex_trylock(struct mutex *lock);
```

这个应用编程接口的返回值表示运行时发生的事情:

*   `1`的返回值表示锁已成功获取。
*   `0`的返回值表示锁当前被竞争(锁定)。

Though it might sound tempting to, do *not* attempt to use the `mutex_trylock()` API to figure out if a mutex lock is in a locked or unlocked state; this is inherently "racy". Next, note that using this trylock variant in a highly contended lock path may well reduce your chances of acquiring the lock. The trylock variant has been traditionally used in deadlock prevention code that might need to back out of a certain lock order sequence and be retried via another sequence (ordering).

此外，关于 trylock 变体，即使文献使用术语*原子地尝试和获取互斥体*，它也不在原子或中断上下文中工作——它只有*在进程上下文中工作(如同任何类型的互斥锁)。像往常一样，锁必须通过所有者上下文调用`mutex_unlock()`来释放。*

我建议您尝试使用 trylock 互斥变量作为练习。作业参见本章末尾的*问题*部分！

#### 互斥体可中断和可杀死的变体

正如您已经了解到的，当驱动程序(或模块)愿意确认任何(用户空间)中断它的信号(并返回`-ERESTARTSYS`告诉内核 VFS 层执行信号处理时，使用`mutex_lock_interruptible()`API；用户空间系统调用将在`errno`设置为`EINTR`时失败)。一个例子可以在内核中的模块处理代码中找到，在`delete_module(2)`系统调用中(它`rmmod(8)`调用):

```sh
// kernel/module.c
[ ... ]
SYSCALL_DEFINE2(delete_module, const char __user *, name_user,
        unsigned int, flags)
{
    struct module *mod;
    [ ... ]
    if (!capable(CAP_SYS_MODULE) || modules_disabled)
        return -EPERM;
    [ ... ]
    if (mutex_lock_interruptible(&module_mutex) != 0)
 return -EINTR;
    mod = find_module(name);
    [ ... ]
out:
    mutex_unlock(&module_mutex);
    return ret;
}
```

注意失败时 API 如何返回`-EINTR`。(`SYSCALL_DEFINEn()`宏变成系统调用签名；`n`表示该特定系统调用接受的参数数量。此外，请注意功能检查–除非您以 root 用户身份运行或具有`CAP_SYS_MODULE`功能(或模块加载完全禁用)，否则系统调用只会返回一个故障(`-EPERM`)。)

但是，如果您的驱动程序只愿意被致命信号中断(那些*将杀死*用户空间上下文的信号)，那么使用`mutex_lock_killable()`应用编程接口(签名与可中断变体的签名相同)。

#### 互斥 io 变量

`mutex_lock_io()` API 在语法上与`mutex_lock()` API 相同；唯一不同的是，内核认为失败线程的等待时间与等待 I/O 的时间相同(`kernel/locking/mutex.c:mutex_lock_io()`中的代码注释清楚地记录了这一点；看一看)。就会计而言，这很重要。

You can find fairly exotic APIs such as `mutex_lock[_interruptible]_nested()` within the kernel, with the emphasis here being on the `nested` suffix. However, note that the Linux kernel does not prefer developers to use nested (or recursive) locking (as we mentioned in the *Correctly using the mutex lock* section). Also, these APIs only get compiled in the presence of the `CONFIG_DEBUG_LOCK_ALLOC` config option; in effect, the nested APIs were added to support the kernel lock validator mechanism. They should only be used in special circumstances (where a nesting level must be incorporated between instances of the same lock type).

在下一节中，我们将回答一个典型的常见问题:互斥体和信号量对象有什么区别？Linux 甚至有信号量对象吗？请继续阅读了解详情！

### 信号量和互斥量

Linux 内核确实提供了一个信号量对象，以及您可以对(二进制)信号量执行的常见操作:

*   信号量锁通过`down[_interruptible]()`(和变体)应用编程接口获取
*   通过`up()`应用编程接口解锁信号量。

In general, the semaphore is an older implementation, so it's advised that you use the mutex lock in place of it.

一个值得关注的常见问题是:*互斥体和信号量有什么区别？*它们在概念上看似相似，但实际上有很大不同:

*   信号量是互斥体的一种更广义的形式；互斥锁可以被获取(随后释放或解锁)一次，而信号量可以被获取(随后释放)多次。
*   互斥体用于保护关键部分不被同时访问，而信号量应该作为一种机制来通知另一个等待的任务已经到达某个里程碑(通常，生产者任务通过信号量对象发布信号，消费者任务正在等待接收该信号，以便继续进一步的工作)。
*   互斥体有锁所有权的概念，只有所有者上下文可以执行解锁；二进制信号量没有所有权。

### 优先级反转和实时互斥

在使用任何类型的锁定时，需要注意的一点是，您应该仔细设计和编码，以防止可能出现的可怕的*死锁*情况(在*的下一章锁验证器 lock dep–及早捕捉锁定问题*一节中有更多关于这方面的内容)。

除了死锁之外，在使用互斥体时还会出现另一种危险的情况:优先级反转(同样，我们不会在本书中深入研究细节)。只要说无界**优先级反转**的情况可能是致命的就够了；最终结果是，产品的高(est)优先级线程在 CPU 之外的时间过长。

As I covered in some detail in my earlier book, *Hands-on System Programming with Linux,* it's precisely this priority inversion issue that struck NASA's Mars Pathfinder robot, on the Martian surface no less, back in July 1997! See the *Further reading* section of this chapter for interesting resources about this, something that every software developer should be aware of!

用户空间 Pthreads 互斥实现当然有**优先级继承** ( **PI** )语义可用。但是在 Linux 内核中呢？为此，Ingo Molnar 提供了基于 PI-futex 的 RT-mutex(一个实时互斥体；实际上，互斥体被扩展为具有 PI 功能。`futex(2)`是一个复杂的系统调用，提供了一个快速的用户空间互斥)。当`CONFIG_RT_MUTEXES`配置选项启用时，这些选项变为可用。与“常规”互斥语义非常相似，RT-mutex API 被提供来初始化，(解除)锁定和销毁 RT-mutex 对象。(这段代码已经从英戈·莫尔纳尔的`-rt`树合并到主线内核中)。就实际使用而言，RT-mutex 用于在内部实现 PI futex(系统调用`futex(2)`本身在内部实现 userspace Pthreads mutex)。除此之外，内核锁定自检代码和 I2C 子系统直接使用 RT-mutex。

因此，对于一个典型的模块(或驱动程序)作者来说，这些 API 不会被频繁使用。内核确实在[https://www . kernel . org/doc/Documentation/locking/RT-mutex-design . rst](https://www.kernel.org/doc/Documentation/locking/rt-mutex-design.rst)提供了一些关于 RT-mutex 内部设计的文档(涵盖优先级反转、优先级继承等等)。

### 内部设计

一句关于内核结构内部实现互斥锁的现实:Linux 试图在可能的情况下实现一种*快速路径*方法。

A **fast path** is the most optimized high-performance type of code path; for example, one with no locks and no blocking. The intent is to have code follow this fast path as far as possible. Only when it really isn't possible does the kernel fall back to a (possible) "mid path", and then a "slow path", approach; it still works but is slow(er).

该快速路径是在没有锁争用的情况下采用的(也就是说，锁一开始就处于解锁状态)。所以，锁很快就被锁上了。但是，如果互斥体已经被锁定，那么内核通常使用中间路径乐观旋转实现，使其更像是混合(互斥体/自旋锁)锁类型。如果连这都做不到，就会遵循“慢路径”——试图获取锁的进程上下文很可能会进入睡眠状态。如果你对它的内部实现感兴趣，更多细节可以在官方内核文档中找到:[https://www . kernel . org/doc/Documentation/locking/mutex-design . rst](https://www.kernel.org/doc/Documentation/locking/mutex-design.rst)。

*LDV (Linux Driver Verification) project:* in the companion guide *Linux Kernel Programming -* *Chapter 1*, *Kernel Workspace Setup*, in the section *The LDV – Linux Driver Verification – project*, we mentioned that this project has useful "rules" with respect to various programming aspects of Linux modules (drivers, mostly) as well as the core kernel.

With regard to our current topic, here's one of the rules: *Locking a mutex twice or unlocking without prior locking* ([http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0032](http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0032)). It mentions the kind of things you cannot do with the mutex lock (we have already covered this in the *Correctly using the mutex lock* section). The interesting thing here: you can see an actual example of a bug – a mutex lock double-acquire attempt, leading to (self) deadlock – in a kernel driver (as well as the subsequent fix).

现在您已经理解了如何使用互斥锁，让我们继续看内核中另一个非常常见的锁——自旋锁。

# 使用自旋锁

在*互斥还是自旋锁？在*部分，您学习了何时使用自旋锁而不是互斥锁，反之亦然。为了方便起见，我们复制了之前在此提供的关键陈述:

*   **临界区是在原子(中断)上下文中运行，还是在无法休眠的进程上下文中运行？**使用自旋锁。
*   **临界区是否在进程上下文中运行，临界区的休眠是否必要？**使用互斥锁。

在本节中，我们将考虑您现在已经决定使用自旋锁。

## 自旋锁–简单的用法

对于所有的 spinlock APIs，您必须包含相关的头文件；也就是`include <linux/spinlock.h>`。

类似于互斥锁，你*在使用前必须*声明并初始化自旋锁到解锁状态。自旋锁是通过名为`spinlock_t`的`typedef`数据类型声明的“对象”(在内部，它是在`include/linux/spinlock_types.h`中定义的结构)。可以通过`spin_lock_init()`宏动态初始化:

```sh
spinlock_t lock;
spin_lock_init(&lock);
```

或者，这可以用`DEFINE_SPINLOCK(lock);`静态执行(声明和初始化)。

和互斥一样，在(全局/静态)数据结构中声明自旋锁是为了防止并发访问，这通常是一个非常好的主意。正如我们前面提到的，这个想法经常在内核中使用；例如，表示 Linux 内核上打开文件的数据结构称为`struct file`:

```sh
// include/linux/fs.h
struct file {
    [...]
    struct path f_path;
    struct inode *f_inode; /* cached value */
    const struct file_operations *f_op;
    /*
     * Protects f_ep_links, f_flags.
     * Must not be taken from IRQ context.
     */
    spinlock_t f_lock;
    [...]
    struct mutex f_pos_lock;
    loff_t f_pos;
    [...]
```

检查一下:对于`file`结构，名为`f_lock`的自旋锁变量是保护`file`数据结构的`f_ep_links`和`f_flags`成员的自旋锁(它还有一个互斥锁来保护另一个成员；即文件的当前寻道位置–`f_pos`)。

如何锁定和解锁自旋锁？内核向用户模块/驱动程序作者公开了相当多的 API 变体；旋转(解除)锁定 API 的最简单形式如下:

```sh
void spin_lock(spinlock_t *lock);
<< ... critical section ... >>
void spin_unlock(spinlock_t *lock);
```

请注意，没有等同于`mutex_destroy()`应用编程接口的自旋锁。

现在，让我们来看看 spinlock APIs 的运行情况！

## spin lock–示例驱动程序

类似于我们对互斥锁示例驱动程序所做的工作(*互斥锁–示例驱动程序*部分)，为了说明自旋锁的简单用法，我们将复制我们早期的`ch12/1_miscdrv_rdwr_mutexlock`驱动程序作为启动模板，然后将其放入新的内核驱动程序中；也就是`ch12/2_miscdrv_rdwr_spinlock`。同样，在这里，我们将只显示该程序和该程序之间差异的一小部分(差异，由`diff(1)`生成的增量)(我们不会显示差异的每一行，只显示相关部分):

```sh
// location: ch12/2_miscdrv_rdwr_spinlock/
+#include <linux/spinlock.h>
[ ... ]
-#define OURMODNAME "miscdrv_rdwr_mutexlock"
+#define OURMODNAME "miscdrv_rdwr_spinlock"
[ ... ]
static int ga, gb = 1;
-DEFINE_MUTEX(lock1); // this mutex lock is meant to protect the integers ga and gb
+DEFINE_SPINLOCK(lock1); // this spinlock protects the global integers ga and gb
[ ... ]
+/* The driver 'context' data structure;
+ * all relevant 'state info' reg the driver is here.
  */
 struct drv_ctx {
    struct device *dev;
@@ -63,10 +66,22 @@
    u64 config3;
 #define MAXBYTES 128
    char oursecret[MAXBYTES];
- struct mutex lock; // this mutex protects this data structure
+ struct mutex mutex; // this mutex protects this data structure
+ spinlock_t spinlock; // ...so does this spinlock
 };
 static struct drv_ctx *ctx;
```

这一次，为了保护我们的`drv_ctx`全局数据结构的成员，我们同时拥有了原始的互斥锁和新的自旋锁。这是相当常见的；互斥锁保护可能发生阻塞的关键部分中的成员使用，而自旋锁用于保护不能发生阻塞(休眠——回想一下它可能休眠)的关键部分中的成员。

当然，我们必须确保初始化所有锁，使它们处于解锁状态。我们可以在驱动程序的`init`代码中这样做(继续补丁输出):

```sh
-   mutex_init(&ctx->lock);
+   mutex_init(&ctx->mutex);
+   spin_lock_init(&ctx->spinlock);
```

在驱动程序的`open`方法中，我们用自旋锁替换互斥锁，以保护全局整数的增量和减量:

```sh
 * open_miscdrv_rdwr()
@@ -82,14 +97,15 @@

    PRINT_CTX(); // displays process (or intr) context info

-   mutex_lock(&lock1);
+   spin_lock(&lock1);
    ga++; gb--;
-   mutex_unlock(&lock1);
+   spin_unlock(&lock1);
```

现在，在驱动程序的`read`方法中，我们使用自旋锁代替互斥来保护一些关键部分:

```sh
 static ssize_t read_miscdrv_rdwr(struct file *filp, char __user *ubuf, size_t count, loff_t  *off)
 {
-   int ret = count, secret_len;
+   int ret = count, secret_len, err_path = 0;
    struct device *dev = ctx->dev;

-   mutex_lock(&ctx->lock);
+   spin_lock(&ctx->spinlock);
    secret_len = strlen(ctx->oursecret);
-   mutex_unlock(&ctx->lock);
+   spin_unlock(&ctx->spinlock);
```

然而，这还不是全部！继续司机的`read`方法，仔细看看下面的代码和注释:

```sh
[ ... ]
@@ -139,20 +157,28 @@
     * member to userspace.
     */
    ret = -EFAULT;
-   mutex_lock(&ctx->lock);
+   mutex_lock(&ctx->mutex);
+   /* Why don't we just use the spinlock??
+    * Because - VERY IMP! - remember that the spinlock can only be used when
+    * the critical section will not sleep or block in any manner; here,
+    * the critical section invokes the copy_to_user(); it very much can
+    * cause a 'sleep' (a schedule()) to occur.
+    */
    if (copy_to_user(ubuf, ctx->oursecret, secret_len)) {
[ ... ]
```

当保护关键部分可能有阻塞 API 的数据时，例如在`copy_to_user()`中，我们*必须*只使用互斥锁！(由于空间不足，我们没有在这里显示更多的代码差异；我们希望您通读 spinlock 示例驱动程序代码，并亲自尝试一下。)

## 测试-在原子环境中睡眠

你已经知道了我们不应该做的一件事是在任何原子或中断上下文中休眠(阻塞)。让我们来测试一下。一如既往，经验方法——你为自己测试而不是依赖他人的经验——是关键！

我们到底如何测试这个？简单:我们将使用一个简单的整数模块参数`buggy`，当设置为`1`(默认值为`0`)时，它将在自旋锁的临界区内执行一个违反该规则的代码路径。我们将调用`schedule_timeout()`应用编程接口(正如您在[第 5 章](5.html)、*中学习的那样，在*理解如何使用*sleep()阻塞应用编程接口*一节中使用内核定时器、线程和工作队列*)内部调用`schedule()`；这就是我们在内核空间睡觉的方式)。以下是相关代码:

```sh
// ch12/2_miscdrv_rdwr_spinlock/2_miscdrv_rdwr_spinlock.c
[ ... ]
static int buggy;
module_param(buggy, int, 0600);
MODULE_PARM_DESC(buggy,
"If 1, cause an error by issuing a blocking call within a spinlock critical section");
[ ... ]
static ssize_t write_miscdrv_rdwr(struct file *filp, const char __user *ubuf,
                size_t count, loff_t *off)
{
    int ret, err_path = 0;
    [ ... ]
    spin_lock(&ctx->spinlock);
    strscpy(ctx->oursecret, kbuf, (count > MAXBYTES ? MAXBYTES : count));
    [ ... ]
    if (1 == buggy) {
        /* We're still holding the spinlock! */
        set_current_state(TASK_INTERRUPTIBLE);
        schedule_timeout(1*HZ); /* ... and this is a blocking call!
 * Congratulations! you've just engineered a bug */
    }
    spin_unlock(&ctx->spinlock);
    [ ... ]
}
```

现在，对于有趣的部分:让我们在两个内核中测试这个(有问题的)代码路径:首先，在我们定制的 5.4“调试”内核中(在这个内核中，我们已经启用了几个内核调试配置选项(大部分来自`make menuconfig`中的`Kernel Hacking`菜单)，如配套指南 *Linux 内核编程-* *第 5 章*、*编写您的第一个内核模块–LKMs Part 2*中所述)，其次，在一个通用发行版(我们通常在 Ubuntu 上运行)5.4 内核上，没有任何相关的内核调试

### 在 5.4 调试内核上进行测试

首先，确保您已经构建了定制的 5.4 内核，并且启用了所有必需的内核调试配置选项(同样，如果需要，请参考配套指南 *Linux 内核编程-* *第 5 章*、*编写您的第一个内核模块–LKMs 第 2 部分*、*配置调试内核*部分)。然后，启动你的调试内核(这里，它被命名为`5.4.0-llkd-dbg`)。现在，根据这个调试内核构建驱动程序(在`ch12/2_miscdrv_rdwr_spinlock/`中)(驱动程序目录中通常的`make`应该这样做；您可能会发现，在调试内核上，构建速度明显较慢！):

```sh
$ lsb_release -a 2>/dev/null | grep "^Description" ; uname -r
Description: Ubuntu 20.04.1 LTS
5.4.0-llkd-dbg $ make
[ ... ]
$ modinfo ./miscdrv_rdwr_spinlock.ko 
filename: /home/llkd/llkd_src/ch12/2_miscdrv_rdwr_spinlock/./miscdrv_rdwr_spinlock.ko
[ ... ]
description: LLKD book:ch12/2_miscdrv_rdwr_spinlock: simple misc char driver rewritten with spinlocks
[ ... ]
parm: buggy:If 1, cause an error by issuing a blocking call within a spinlock critical section (int)
$ sudo virt-what
virtualbox
kvm
$ 
```

如您所见，我们正在 x86_64 Ubuntu 20.04 来宾虚拟机上运行定制的 5.4.0“调试”内核。

How do you know whether you're running on a **virtual machine** (**VM**) or on the "bare metal" (native) system? `virt-what(1)` is a useful little script that shows this (you can install it on Ubuntu with `sudo apt install virt-what`).

要运行我们的测试用例，将驱动程序插入内核并将`buggy`模块参数设置为`1`。调用驱动程序的`read`方法(通过我们的用户空间应用；也就是说，`ch12/miscdrv_rdwr/rdwr_test_secret`)不是问题，如下图所示:

```sh
$ sudo dmesg -C
$ sudo insmod ./miscdrv_rdwr_spinlock.ko buggy=1
$ ../../ch12/miscdrv_rdwr/rdwr_test_secret 
Usage: ../../ch12/miscdrv_rdwr/rdwr_test_secret opt=read/write device_file ["secret-msg"]
 opt = 'r' => we shall issue the read(2), retrieving the 'secret' form the driver
 opt = 'w' => we shall issue the write(2), writing the secret message <secret-msg>
  (max 128 bytes)
$ 
$ ../../ch12/miscdrv_rdwr/rdwr_test_secret r /dev/llkd_miscdrv_rdwr_spinlock 
Device file /dev/llkd_miscdrv_rdwr_spinlock opened (in read-only mode): fd=3
../../ch12/miscdrv_rdwr/rdwr_test_secret: read 7 bytes from /dev/llkd_miscdrv_rdwr_spinlock
The 'secret' is:
 "initmsg"
$ 
```

接下来，我们通过用户模式 app 向驾驶员发出`write(2)`；这一次，我们的错误代码路径被执行。如您所见，我们在自旋锁临界区(即锁定和解锁之间)发出了`schedule_timeout()`。调试内核将此检测为一个错误，并在内核日志中生成(非常大的)调试诊断(请注意，像这样的错误很可能会挂起您的系统，因此首先在虚拟机上测试它):

![](assets/3c6f7129-6f1c-4a04-9f5c-df29e28b0420.png)

Figure 6.9 – Kernel diagnostics being triggered by the "scheduling in atomic context" bug we've deliberately hit here

前面的截图显示了发生的部分情况(在`ch12/2_miscdrv_rdwr_spinlock/2_miscdrv_rdwr_spinlock.c`中查看驱动程序代码时跟随):

1.  首先，我们有我们的用户模式 app 的流程上下文(`rdwr_test_secre`；注意名称是如何被截断到前 16 个字符的，包括`NULL`字节)，这就进入了驱动程序的写方法；也就是`write_miscdrv_rdwr()`。这可以在我们有用的`PRINT_CTX()`宏的输出中看到(我们在这里复制了这条线):

```sh
miscdrv_rdwr_spinlock:write_miscdrv_rdwr(): 004) rdwr_test_secre :23578 | ...0 /*  write_miscdrv_rdwr() */
```

2.  它从用户空间写入器进程中复制新的“秘密”并写入，为 24 字节。
3.  然后，它“获取”自旋锁，进入临界区，并将该数据复制到我们的驱动程序上下文结构的`oursecret`成员。
4.  之后，`if (1 == buggy) {`评估为真。
5.  然后，它调用`schedule_timeout()`，这是一个阻塞 API(就像它内部调用`schedule()`)，触发 bug，用红色突出显示:

```sh
BUG: scheduling while atomic: rdwr_test_secre/23578/0x00000002
```

6.  内核现在转储了大量的诊断输出。首先要转储的是**调用栈**。

在*图 6.9* 中可以清楚地看到进程内核模式堆栈的调用堆栈或堆栈回溯(或“调用跟踪”)——这里是我们的用户空间应用`rdwr_drv_secret`，它正在进程上下文中运行我们的(有问题的)驱动程序代码。`Call Trace:`头后的每一行本质上都是内核堆栈上的一个调用帧。

作为提示，忽略以`?`符号开始的堆栈帧；它们实际上是有问题的调用帧，很可能是同一内存区域中先前堆栈使用的“剩余部分”。这里值得采取一个与内存相关的小转移:这是堆栈分配真正的工作方式；堆栈内存不是在每次调用帧的基础上分配和释放的，因为那样会非常昂贵。只有当一个堆栈内存页面耗尽时，一个新页面才会自动出现故障！(回想一下我们在配套指南 *Linux 内核编程-* *第 9 章*、*模块作者的内核内存分配–第 2 部分*中的讨论，在*内存分配和按需分页*一节中。)所以，现实是，当代码从函数中调用和返回时，同一个堆栈内存页往往会不断被重用。

不仅如此，出于性能原因，每次都不会擦除内存，导致之前帧的残羹剩饭经常出现。(他们可以随便“糟蹋”画面。然而，幸运的是，现代堆栈调用帧跟踪算法通常能够出色地找出正确的堆栈跟踪。)

遵循堆栈跟踪自下而上(*总是自下而上*读取)，我们可以看到，不出所料，我们的用户空间`write(2)`系统调用(它经常显示为(类似于)`SyS_write`或者，在 x86 上显示为`__x64_sys_write`，虽然在*图 6.9* 中不可见)调用了内核的 VFS 层代码(这里可以看到`vfs_write()`，它调用了`__vfs_write()`，这进一步调用了我们的驱动程序的写方法；也就是`write_miscdrv_rdwr()`！正如我们所知，这段代码调用了我们称之为`schedule_timeout()`的错误代码路径，该路径又调用了`schedule()`(和`__schedule()`)，导致整个 **`BUG: scheduling while atomic`** bug 被触发。

`scheduling while atomic`代码路径的格式是从下面一行代码中检索出来的，可以在`kernel/sched/core.c`中找到:

```sh
printk(KERN_ERR "BUG: scheduling while atomic: %s/%d/0x%08x\n", prev->comm, prev->pid, preempt_count());
```

有意思！在这里，您可以看到它打印了以下字符串:

```sh
      BUG: scheduling while atomic: rdwr_test_secre/23578/0x00000002
```

在`atomic:`之后，它打印进程名——PID——然后调用`preempt_count()`内联函数，该函数打印*抢占深度*；抢占深度是一个计数器，每次锁定时递增，每次解锁时递减。所以，如果它是正的，这意味着代码在一个关键的或原子的部分；在这里，它表现为价值`2`。

请注意，这个错误在这个测试运行期间被巧妙地解决了，正是因为`CONFIG_DEBUG_ATOMIC_SLEEP`调试内核配置选项被打开了。它之所以打开，是因为我们正在运行一个定制的“调试内核”(内核版本 5.4.0)！配置选项详细信息(您可以在`Kernel Hacking`菜单下的`make menuconfig`中交互查找和设置该选项)如下:

```sh
// lib/Kconfig.debug
[ ... ]
config DEBUG_ATOMIC_SLEEP
    bool "Sleep inside atomic section checking"
    select PREEMPT_COUNT
    depends on DEBUG_KERNEL
    depends on !ARCH_NO_PREEMPT
    help 
      If you say Y here, various routines which may sleep will become very 
 noisy if they are called inside atomic sections: when a spinlock is
 held, inside an rcu read side critical section, inside preempt disabled
 sections, inside an interrupt, etc...
```

### 在 5.4 非调试发行版内核上进行测试

作为对比测试，我们现在将在我们的 Ubuntu 20.04 LTS 虚拟机上执行同样的操作，我们将通过其默认的通用“发行版”5.4 Linux 内核进行引导，该内核通常是*而不是配置为“调试”内核*(这里`CONFIG_DEBUG_ATOMIC_SLEEP`内核配置选项尚未设置)。

首先，我们插入我们的(有问题的)驱动程序。然后，当我们运行我们的`rdwr_drv_secret`进程以便向驱动程序写入新的秘密时，错误的代码路径被执行。然而，这一次，内核*没有崩溃，也没有报告任何问题*(查看`dmesg(1)`输出验证了这一点):

```sh
$ uname -r
5.4.0-56-generic
$ sudo insmod ./miscdrv_rdwr_spinlock.ko buggy=1
$ ../../ch12/miscdrv_rdwr/rdwr_test_secret w /dev/llkd_miscdrv_rdwr_spinlock "passwdcosts500bucksdude"
Device file /dev/llkd_miscdrv_rdwr_spinlock opened (in write-only mode): fd=3
../../ch12/miscdrv_rdwr/rdwr_test_secret: wrote 24 bytes to /dev/llkd_miscdrv_rdwr_spinlock
$ dmesg 
[ ... ]
[ 65.420017] miscdrv_rdwr_spinlock:miscdrv_init_spinlock(): LLKD misc driver (major # 10) registered, minor# = 56, dev node is /dev/llkd_miscdrv_rdwr
[ 81.665077] miscdrv_rdwr_spinlock:miscdrv_exit_spinlock(): miscdrv_rdwr_spinlock: LLKD misc driver deregistered, bye
[ 86.798720] miscdrv_rdwr_spinlock:miscdrv_init_spinlock(): VERMAGIC_STRING = 5.4.0-56-generic SMP mod_unload 
[ 86.799890] miscdrv_rdwr_spinlock:miscdrv_init_spinlock(): LLKD misc driver (major # 10) registered, minor# = 56, dev node is /dev/llkd_miscdrv_rdwr
[ 130.214238] misc llkd_miscdrv_rdwr_spinlock: filename: "llkd_miscdrv_rdwr_spinlock"
                wrt open file: f_flags = 0x8001
                ga = 1, gb = 0
```

```sh
[ 130.219233] misc llkd_miscdrv_rdwr_spinlock: stats: tx=0, rx=0
[ 130.219680] misc llkd_miscdrv_rdwr_spinlock: rdwr_test_secre wants to write 24 bytes
[ 130.220329] misc llkd_miscdrv_rdwr_spinlock: 24 bytes written, returning... (stats: tx=0, rx=24)
[ 131.249639] misc llkd_miscdrv_rdwr_spinlock: filename: "llkd_miscdrv_rdwr_spinlock"
                ga = 0, gb = 1
[ 131.253511] misc llkd_miscdrv_rdwr_spinlock: stats: tx=0, rx=24
$ 
```

我们知道我们的写方法有一个致命的错误，但它似乎没有以任何方式失败！这真的很糟糕；正是这种事情会错误地让你认为你的代码很好，而实际上有一个讨厌的 bug 静静地躺在那里，等待某一天突然出现！

为了帮助我们调查引擎盖下到底发生了什么，让我们再次运行我们的测试应用(T0)进程，但这次是通过强大的`trace-cmd(1)`工具(Ftrace 内核基础设施上非常有用的包装器；以下是它的截断输出:

The Linux kernel's **Ftrace** infrastructure is the kernel's primary tracing infrastructure; it provides a detailed trace of pretty much every function that's been executed in the kernel space. Here, we are leveraging Ftrace via a convenient frontend: the `trace-cmd(1)` utility. These are indeed very powerful and useful debug tools; we've mentioned several others in the companion guide *Linux Kernel Programming -* *Chapter 1*, *Kernel Workspace Setup*, but unfortunately, the details are beyond the scope of this book. Check out the man pages to learn more.

```sh
$ sudo trace-cmd record -p function_graph -F ../../ch12/miscdrv_rdwr/rdwr_test_secret w /dev/llkd_miscdrv_rdwr_spinlock "passwdcosts500bucks"
$ sudo trace-cmd report -I -S -l > report.txt
$ sudo less report.txt
[ ... ]
```

输出可以在下面的截图中看到:

![](assets/63b163f7-5ce7-45f1-907e-b10a06909ef3.png)

Figure 6.10 – A partial screenshot of the trace-cmd(1) report output

如您所见，来自我们的用户模式应用的`write(2)`系统调用如预期的那样变成了`vfs_write()`，它本身(在安全检查之后)调用`__vfs_write()`，反过来调用我们的驱动程序的写方法–函数`write_miscdrv_rdwr()`！

在(大的)Ftrace 输出流中，我们可以看到`schedule_timeout()`函数确实被调用了:

![](assets/1cd0401a-b6a2-43e1-998d-10994995cdd6.png)

Figure 6.11 – A partial screenshot of the trace-cmd(1) report output, showing the (buggy!) calls to schedule_timeout() and schedule() within an atomic context

`schedule_timeout()`后的几行输出，我们可以清晰的看到`schedule()`被调用！于是，我们就有了:我们的司机(当然是故意的)做了一些错误的事情——在原子环境中称之为`schedule()`。但是，这里的关键点是，在这个 Ubuntu 系统上，我们运行的是*而不是*的“调试”内核，这就是为什么我们有以下内容:

```sh
$ grep DEBUG_ATOMIC_SLEEP /boot/config-5.4.0-56-generic
# CONFIG_DEBUG_ATOMIC_SLEEP is not set
$
```

这就是为什么这个 bug 没有被报告！这证明了在“调试”内核上运行测试用例——实际上是执行内核开发——的有用性，这是一个启用了许多调试功能的内核。(作为练习，如果您还没有这样做，准备一个“调试”内核，并在其上运行这个测试用例。)

**Linux Driver Verification (LDV) project**: In the companion guide *Linux Kernel Programming -* *Chapter 1*, *Kernel Workspace Setup*, in the section *The LDV – Linux Driver Verification – project*, we mentioned that this project has useful "rules" with respect to various programming aspects of Linux modules (drivers, mostly) as well as the core kernel.

With regard to our current topic, here's one of the rules: *Usage of spin lock and unlock functions* ([http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0039](http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0039)). It mentions key points with regard to the correct usage of spinlocks; interestingly, here, it shows an actual bug instance in a driver where a spinlock was attempted to be released twice – a clear violation of the locking rules, leading to an unstable system.

# 锁定和中断

到目前为止，我们已经学习了如何使用互斥锁，对于自旋锁，我们还学习了基本的`spin_[un]lock()`API。spinlock 上还有一些其他的 API 变体，我们将在这里研究更常见的变体。

为了确切地理解为什么您可能需要用于自旋锁的其他 API，让我们来看一个场景:作为驱动程序作者，您发现您正在处理的设备断言硬件中断；相应地，您为它编写中断处理程序。现在，在为您的驱动程序实现`read`方法时，您发现其中有一个非阻塞临界区。这很容易处理:正如您所了解的，您应该使用自旋锁来保护它。太好了。但是如果在`read`方法的关键部分，设备的硬件中断触发了呢？如你所知，*硬件中断抢占一切*；因此，控制将转到中断处理程序代码，抢占驱动程序的`read`方法。

这里的关键问题是:这是一个问题吗？这个答案既取决于你的中断处理器和你的`read`方法在做什么，也取决于它们是如何实现的。让我们想象几个场景:

*   中断处理程序(理想情况下)只使用局部变量，所以即使`read`方法在临界区，也真的没关系；中断处理将很快完成，控制权将交还给被中断的任何一方(同样，事情不止如此；如您所知，任何现有的下半部分，如小任务或 softirq，也可能需要执行)。换句话说，正因为如此，这种情况下真的没有种族。
*   中断处理程序正在处理(全局)共享可写数据，但不是您的读取方法正在使用的数据项。因此，同样，与读取的代码没有冲突和竞争。当然，你应该意识到的是，中断代码*确实有一个关键部分，它必须受到保护*(也许用另一个自旋锁)。
*   中断处理程序正在处理您的`read`方法正在使用的同一个全局共享可写数据。在这种情况下，我们可以看到一场比赛的潜力肯定存在，所以我们需要锁定！

让我们关注第三种情况。显然，我们应该使用自旋锁来保护中断处理代码中的关键部分(回想一下，当我们处于任何类型的中断上下文中时，都不允许使用互斥锁)。此外，*除非我们在`read`方法和中断处理程序的代码路径中使用完全相同的自旋锁*，否则它们根本不会受到保护！(使用锁时要小心；花时间仔细思考你的设计和代码。)

让我们试着让它更实际一点(现在用伪代码):假设我们有一个名为`gCtx`的全局(共享)数据结构；我们在驱动程序内的`read`方法和中断处理程序(hardirq 处理程序)中对其进行操作。因为它是共享的，所以它是一个关键部分，因此需要保护；由于我们在原子(中断)上下文中运行，我们*不能使用互斥体*，所以我们必须使用自旋锁来代替(这里，自旋锁变量被称为`slock`)。下面的伪代码显示了这种情况下的一些时间戳(`t1, t2, ...`):

```sh
// Driver read method ; WRONG ! driver_read(...)                  << time t0 >>
{
    [ ... ]
    spin_lock(&slock);
    <<--- time t1 : start of critical section >>
... << operating on global data object gCtx >> ...
    spin_unlock(&slock);
    <<--- time t2 : end of critical section >>
    [ ... ]
}                                << time t3 >>
```

以下伪代码用于设备驱动程序的中断处理程序:

```sh
handle_interrupt(...)           << time t4; hardware interrupt fires!     >>
{
    [ ... ]
    spin_lock(&slock);
    <<--- time t5: start of critical section >>
    ... << operating on global data object gCtx >> ...
    spin_unlock(&slock);
    <<--- time t6 : end of critical section >>
    [ ... ]
}                               << time t7 >> 
```

这可以用下图来概括:

![](assets/3d9d5f60-bb83-44a5-a694-2a05f28df4f8.png)

Figure 6.12 – Timeline – the driver's read method and hardirq handler run sequentially when working on global data; there's no issues here

幸运的是，一切都很顺利——“幸运”是因为硬件中断在`read`功能的关键部分完成后触发了*。当然，我们不能指望运气作为我们产品的独家安全标志！硬件中断是异步的；如果它在一个不太合适的时间(对我们来说)启动了呢——比如说，当`read`方法的关键部分在时间 T1 和 t2 之间运行的时候？那么，spinlock 是不是要做好它的工作，保护我们的数据呢？*

此时，中断处理程序的代码将尝试获取相同的自旋锁(`&slock`)。等一下-它无法“获取”它，因为它当前已被锁定！在这种情况下，它会“旋转”，实际上是在等待解锁。但是怎么才能解锁呢？它不能，我们有它:一个**(自我)僵局**。

有趣的是，自旋锁更直观，在 SMP(多核)系统上也有意义。我们假设`read`方法运行在 CPU 核心 1 上；中断可以在另一个中央处理器内核上传递，比如说内核 2。中断代码路径将在 CPU 内核 2 上的锁上“旋转”，而内核 1 上的`read`方法完成关键部分，然后解锁旋转锁，从而解锁中断处理程序。但是在 **UP** ( **单处理器**，只有一个 CPU 内核)上呢？那么它将如何工作呢？啊，这就是这个难题的解决方案:当与中断“赛跑”时，*不管是单处理器还是 SMP，只需使用 spinlock API* 的 `_irq` *变体* *:*

```sh
#include <linux/spinlock.h>
void spin_lock_irq(spinlock_t *lock);
```

`spin_lock_irq()` API 在内部禁用它运行的处理器内核上的中断；也就是本地核心。因此，通过在我们的`read`方法中使用这个应用编程接口，中断将在本地内核上被禁用，从而使任何可能的“竞争”不可能通过中断实现。(如前所述，如果中断确实在另一个中央处理器内核上触发，自旋锁技术将简单地像广告宣传的那样工作！)

The `spin_lock_irq()` implementation is pretty nested (as with most of the spinlock functionality), yet fast; down the line, it ends up invoking the `local_irq_disable()` and `preempt_disable()` macros, disabling both interrupts and kernel preemption on the local processor core that it's running on. (Disabling hardware interrupts has the (desirable) side effect of disabling kernel preemption as well.)

`spin_lock_irq()`与相应的`spin_unlock_irq()`原料药配对。因此，这个场景中自旋锁的正确用法(与我们之前看到的相反)如下:

```sh
// Driver read method ; CORRECT ! driver_read(...)                  << time t0 >>
{
    [ ... ]
    spin_lock_irq(&slock);
    <<--- time t1 : start of critical section >>
*[now all interrupts + preemption on local CPU core are masked (disabled)]*
... << operating on global data object gCtx >> ...
    spin_unlock_irq(&slock);
    <<--- time t2 : end of critical section >>
    [ ... ]
}                                << time t3 >>
```

在拍拍自己的背，休息一天之前，让我们考虑另一种情况。这一次，在一个更复杂的产品(或项目)上，很有可能在几个开发代码库的开发人员中，有人故意将中断掩码设置为某个值，从而在允许其他中断的同时阻止了一些中断。为了我们的例子，让我们假设这已经在更早的时间点`t0`发生了。现在，正如我们之前描述的，另一个开发人员(你！)出现，并且为了保护驱动程序读取方法内的关键部分，使用了`spin_lock_irq()` API。听起来没错，对吧？是的，但是这个应用编程接口有能力*关闭(屏蔽)本地中央处理器内核上的所有硬件中断*(以及内核抢占，我们现在将忽略)。它通过在较低的级别上操作(非常特殊的)硬件中断屏蔽寄存器来实现这一点。假设将对应于中断的位设置为`1`将启用该中断，而将该位清零(至`0`)将禁用或屏蔽该中断。因此，我们可能会出现以下情况:

*   时间`t0`:中断屏蔽设置为某个值，比如`0x8e (10001110b)`，启用一些中断，禁用一些中断。这对项目很重要(这里，为了简单起见，我们假设有一个 8 位屏蔽寄存器)
    *[...时间流逝...].*
*   时间`t1`:就在进入驾驶员`read`法的关键路段之前，呼叫
    `spin_lock_irq(&slock);`。该 API 将具有清除注册到`0`的中断掩码中所有位的内部效果，从而禁用所有中断(正如我们*所认为的*所希望的那样)。
*   时间`t2`:现在，硬件中断无法在这个 CPU 核心上触发，所以我们继续完成关键部分。一旦完成，我们称之为`spin_unlock_irq(&slock);`。该应用编程接口的内部作用是将中断屏蔽寄存器中的所有位设置为`1`，重新启用所有中断。

然而，中断屏蔽寄存器现在已经被错误地“恢复”到一个值`0xff (11111111b)`，*而不是最初的开发者想要的、要求的和假设的值*！这可以(也可能会)打破项目中的某些东西。

解决方法很简单:不要假设任何事情，**只需保存并恢复中断屏蔽**。这可以通过以下应用编程接口对来实现:

```sh
#include <linux/spinlock.h>>
 unsigned long spin_lock_irqsave(spinlock_t *lock, unsigned long flags);
 void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags);
```

锁定和解锁函数的第一个参数是要使用的 spinlock 变量。第二个参数`flags`、*必须是`unsigned long`类型的局部变量*。这将用于保存和恢复中断屏蔽:

```sh
spinlock_t slock;
spin_lock_init(&slock);
[ ... ]
driver_read(...) 
{
    [ ... ]
    spin_lock_irqsave(&slock, flags);
    << ... critical section ... >>
    spin_unlock_irqrestore(&slock, flags);
    [ ... ]
}
```

To be pedantic, `spin_lock_irqsave()` is not an API, but a macro; we've shown it as an API for readability. Also, although the return value of this macro is not void, it's an internal detail (the `flags` parameter variable is updated here).

如果一个小任务或者一个 softirq(一个下半部分的中断机制)有一个与你的进程上下文代码路径“竞争”的关键部分呢？在这种情况下，可能需要使用`spin_lock_bh()`例程，因为它可以禁用本地处理器的下半部分，然后获取自旋锁，从而保护关键部分(类似于`spin_lock_irq[save]()`通过禁用本地内核上的硬件中断来保护进程上下文中的关键部分的方式):

```sh
void spin_lock_bh(spinlock_t *lock);
```

当然，*开销*在对性能高度敏感的代码路径中确实很重要(网络堆栈就是一个很好的例子)。因此，使用最简单形式的自旋锁将有助于更复杂的变体。话虽如此，但肯定会有一些场合需要使用更强形式的 spinlock API。例如，在 5.4.0 Linux 内核上，这是我们看到的不同形式的 spinlock APIs 的使用实例数量的近似值:`spin_lock()`:超过 9400 个使用实例；`spin_lock_irq()`:超过 3600 个使用实例；`spin_lock_irqsave()`:超过 15，000 个使用实例；`spin_lock_bh()`:超过 3700 个使用实例。(我们不由此得出任何重大推论；只是我们希望指出，使用更强形式的 spinlock APIs 在 Linux 内核中相当普遍)。

最后，让我们对 spinlock 的内部实现做一个非常简短的说明:就幕后内部而言，实现往往是非常特殊的代码，通常由在微处理器上执行非常快的原子机器语言指令组成。例如，在流行的 x86[_64]架构上，自旋锁最终归结为自旋锁结构成员上的*原子测试和设置*机器指令(通常通过`cmpxchg`机器语言指令实现)。在 ARM 机器上，正如我们前面提到的，通常是`wfe`(等待事件，以及**设置事件** ( **SEV** )机器指令处于实现的核心。(您可以在*进一步阅读*部分找到关于其内部实现的资源)。无论如何，作为内核或驱动程序作者，在使用自旋锁时，您应该只使用公开的 API(和宏)。

## 使用自旋锁–快速总结

让我们快速总结一下自旋锁:

*   **最简单，开销最低**:在保护进程上下文中的关键部分时，使用非 irq 自旋锁原语，`spin_lock()` / `spin_unlock()`(要么没有中断需要处理，要么有中断，但我们根本不与它们竞争；实际上，当中断不起作用或无关紧要时使用这个)。
*   **中等开销**:使用 IRQ-disable(以及内核抢占禁用)版本`spin_lock_irq() / spin_unlock_irq()`，当中断正在进行并且确实重要时(进程和中断上下文可以“竞争”；也就是说，它们共享全局数据)。
*   **最强(相对)，高开销**:这是使用自旋锁最安全的方式。除了通过`spin_lock_irqsave()` / `spin_unlock_irqrestore()`对中断掩码执行保存和恢复之外，它与介质开销相同，以保证先前的中断掩码设置不会被无意中覆盖，这在先前的情况下是可能发生的。

正如我们之前看到的，自旋锁——在等待锁时在其上运行的处理器上“旋转”的意义——在 UP 上是不可能的(当另一个线程同时在同一个 CPU 上运行时，如何在一个可用的 CPU 上旋转？).事实上，在 UP 系统上，spinlock APIs 唯一真正的效果是它可以在处理器上禁用硬件中断和内核抢占！然而，在 SMP(多核)系统上，旋转逻辑实际上发挥了作用，因此锁定语义按预期工作。但是坚持住——这不应该给你压力，萌芽中的内核/驱动开发者；事实上，关键是您应该简单地使用所描述的 spinlock APIs，您将永远不必担心 UP 与 SMP 的对比；做什么和不做什么的细节都被内部实现所隐藏。

Though this book is based on the 5.4 LTS kernel, a new feature was added to the 5.8 kernel from the **Real-Time Linux** (**RTL**, previously called PREEMPT_RT) project, which deserves a quick mention here: "**local locks**". While the main use case for local locks is for (hard) real-time kernels, they help with non-real-time kernels too, mainly for lock debugging via static analysis, as well as runtime debugging via lockdep (we cover lockdep in the next chapter). Here's the LWN article on the subject: [https://lwn.net/Articles/828477/](https://lwn.net/Articles/828477/).

至此，我们完成了关于自旋锁的部分，自旋锁是 Linux 内核中非常常见的密钥锁，几乎所有子系统都使用它，包括驱动程序。

# 摘要

祝贺你完成这一章！

理解并发性及其相关问题对于任何软件专业人员来说都是至关重要的。在这一章中，您学习了关于关键部分的关键概念，在这些部分中独占执行的需要，以及原子性的含义。然后，您了解了*为什么*我们在为 Linux 操作系统编写代码时需要关注并发性。之后，我们详细研究了实际的锁定技术——互斥锁和自旋锁。你也学会了什么时候应该用什么锁。最后，学习了当硬件中断(及其可能的下半部分)发生时如何处理并发问题。

但是我们还没有完成！我们需要了解更多的概念和技术，这正是我们在本书下一章，也是最后一章将要做的。我建议你先浏览一下这一章的内容，以及*进一步阅读*部分的资源和提供的练习，然后再进入最后一章！

# 问题

作为我们的总结，这里有一个问题列表，供您测试您对本章材料的知识:[https://github . com/packt publishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions)。你会在这本书的 GitHub repo 中找到一些问题的答案:[https://GitHub . com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions _ to _ assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn)。

# 进一步阅读

为了帮助您用有用的材料更深入地研究这个主题，我们在本书的 GitHub 存储库中的进一步阅读文档中提供了一个相当详细的在线参考资料和链接列表(有时甚至是书籍)。*进一步阅读*文档可在此处获得:[https://github . com/packt publishing/Linux-Kernel-Programming/blob/master/进一步阅读. md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md) 。**