# 五、使用内核定时器、线程和工作队列

如果设备驱动程序的低级规范要求在执行`func_a()`和`func_b()`之间应该有 50 毫秒的延迟，该怎么办？此外，根据您的情况，当您在进程或中断上下文中运行时，延迟应该有效。如果在驱动程序的另一部分中，您需要异步地和周期性地(比如说，每秒钟)执行某种监控功能，会怎么样？或者您需要让一个线程(或几个线程)在后台但在内核中默默执行工作吗？

这些都是各种软件中非常常见的需求，包括我们所处的宇宙角落——Linux 内核模块(和驱动)开发！在本章中，您将学习如何在内核空间中运行时设置、理解和使用延迟，以及如何使用内核定时器、内核线程和工作队列。

在本章中，您将学习如何以最佳方式执行这些任务。简而言之，我们将涵盖以下主题:

*   在内核中延迟给定的时间
*   设置和使用内核定时器
*   创建和使用内核线程
*   使用内核工作队列

我们开始吧！

# 技术要求

我假设您已经完成了前言部分，以充分利用这本书，并适当地准备了一个运行 Ubuntu 18.04 LTS(或更高的稳定版本)的来宾虚拟机，并安装了所有必需的软件包。如果没有，我强烈建议你先做这个。为了最大限度地利用这本书，我强烈建议您首先设置工作空间环境，包括克隆这本书的 GitHub 代码存储库，并以动手的方式进行处理。存储库可以在这里找到:[https://github . com/PacktPublishing/Linux-内核-编程-Part-2](https://github.com/PacktPublishing/Linux-Kernel-Programming-Part-2/tree/main/ch5) 。

# 在内核中延迟给定的时间

通常，您的内核或驱动程序代码需要等待给定的时间，然后才能继续执行下一条指令。这可以通过一组延迟 API 在 Linux 内核空间中实现。从一开始，需要理解的一个关键点是，您可以通过两种广泛的方式来实施延迟:

*   通过非阻塞或原子 API 的延迟永远不会导致休眠进程发生(换句话说，它永远不会超时)
*   通过阻塞导致当前进程上下文休眠的 API 来延迟(换句话说，通过调度)

(正如我们在配套指南 *Linux 内核编程中详细介绍的那样，*我们关于 CPU 调度的章节*第 10 章，**CPU 调度器–第 1 部分*和 *第 11 章**CPU 调度器–第 2 部分*)，将进程上下文置于内部休眠状态意味着内核的核心`schedule()`功能在某个时刻被调用，最终导致上下文切换发生。这就引出了一个非常重要的问题(我们之前提到过！):在任何类型的原子或中断上下文中运行时，您永远不要调用`schedule()`。

通常，就像我们这里插入延迟的情况一样，您必须弄清楚您打算插入延迟的代码运行在什么上下文中。我们在配套指南 *Linux 内核编程-* *第 6 章**内核内部要素–进程和线程*中的*确定上下文*一节中介绍了这一点；如果你不清楚，请回头参考。(我们在 [*第 4 章*](4.html)*处理硬件中断*中对此进行了更详细的描述。)

接下来，仔细想一想:如果你确实处于原子(或中断)上下文中，真的有必要延迟吗？原子或中断上下文的全部意义在于它的执行被限制在尽可能短的持续时间内；强烈建议您这样设计。这意味着您不会在原子代码中插入延迟，除非您无法避免这样做。

*   **使用第一种类型**:这些是非阻塞或原子 API，永远不会导致睡眠发生。当您的代码处于原子(或中断)上下文中，并且您确实需要一个持续时间很短的非阻塞延迟时，您应该使用它；但是这有多短呢？根据经验，将这些 API 用于 1 毫秒或更短的非阻塞原子延迟。即使您需要在原子上下文中延迟一毫秒以上——比如说，在中断处理程序的代码中(*)但为什么要在中断中延迟呢！？*)–使用这些`*delay()`API(字符`*`表示通配符；在这里，你会看到，它暗示了`ndelay()`、`delay()`和`mdelay()`的套路。
*   **使用第二种类型**:这些是导致当前进程上下文休眠的阻塞 API。当您的代码处于流程(或任务)上下文中时，您应该使用它，因为延迟本质上是阻塞的，并且持续时间更长；实际上，延迟超过一毫秒。这些内核 API 遵循`*sleep()`的形式。(同样，在不涉及太多细节的情况下，考虑一下这个问题:如果你在一个过程上下文*中，但是*在一个自旋锁的关键部分中，这是一个原子上下文——如果你必须包含一个延迟，那么你必须使用`*delay()`API！我们将在这本书的最后两章讨论自旋锁和更多内容。)

现在，让我们看看这些内核 API，看看它们是如何使用的。我们将从查看`*delay()`原子 API 开始。

## 了解如何使用*delay()原子 API

废话不多说，我们来看一个表，它快速总结了可用的(对我们模块作者来说)非阻塞或原子`*delay()`内核 APIs*它们应该用在任何种类的原子或中断上下文中，在这些上下文中，您不能阻塞或休眠*(或调用`schedule()`):

| API | comment |
| `ndelay(ns);` | 延迟为`ns`纳秒。 |
| `udelay(us);` | 延迟`us`微秒。 |
| `mdelay(ms);` | 延迟`ms`毫秒。 |

Table 5.1 – The *delay() non-blocking APIs

关于这些 API、它们的内部实现以及它们的使用，有几点需要注意:

*   在使用这些宏/API 时，一定要包含`<linux/delay.h>`头。
*   您需要根据必须延迟的时间调用适当的例程；例如，如果需要执行 30 毫秒的原子无阻塞延迟，则应该调用`mdelay(30)`而不是`udelay(30*1000)`。内核代码提到了这一点:`linux/delay.h`–*“对于高 loops_per_jiffy(高 bogomips)机器来说，使用 udelay()的时间间隔超过几毫秒可能会有溢出的风险……”。*
*   这些 API 的内部实现和 Linux 上的许多 API 一样，是细致入微的:在`<linux/delay.h>`头中有这些函数(或者宏，视情况而定)的更高级抽象实现；在特定于 arch 的标头(`<asm-<arch>/delay.h>` *或* `<asm-generic/delay.h>`)中，通常有一个低级的特定于 arch 的实现；其中`arch`当然是指 CPU)，它将在调用时自动覆盖高级版本(链接器将确保这一点)。

*   在当前的实现中，这些 API 最终归结为`udelay()`之上的包装器；这个函数本身归结为一个紧密的汇编循环，执行所谓的“忙循环”！(对于 x86，代码可以在`arch/x86/lib/delay.c:__const_udelay()`中找到)。在启动过程的早期，内核没有深入到血淋淋的细节，而是校准了几个值:所谓的**bogomips–**伪 MIPS–**每一瞬间循环次数** ( **lpj** )值。本质上，内核计算出，在那个特定的系统上，一个循环必须重复多少次，才能经过一次计时器滴答或一瞬间。这个值被称为系统的 bogomips 值，可以在内核日志中看到。例如，在我的酷睿 i7 笔记本电脑上，它如下所示:

```sh
Calibrating delay loop (skipped), value calculated using timer frequency.. 5199.98 BogoMIPS (lpj=10399968)
```

*   对于超过`MAX_UDELAY_MS`的延迟(设置为 5 毫秒)，内核将在内部循环调用`udelay()`函数。

请记住，当您在任何类型的原子上下文中需要延迟时，必须使用`*delay()`API，例如中断处理程序(上半部分或下半部分)，因为它们保证永远不会发生休眠，因此不会调用`schedule()`。提醒(我们在 [*第四章*](4.html)*处理硬件中断*中提到了这一点):`might_sleep()`作为调试辅助；内核(和驱动程序)在进程上下文中运行代码的代码库中的地方内部使用`might_sleep()`宏；也就是它可以睡觉的地方。现在，如果`might_sleep()`曾经在原子上下文中被调用过，那就大错特错了——一个嘈杂的 printk 堆栈跟踪随后被发出，从而帮助你及早发现并解决这些问题。您也可以在流程上下文中使用这些`*delay()`应用编程接口。

In these discussions, you will often come across the `jiffies` kernel variable; essentially, think of `jiffies` as a global unsigned 64-bit value that is incremented on every timer interrupt (or timer tick; it's internally protected against overflow). Thus, the continually incrementing variable is used as a way to measure uptime, as well as a means of implementing simple timeouts and delays.

现在，让我们看看第二种可用的延迟 APIs 阻塞型。

## 了解如何使用*sleep()阻塞 API

让我们看看另一个表格，它快速总结了可用的(对我们模块作者来说)阻塞`*sleep*()`内核 APIs 这些是*只打算在过程上下文中使用，当它是安全的睡眠*；也就是说`schedule()`的调用不是问题。换句话说，延迟是由进程上下文实现的，它在延迟期间实际上处于睡眠状态，然后在完成时被唤醒:

| API | **内部“有”【T1 支持】** | comment |
| `usleep_range(umin, umax);` | `hrtimers`(高分辨率计时器) | 睡眠时间在`umin`和`umax`微秒之间。在唤醒时间灵活的地方使用。这是**推荐使用的原料药**。 |
| `msleep(ms);` | `jiffies` / `legacy_timers` | 睡眠`ms`毫秒。通常指持续时间为 10 毫秒或更长的睡眠。 |
| `msleep_interruptible(ms);` | `jiffies` / `legacy_timers` | `msleep(ms);`的可中断变体。 |
| `ssleep(s);` | `jiffies` / `legacy_timers` | 睡眠`s`秒。这是用来睡觉的> 1 s(包裹`msleep()`)。 |

Table 5.2 – The *sleep*() blocking APIs

关于这些 API、它们的内部实现以及它们的使用，有几点需要注意:

*   使用这些宏/API 时，确保包含`<linux/delay.h>`头。
*   所有这些`*sleep()`API 都是以这样一种方式在内部实现的:它们*使当前流程上下文休眠*(即通过内部调用`schedule()`)；因此，当然，它们必须只在“睡眠安全”的过程上下文中被调用。同样，仅仅因为您的代码在流程上下文中并不一定意味着它是安全的；例如，自旋锁的临界区是原子的；因此，你不能在那里调用前面提到的`*sleep()`API！
*   我们提到`usleep_range()`是**首选/推荐的**原料药，当你想要短时间睡眠时使用——但是为什么呢？这一点在*中会变得更加清晰，让我们试试——延迟和睡眠真的需要多长时间？*节。

如您所知，Linux 上的睡眠有两种类型:可中断的和不间断的。后者意味着没有信号任务可以“打扰”睡眠。因此，当您调用`msleep(ms);`时，通过内部调用以下内容，我将当前流程上下文置于`ms`睡眠状态:

```sh
__set_current_state(TASK_UNINTERRUPTIBLE);
return schedule_timeout(timeout);
```

`schedule_timeout()`例程通过设置内核定时器工作(我们的下一个话题！)将在期望的时间到期，然后通过调用`schedule()`立即使进程进入睡眠状态！(好奇的人可以在这里偷看一下它的代码:`kernel/time/timer.c:schedule_timeout()`)。)除了称呼`__set_current_state(TASK_INTERRUPTIBLE);`之外，`msleep_interruptible()`的实现非常相似。作为设计启发式，遵循*的 UNIX 范式提供机制，而不是策略*；这样，在以下情况下调用`msleep_interruptible()`可能是一个好主意:如果 userspace 应用中止了工作(也许是通过用户按下`^C`，内核或驱动程序顺从地释放了任务:它的进程上下文被唤醒，它运行适当的信号处理程序，生命继续。在内核空间不受用户生成的信号干扰这一点很重要的情况下，使用`msleep()`变体。

同样，根据经验，根据延迟的持续时间，使用以下 API:

*   **延迟超过 10 毫秒** : `msleep()`或`msleep_interruptible()`
*   **延迟超过 1 秒** : `ssleep()`

正如你所料，`ssleep()`是`msleep();`的简单包装，变成了`msleep(seconds * 1000);`。

实现用户空间`sleep(3)` API 的(近似)等价的一个简单方法可以在我们的`convenient.h`头中看到；本质上，它采用了`schedule_timeout()`原料药:

```sh
#ifdef __KERNEL__
void delay_sec(long);
/*------------ delay_sec --------------------------------------------------
 * Delays execution for @val seconds.
 * If @val is -1, we sleep forever!
 * MUST be called from process context.
 * (We deliberately do not inline this function; this way, we can see it's
 * entry within a kernel stack call trace).
 */
void delay_sec(long val)
{
    asm (""); // force the compiler to not inline it!
    if (in_task()) {
        set_current_state(TASK_INTERRUPTIBLE);
        if (-1 == val)
            schedule_timeout(MAX_SCHEDULE_TIMEOUT);
        else
            schedule_timeout(val * HZ);
    } 
}
#endif /* #ifdef __KERNEL__ */
```

既然您已经学会了如何延迟(是的，请微笑)，让我们继续学习一个有用的技能:时间戳内核代码。这允许您快速计算特定代码执行所需的时间。

## 获取内核代码中的时间戳

当内核使用这个工具时，能够获取准确的时间戳是很重要的。例如，`dmesg(1)`实用程序以`seconds.microseconds`格式显示系统启动后的时间；Ftrace 跟踪通常显示函数执行所需的时间。在用户模式下，我们经常使用`gettimeofday(2)`系统调用来获取时间戳。在内核中，存在几个接口；通常，`ktime_get_*()`系列例程用于获取准确的时间戳。出于我们的目的，以下例程很有用:

```sh
u64 ktime_get_real_ns(void);
```

该例程通过`ktime_get_real()` API 在内部查询挂钟时间，然后将结果转换为纳秒量。我们这里就不打扰内部细节了。此外，该应用编程接口的几个变体是可用的；例如，`ktime_get_real_fast_ns()`、`ktime_get_real_ts64()`等等。前者既快速又安全。

现在您知道如何获取时间戳了，您可以计算出一些代码需要多长时间才能执行到很高的精度，纳秒级的分辨率也不逊色！您可以使用以下伪代码来实现这一点:

```sh
#include <linux/ktime.h>
t1 = ktime_get_real_ns();
foo();
bar();
t2 = ktime_get_real_ns();
time_taken_ns = (t2 -> t1);
```

这里，计算(虚构的)`foo()`和`bar()`函数执行所花费的时间，并且结果(以纳秒为单位)可以在`time_taken_ns`变量中获得。`<linux/ktime.h>`内核头本身包括`<linux/timekeeping.h>`头，这是`ktime_get_*()`系列例程定义的地方。

A macro to help you calculate the time taken between two timestamps has been provided in our `convenient.h` header file: `SHOW_DELTA(later, earlier);`. Ensure that you pass the later timestamp as the first parameter and the first timestamp as the second parameter.

下一节中的代码示例将帮助我们采用这种方法。

## 让我们试试——延迟和睡眠真的需要多长时间？

现在，您已经知道如何使用`*delay()`和`*sleep()`API 来构造延迟和休眠(分别是非阻塞和阻塞)。不过，请稍等，我们还没有在内核模块中真正尝试过。不仅如此，延迟和睡眠是否像我们被引导相信的那样准确？让我们像往常一样*实证*(这很重要！)而不做任何假设。让我们自己来试试吧！

我们将在本小节中看到的演示内核模块按顺序执行两种延迟:

*   首先，它使用`*delay()`例程(您在*了解如何使用*delay()原子* *APIs* 部分中了解到)来实现 10 ns、10 us 和 10 ms 的原子非阻塞延迟
*   接下来，它使用`*sleep()`例程(您在*了解如何使用*sleep()阻塞**API*部分中了解到的)来实现 10 us、10 ms 和 1 秒的阻塞延迟。

我们这样称呼代码:

```sh
DILLY_DALLY("udelay() for     10,000 ns", udelay(10));
```

这里，`DILLY_DALLY()`是自定义宏。其实施如下:

```sh
// ch5/delays_sleeps/delays_sleeps.c
/*
 * DILLY_DALLY() macro:
 * Runs the code @run_this while measuring the time it takes; prints the string
 * @code_str to the kernel log along with the actual time taken (in ns, us
 * and ms).
 * Macro inspired from the book 'Linux Device Drivers Cookbook', PacktPub.
 */
#define DILLY_DALLY(code_str, run_this) do {    \
    u64 t1, t2;                                 \
    t1 = ktime_get_real_ns();                   \
 run_this;                                   \
 t2 = ktime_get_real_ns();                   \
    pr_info(code_str "-> actual: %11llu ns = %7llu us = %4llu ms\n", \
        (t2-t1), (t2-t1)/1000, (t2-t1)/1000000);\
} while(0)
```

这里，我们简单地实现了时间增量计算；一个好的实现将包括检查`t2`的值是否大于`t1`，是否发生溢出，等等。

对于各种延迟和休眠，我们在内核模块的`init`函数中调用它，如下所示:

```sh
    [ ... ]
    /* Atomic busy-loops, no sleep! */
    pr_info("\n1\. *delay() functions (atomic, in a delay loop):\n");
    DILLY_DALLY("ndelay() for         10 ns", ndelay(10));
    /* udelay() is the preferred interface */
    DILLY_DALLY("udelay() for     10,000 ns", udelay(10));
    DILLY_DALLY("mdelay() for 10,000,000 ns", mdelay(10));

    /* Non-atomic blocking APIs; causes schedule() to be invoked */
    pr_info("\n2\. *sleep() functions (process ctx, sleeps/schedule()'s out):\n");
    /* usleep_range(): HRT-based, 'flexible'; for approx range [10us - 20ms] */
    DILLY_DALLY("usleep_range(10,10) for 10,000 ns", usleep_range(10, 10));
    /* msleep(): jiffies/legacy-based; for longer sleeps (> 10ms) */
    DILLY_DALLY("msleep(10) for      10,000,000 ns", msleep(10));
    DILLY_DALLY("msleep_interruptible(10)         ", msleep_interruptible(10));
    /* ssleep() is a wrapper over msleep(): = msleep(ms*1000); */
    DILLY_DALLY("ssleep(1)                        ", ssleep(1));
```

以下是内核模块在我们值得信赖的 x86_64 Ubuntu 虚拟机上运行时的一些示例输出:

![](assets/f9d48d06-517c-4f1c-8883-91e2d9d6f34e.png)

Figure 5.1 – A partial screenshot showing the output of our delays_sleeps.ko kernel module

仔细研究前面的输出；奇怪的是`udelay(10)`和`mdelay(10)`例程似乎都在期望的延迟期到期之前完成了它们的执行*(在我们的示例输出中，分别在`9 us`和`9 ms`)！怎么会这样现实是**`*delay()`套路往往完成得更早**。内核源代码中记录了这一事实。让我们看一下代码的相关部分(它是不言自明的):*

```sh
// include/linux/delay.h
/*
 [ ... ]
 * Delay routines, using a pre-computed "loops_per_jiffy" value.
 *
 * Please note that ndelay(), udelay() and mdelay() may return early for
 * several reasons:
 * 1\. computed loops_per_jiffy too low (due to the time taken to
 * execute the timer interrupt.)
 * 2\. cache behavior affecting the time it takes to execute the
 * loop function.
 * 3\. CPU clock rate changes.
 *
 * Please see this thread:
 * http://lists.openwall.net/linux-kernel/2011/01/09/56
```

`*sleep()`套路有反特点；他们几乎总是倾向于让 T2 睡得比 T4 长。同样，在非实时操作系统(如标准 Linux)中，这些也是意料之中的问题。

您可以通过几种方式**缓解这些问题**:

*   在标准 Linux 上，在用户模式下，执行以下操作:
    *   首先，最好使用**高分辨率定时器(HRT)** 接口，精度高。同样，这是从 RTL 项目合并到主流 Linux 的代码(早在 2006 年)。它支持要求分辨率低于单个 *jiffy* 的定时器(如您所知，它与内核`CONFIG_HZ`值定时器“tick”紧密耦合)；例如，`HZ`值为 100 时，一个 jiffy 为 1000/100 = 10ms；`HZ`250，一瞬间是 4 毫秒，以此类推。
    *   一旦你做到了这一点，为什么不使用 Linux 的软实时调度功能呢？在这里，您可以为您的用户模式线程指定`SCHED_FIFO`或`SCHED_RR`的调度策略和高优先级(范围为`1`到`99`；我们在配套指南 *Linux 内核编程-* *第 10 章* *中央处理器调度程序–第 1 部分*中介绍了这些细节。

Most modern Linux systems will have HRT support. However, how do you exploit it? This is simple: you're recommended to write your timer code *in user space* and employ standard POSIX timer APIs (such as the `timer_create(2)` and `timer_settime(2)` system calls). Since this book is concerned with kernel development, we won't delve into these user space APIs here. In fact, this topic was covered in some detail in my earlier book, *Hands-On System Programming with Linux*, in *Chapter 13, Timers*, in the *The newer POSIX (interval) timers mechanism* section.

*   当您在内核中使用这些延迟和睡眠 API 时，内核开发人员会不厌其烦地清晰地记录一些优秀的建议。在官方内核文档中浏览这个文档真的很重要:https://www . kernel . org/doc/Documentation/timers/timers-how to . rst。
*   将 Linux 操作系统配置并构建为 RTOS；这将显著降低调度“抖动”(我们在配套指南 *Linux 内核编程-* *第 11 章、**CPU 调度器–第 2 部分*中的*将主线 Linux 转换为 RTOS* 部分中详细介绍了该主题)。

有趣的是，使用我们“更好”的 Makefile 的 checkpatch 目标可能是一个真正的福音。让我们看看它(内核的检查补丁 Perl 脚本)捕获了什么(首先确保您在正确的源目录中):

```sh
$ cd <...>/ch5/delays_sleeps $ make checkpatch 
make clean
[ ... ]
--- cleaning ---
[ ... ]
--- kernel code style check with checkpatch.pl ---

/lib/modules/5.4.0-58-generic/build/scripts/checkpatch.pl --no-tree -f --max-line-length=95 *.[ch]
[ ... ]
WARNING: usleep_range should not use min == max args; see Documentation/timers/timers-howto.rst
#63: FILE: delays_sleeps.c:63:
+ DILLY_DALLY("XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX", usleep_range(10, 10));

total: 0 errors, 2 warnings, 79 lines checked
[ ... ]
```

那真是太好了！确保您使用我们的“更好的”`Makefile`中的目标(我们在配套指南 *Linux 内核编程-* *第 5 章，编写您的第一个内核模块 LKMs–第 2 部分*中的*内核模块的“更好的”Makefile 模板*部分中详细介绍了这一点)。

至此，我们已经看完了内核延迟和内核休眠。以此为基础，您现在将在本章的剩余部分学习如何设置和使用内核定时器、内核线程和工作队列。

## “sed”驱动程序–演示内核定时器、kthreads 和工作队列

为了使这一章更加有趣和实际操作，我们将开始开发一个名为**简单加密解密**或 **sed** 的杂项类字符“驱动程序”(不要与众所周知的`sed(1)`实用程序混淆)。不，你不会因为猜测它提供了某种非常简单的文本加密/解密支持而获得大奖。

这里的要点是，我们应该想象在这个驱动程序的规范中，有一个条款要求工作(实际上，加密/解密功能)在给定的时间间隔内执行——实际上，在给定的截止日期内*。为了检查这一点，我们将设计我们的驱动程序，使它有一个内核定时器，将在给定的时间间隔内到期；驱动程序将检查功能是否确实在这个时间限制内完成！*

我们将开发一系列`sed`驱动程序及其用户空间对应程序(应用):

*   第一个驱动程序-`sed1`驱动程序和用户模式应用(`ch5/sed1`)将执行我们刚刚描述的内容:演示用户模式应用将使用`ioctl`系统调用与驱动程序接口，并使加密/解密消息功能正常运行。驱动程序将关注一个内核定时器，我们将设置它在给定的截止日期到期。如果它确实过期，我们认为操作失败了；如果没有，计时器被取消，操作成功。
*   第二个版本`sed2` ( `ch5/sed2`)将与`sed1`相同，除了这里的实际加密/解密消息功能将在单独创建的内核线程的上下文中执行！这改变了项目的设计。
*   第三个版本`sed3` ( `ch5/sed3`)将再次执行与`sed1`和`sed2`相同的操作，只是这次实际的加密/解密消息功能将由内核工作队列执行！

既然您已经学习了如何执行延迟(原子延迟和阻塞延迟)以及捕获时间戳，那么让我们学习如何设置和使用内核定时器。

# 设置和使用内核定时器

一个**定时器**为软件提供了一种方法，当指定的时间量过去时，它会被异步通知。用户和内核空间中的各种软件都需要定时器；这通常包括网络协议实现、块层代码、设备驱动程序和各种内核子系统。这个计时器提供了一种异步通知的方式，因此允许驱动程序与正在运行的计时器并行执行工作。出现的一个重要问题是，*我怎么知道计时器什么时候到期？*在用户空间应用中，通常内核会向相关进程发送信号(信号通常为`SIGALRM`)。

在内核空间，这有点微妙。从我们关于硬件中断的上半部分和下半部分的讨论(参见*第 4 章，处理硬件中断*、*理解和使用上半部分和下半部分*部分)中，您会知道，在定时器中断的上半部分(或 ISR)完成后，内核将确保运行定时器中断下半部分或定时器软件 irq(如我们在[第 4 章](4.html)、*处理硬件中断*部分*可用软件 IRQ 以及它们用于*的表格中所示)。这是一个非常高优先级的软件，叫做`TIMER_SOFTIRQ`。这个软件消耗过期的定时器！实际上——理解这一点非常重要——定时器的“回调”功能——定时器到期时将运行的功能——由定时器软件*运行，因此在原子(中断)上下文*中运行。因此，它能做什么和不能做什么是有限的(同样，这在*第 4 章*、*处理硬件中断*中有详细解释)。

在下一节中，您将学习如何设置和使用内核定时器。

## 使用内核定时器

为了使用内核定时器，您必须遵循几个步骤。简而言之，以下是要做的事情(我们将在后面更详细地讨论):

1.  用`timer_setup()`宏初始化定时器元数据结构(`struct timer_list`)。这里初始化的关键项目如下:
    *   定时器到期时间(定时器到期时`jiffies`应达到的值)
    *   定时器到期时要调用的函数——实际上是定时器“回调”函数
2.  编写定时器回调例程的代码。
3.  在适当的时候，通过调用`add_timer()`(或`mod_timer()`)功能来“启动”计时器，即启动计时器。
4.  当定时器超时(过期)时，操作系统会自动调用你定时器的回调函数(你在*步骤 2* 设置的那个)；请记住，它将在 timer softirq 或原子或中断上下文中运行。
5.  (可选)*定时器不是循环的，默认情况下是一次性的*。要让计时器再次运行，您必须调用`mod_timer()`应用编程接口；这就是如何设置时间间隔计时器——在给定的固定时间间隔内超时的计时器。如果您不执行这一步，您的计时器将是一次性计时器-它将倒计时并正好过期一次。

6.  完成后，用`del_timer[_sync]()`删除定时器；这也可以用来取消超时。它返回一个值，表示挂起的计时器是否已被停用；也就是说，对于活动计时器，它返回`1`，对于取消的非活动计时器，它返回`0`。

`timer_list`数据结构与我们这里的工作相关；其中显示了相关成员(模块/驱动程序作者):

```sh
// include/linux/timer.h
struct timer_list {[ ... ]
    unsigned long expires;
    void (*function)(struct timer_list *);
    u32 flags; 
[ ...] };
```

使用`timer_setup()`宏进行初始化:

```sh
timer_setup(timer, callback, flags);
```

`timer_setup()`参数如下:

*   `@timer`:指向`timer_list`数据结构的指针(这个应该先分配内存；此外，在形式参数名称前加上一个`@`是一个常见的惯例)。
*   `@callback`:回调函数的指针。这是定时器到期时操作系统调用的功能(在 softirq 上下文中)。其签名为`void (*function)(struct timer_list *);`。您在回调函数中收到的参数是指向`timer_list`数据结构的指针。那么，我们如何在计时器回调中传递和访问一些任意数据呢？我们将很快回答这个问题。
*   `@flags`:这些是定时器标志。我们通常称之为`0`(暗示没有特殊行为)。您可以指定的旗帜有`TIMER_DEFERRABLE`、`TIMER_PINNED`和`TIMER_IRQSAFE`。让我们看看内核源代码中的两者:

```sh
// include/linux/timer.h
/**
 * @TIMER_DEFERRABLE: A deferrable timer will work normally when the
 * system is busy, but will not cause a CPU to come out of idle just
 * to service it; instead, the timer will be serviced when the CPU
 * eventually wakes up with a subsequent non-deferrable timer.
  [ ... ]
 * @TIMER_PINNED: A pinned timer will not be affected by any timer
 * placement heuristics (like, NOHZ) and will always expire on the CPU
 * on which the timer was enqueued.
```

当必须监视功耗时(例如在电池供电的设备上)，使用`TIMER_DEFERRABLE`标志非常有用。第三个标志`TIMER_IRQSAFE`，仅为专用；避免使用它。

接下来，使用`add_timer()`应用编程接口启动定时器。一旦被调用，计时器将“实时”并开始倒计时:

```sh
void add_timer(struct timer_list *timer);
```

它的参数是指向您刚刚初始化的`timer_list`结构的指针(通过`timer_setup()`宏)。

### 我们简单的内核定时器模块——代码视图 1

不用多说，让我们深入研究一个简单内核定时器的代码，它是使用**可加载内核模块** ( **LKM** )框架编写的(这可以在`ch5/timer_simple`找到)。与大多数驱动程序一样，我们保留一个包含运行时所需信息的上下文或私有数据结构；在这里，我们称之为`st_ctx`。我们将其实例化为`ctx`变量。我们还在名为`exp_ms`的全局中指定了过期时间(420 毫秒):

```sh
// ch5/timer_simple/timer_simple.c
#include <linux/timer.h>
[ ... ]
static struct st_ctx {
    struct timer_list tmr;
    int data;
} ctx;
static unsigned long exp_ms = 420;
```

现在，让我们来看看 *init* 代码的第一部分:

```sh
static int __init timer_simple_init(void)
{
    ctx.data = INITIAL_VALUE;

    /* Initialize our kernel timer */
    ctx.tmr.expires = jiffies + msecs_to_jiffies(exp_ms);
    ctx.tmr.flags = 0;
    timer_setup(&ctx.tmr, ding, 0);
```

这很简单。首先，我们初始化`ctx`数据结构，将`data`成员设置为值`3`。这里的一个关键点是`timer_list`结构在我们的`ctx`结构中，所以我们必须初始化它。现在，设置定时器回调函数(`function`参数)和`flags`参数值很简单；设置过期时间怎么样？您必须将`timer_list.expires`成员设置为内核中`jiffies`变量(实际上是宏)必须达到的值；到那个时候，计时器就会过期！因此，我们通过将 jiffies 的当前值与 420 毫秒的过去时间所花费的 jiffies 值相加，使计时器在未来 420 毫秒到期，如下所示:

```sh
ctx.tmr.expires = jiffies + msecs_to_jiffies(exp_ms);
```

`msecs_to_jiffies()`便利程序在这里帮助我们转换传递给`jiffies`的毫秒值。将这个结果添加到`jiffies`的当前值将会给我们`jiffies`在未来的值，从现在开始的 420 毫秒，也就是我们想要我们的内核定时器到期的时候。

This code is an inline function in `include/linux/jiffies.h:msecs_to_jiffies()`; the comments help us understand how it works. In a similar fashion, the kernel contains the `usecs_to_jiffies()`, `nsecs_to_jiffies()`, `timeval_to_jiffies()`, and `jiffies_to_timeval()` (inline) function helper routines.

*初始化*代码的下一部分如下:

```sh
    pr_info("timer set to expire in %ld ms\n", exp_ms);
    add_timer(&ctx.tmr); /* Arm it; let's get going! */
    return 0;     /* success */
}
```

正如我们所看到的，通过调用`add_timer()` API，我们已经武装(启动)了我们的内核定时器。现在是直播倒计时...大约 420 毫秒后，它将过期。(为什么大约？正如你在*中看到的，让我们试试吧——延迟和睡眠真的需要多长时间？*区段、延迟和睡眠 API 并不都那么精确。事实上，建议您稍后进行的练习是测试超时的准确性；你可以在*问题/内核 _ 定时器 _ 检查*部分找到这个。此外，在本练习的示例解决方案中，我们将展示如何使用`time_after()`宏是一个好主意；它执行有效性检查，以确保第二个时间戳实际上晚于第一个时间戳。类似的宏可以在`include/linux/jiffies.h`找到；请参见该行前面的注释:`include/linux/jiffies.h:#define time_after(a,b)`)。

### 我们简单的内核定时器模块——代码视图 2

`add_timer()`启动我们的内核定时器。正如你刚才看到的，它很快就会过期。在内部，正如我们前面提到的，内核的定时器 softirq 将运行我们的定时器回调函数。在前一节中，我们将回调函数初始化为`ding()`函数(ha，*拟声词*——一个暗示它所描述的声音的词——在起作用！)通过`timer_setup()`原料药。因此，该代码将在计时器到期时运行:

```sh
static void ding(struct timer_list *timer)
{
    struct st_ctx *priv = from_timer(priv, timer, tmr);
    /* from_timer() is in fact a wrapper around the well known
     * container_of() macro! This allows us to retrieve access to our
     * 'parent' driver context structure */
    pr_debug("timed out... data=%d\n", priv->data--);
    PRINT_CTX();

    /* until countdown done, fire it again! */
    if (priv->data)
        mod_timer(&priv->tmr, jiffies + msecs_to_jiffies(exp_ms));
}
```

关于此功能，需要记住几件事:

*   定时器回调处理程序代码(这里是`ding()`)在原子(中断，softirq)上下文中运行；因此，除了使用`GFP_ATOMIC`标志之外，您不允许调用任何执行任何阻塞的应用编程接口、内存分配，或者内核和用户空间之间的任何类型的数据传输(我们在*中断上下文指南的上一章中详细介绍了这一点——做什么和不做什么*部分)。
*   回调函数接收`timer_list`结构的指针作为参数。由于我们(非常有意识地)将`struct timer_list`保留在我们的上下文或私有数据结构中，我们可以有效地使用`from_timer()`宏来检索指向私有结构的指针；也就是`struct st_ctx`)。前面显示的第一行代码就是这样做的。这是如何工作的？让我们看看它的实现:

```sh
 // include/linux/timer.h
 #define from_timer(var, callback_timer, timer_fieldname) \
           container_of(callback_timer, typeof(*var), timer_fieldname)

```

真的是`container_of()`宏的包装！

*   然后我们打印并减少我们的`data`值。
*   然后我们发布我们的`PRINT_CTX()`宏(回想一下它是在我们的`convenient.h`头文件中定义的)。这将表明我们是在软件环境下运行的。
*   接下来，只要我们的数据成员是正的，我们就通过调用`mod_timer()` API 来强制另一个超时(相同周期的超时):

```sh
int mod_timer(struct timer_list *timer, unsigned long expires);
```

可以看到，有了`mod_timer()`，计时器什么时候再次触发完全由你决定；这被认为是更新计时器到期日期的有效方法。通过使用`mod_timer()`，你甚至可以启动一个不活动的计时器(这是`add_timer()`做的工作)；在这种情况下，返回值是`0`，否则就是`1`(意味着我们修改了一个现有的活动计时器)。

### 我们简单的内核定时器模块——运行它

现在，让我们测试我们的内核定时器模块。在我们的 x86_64 Ubuntu 虚拟机上，我们将使用我们的`lkm`便利脚本来加载内核模块。下面的屏幕截图显示了这个和内核日志的部分视图:

![](assets/8d3fa66c-52cc-44fa-98b5-e7f92ccd785d.png)

Figure 5.2 – A partial screenshot of running our timer_simple.ko kernel module

研究这里显示的`dmesg`(内核日志)输出。由于我们已经将私有结构的`data`成员的初始值设置为`3`，内核定时器将到期三次(正如我们的逻辑要求)。查看最左边一列中的时间戳；您可以看到第二个定时器到期发生在`4234.289334`(美国秒)，第三个发生在`4234.737346`；快速减法显示时间差为 448，012 微秒；也就是大约 448 毫秒。这是合理的，因为我们要求一个 420 毫秒的超时(有点超过这个时间；打印机的开销也很重要)。

`PRINT_CTX()`宏观的输出也很有启发性；让我们看看前面截图中显示的第二个:

```sh
[ 4234.290177] timer_simple:ding(): 001) [swapper/1]:0   |  ..s1   /* ding() */
```

这表明(如*第 4 章*、*处理硬件中断*中详细解释的那样)，代码在软件上下文(`..s1`中的`s`)中的 CPU 1(即`001)`)上运行。此外，被定时器中断和 softirq 中断的进程上下文是`swapper/1`内核线程；这是 CPU 1 空闲时运行的 CPU 空闲线程。这是有意义的，在空闲或轻载系统上非常典型。当定时器中断启动时，系统(或至少 CPU 1)处于空闲状态，随后出现了一个 softirq 并运行我们的定时器回调。

## sed1–使用我们的演示 sed 1 驱动程序实现超时

在这一节中，我们将编写一个更有趣的驱动程序(这方面的代码可以在`ch5/sed1/sed1_driver`找到)。我们将对它进行设计，使它能够加密和/或解密给定的消息(当然，非常简单)。基本思路是以用户模式 app(这个可以在`ch5/userapp_sed`找到)作为其用户界面。运行时，它会打开我们的`misc`角色驱动程序的设备文件(`/dev/sed1_drv`)并发出`ioctl(2)`系统调用。

We have provided material online to help you understand how to interface a kernel module or device driver to a user space process via several common methods: via procfs, sysfs, debugfs, netlink sockets, and the `ioctl()` system call ([https://github.com/PacktPublishing/Learn-Linux-Kernel-Development/blob/master/User_kernel_communication_pathways.pdf](https://github.com/PacktPublishing/Learn-Linux-Kernel-Development/blob/master/User_kernel_communication_pathways.pdf))!

`ioctl()`调用传递一个数据结构，该数据结构封装了正在传递的数据、其长度、要对其执行的操作(或转换)以及一个`timed_out`字段(以确定它是否因错过截止日期而失败)。有效的操作如下:

*   加密:`XF_ENCRYPT`
*   解密:`XF_DECRYPT`

由于篇幅不够，我们不打算在这里详细展示代码——毕竟，读了这么多这本书，你现在已经处于一个可以自己浏览、尝试和理解代码的好位置了！然而，与本节相关的某些关键细节将会显示出来。

让我们来看看它的整体设计:

*   我们的`sed1`驱动(`ch5/sed1/sed1_driver/sed1_drv.c`)真的是伪驱动，就是说它不在任何外设硬件控制器或芯片上运行，而是在内存上运行；尽管如此，它还是一个成熟的`misc`级角色设备驱动程序。
*   它将自己注册为`misc`设备；在这个过程中，一个设备节点是由内核自动创建的(这里，我们将称之为`/dev/sed1_drv`)。
*   我们安排它有一个驱动“上下文”结构(`struct stMyCtx`)，包含它始终使用的关键成员；其中之一是内核定时器的`struct timer_list`结构，我们在初始化代码路径中初始化它(使用`timer_setup()`应用编程接口)。
*   一个用户空间应用(`ch5/sed1/userapp_sed/userapp_sed1.c`)打开我们的`sed1`驱动程序的设备文件(它作为一个参数传递给它，还有要加密的消息)。它调用`ioctl(2)`系统调用——要加密的命令——和`arg`参数，该参数是一个指针，指向包含所有所需信息(包括要加密的消息有效载荷)的适当填充的结构。让我们简单地看一下:

```sh
​ kd->data_xform = XF_ENCRYPT;
 ioctl(fd, IOCTL_LLKD_SED_IOC_ENCRYPT_MSG, kd);
```

*   我们`sed1`司机的`ioctl`法接手。在执行有效性检查之后，它复制元数据结构(通过通常的`copy_from_user()`)并启动我们的`process_it()`函数，然后调用我们的`encrypt_decrypt_payload()`例程。
*   `encrypt_decrypt_payload()`是这里的关键套路。它执行以下操作:
    *   启动我们的内核定时器(用`mod_timer()` API)，设置它从现在起在`TIMER_EXPIRE_MS`毫秒内到期(这里，我们已经将`TIMER_EXPIRE_MS`设置为`1`)。
    *   抓取时间戳`t1 = ktime_get_real_ns();`。
    *   开始实际工作——它要么是一个加密操作，要么是一个解密操作(我们把它保持得非常简单:一个简单的`XOR`操作，后面是有效载荷的每个字节的增量；解密则相反)。
    *   一旦工作完成，做两件事:获取第二个时间戳，`t2 = ktime_get_real_ns();`，并取消内核定时器(使用`del_timer()` API)。
    *   显示完成所需的时间(通过我们的`SHOW_DELTA()`宏)。
*   然后，用户空间应用休眠 1 秒钟(自行收集)并运行`ioctl`解密，导致我们的驱动程序解密消息。
*   最后，它终止了。

以下是来自`sed1`司机的相关代码:

```sh
// ch5/sed1/sed1_driver/sed1_drv.c
[ ... ]
static void encrypt_decrypt_payload(int work, struct sed_ds *kd, struct sed_ds *kdret)
{
        int i;
        ktime_t t1, t2;   // a s64 qty
        struct stMyCtx *priv = gpriv;
        [ ... ]
        /* Start - the timer; set it to expire in TIMER_EXPIRE_MS ms */
        mod_timer(&priv->timr, jiffies + msecs_to_jiffies(TIMER_EXPIRE_MS));
        t1 = ktime_get_real_ns();

        // perform the actual processing on the payload
        memcpy(kdret, kd, sizeof(struct sed_ds));
        if (work == WORK_IS_ENCRYPT) {
                for (i = 0; i < kd->len; i++) {
                        kdret->data[i] ^= CRYPT_OFFSET;
                        kdret->data[i] += CRYPT_OFFSET;
                }
        } else if (work == WORK_IS_DECRYPT) {
                for (i = 0; i < kd->len; i++) {
                        kdret->data[i] -= CRYPT_OFFSET;
                        kdret->data[i] ^= CRYPT_OFFSET;
                }
        }
        kdret->len = kd->len;
        // work done!
        [ ... // code to miss the deadline here! (explained below) ... ]
        t2 = ktime_get_real_ns();

        // work done, cancel the timeout
        if (del_timer(&priv->timr) == 0)
                pr_debug("cancelled the timer while it's inactive! (deadline missed?)\n");
        else
                pr_debug("processing complete, timeout cancelled\n");
        SHOW_DELTA(t2, t1);
}
```

差不多就是这样！为了了解它是如何工作的，让我们来看看它是如何工作的。首先，我们必须插入我们的内核驱动程序(LKM):

```sh
$ sudo insmod ./sed1_drv.ko
$ dmesg 
[29519.684832] misc sed1_drv: LLKD sed1_drv misc driver (major # 10) registered, minor# = 55,
 dev node is /dev/sed1_drv
[29519.689403] sed1_drv:sed1_drv_init(): init done (make_it_fail is off)
[29519.690358] misc sed1_drv: loaded.
$ 
```

下面的截图展示了它加密解密的一个示例运行(这里，我们特意运行了这个 app 的**地址杀毒软件** ( **ASan** )调试版本；它可能只是揭示 bug，所以为什么不呢！):

![](assets/b69c5a7c-64ac-4b18-83fb-5b944288b6eb.png)

Figure 5.3 – Our sed1 mini-project encrypting and decrypting a message within the prescribed deadline

这里一切都很顺利。

让我们看看内核定时器回调函数的代码。在这里，在我们简单的`sed1`驱动程序中，我们只需让它执行以下操作:

*   自动将私有结构`timed_out`中的一个整数设置为`1`值，表示失败。当我们将数据结构复制回我们的用户模式应用(通过`ioctl()`)时，这允许它轻松检测故障并报告/记录故障(关于使用原子操作符的细节以及更多细节将在本书的最后两章中介绍)。
*   向内核日志发出`printk`(在`KERN_NOTICE`级别)，表示我们超时了。
*   调用我们的`PRINT_CTX()`宏来显示上下文细节。

内核定时器回调函数的代码如下:

```sh
static void timesup(struct timer_list *timer)
{
    struct stMyCtx *priv = from_timer(priv, timer, timr);

    atomic_set(&priv->timed_out, 1);
    pr_notice("*** Timer expired! ***\n");
    PRINT_CTX();
}
```

我们能看到这个代码——定时器到期功能——运行吗？我们安排下一步做这件事。

### 故意错过公共汽车

我之前遗漏的部分是一个有趣的问题:就在第二个时间戳被获取之前，我们插入了一点代码来故意错过神圣不可侵犯的截止日期！怎么做？这真的很简单:

```sh
static void encrypt_decrypt_payload(int work, struct sed_ds *kd, struct sed_ds *kdret)
{
    [ ... ]
    // work done!
    if (make_it_fail == 1)
 msleep(TIMER_EXPIRE_MS + 1);
    t2 = ktime_get_real_ns();
```

`make_it_fail`是默认设置为`0`的模块参数；由此可见，只有你想危险地生活(是的，有点夸张！)你应该把它叫做`1`。让我们试试看，看看我们的内核定时器是否到期。用户模式应用将检测到这一点并报告故障:

![](assets/29cfeb09-64fd-40e7-92e9-8752bcd8fde6.png)

Figure 5.4 – Our sed1 mini-project running with the make_it_fail module parameter set to 1, causing the deadline to be missed

这一次，在计时器被取消之前，超过了最后期限，从而导致它过期并触发。然后它的`timesup()`回调函数运行(在前面的截图中突出显示)。我强烈建议您花时间详细阅读驱动程序和用户模式应用的代码，并自行试用。

The `schedule_timeout()` function that we briefly used earlier is a great example of using kernel timers! Its internal implementation can be seen here: `kernel/time/timer.c:schedule_timeout()`.

Additional information on timers can be found within the `proc` filesystem; among the relevant (pseudo) files is `/proc/[pid]/timers` (per-process POSIX timers) and the `/proc/timer_list` pseudofile (this contains information about all pending high-resolution timers, as well as all clock event sources. Note that the `/proc/timer_stats` pseudo-file disappeared after kernel version 4.10). You can find out more information about them on the man page about `proc(5)` at [https://man7.org/linux/man-pages/man5/proc.5.html](https://man7.org/linux/man-pages/man5/proc.5.html).

在下一节中，您将学习如何创建和使用内核线程。继续读！

# 创建和使用内核线程

线程是执行路径；它纯粹与执行给定的函数有关。这个功能就是它的生命和范围；一旦它从那个函数返回，它就死了。在用户空间中，线程是进程内的执行路径；进程可以是单线程或多线程的。内核线程在许多方面与用户模式线程非常相似。在内核空间中，线程也是一个执行路径，只是它在内核 VAS 中运行，拥有内核特权。这意味着内核也是多线程的。快速浏览一下`ps(1)`(使用**柏克莱软件发行版** ( **BSD** )样式`aux`选项开关运行)的输出，我们会看到内核线程——它们的名称用方括号括起来:

```sh
$ ps aux
USER         PID %CPU %MEM    VSZ   RSS TTY        STAT START   TIME COMMAND
root           1  0.0  0.5 167464 11548 ?          Ss   06:20   0:00 /sbin/init splash 3
root           2  0.0  0.0      0     0 ?          S    06:20   0:00 [kthreadd]
root           3  0.0  0.0      0     0 ?          I<   06:20   0:00 [rcu_gp]
root           4  0.0  0.0      0     0 ?          I<   06:20   0:00 [rcu_par_gp]
root           6  0.0  0.0      0     0 ?          I<   06:20   0:00 [kworker/0:0H-kblockd]
root           9  0.0  0.0      0     0 ?          I<   06:20   0:00 [mm_percpu_wq]
root          10  0.0  0.0      0     0 ?          S    06:20   0:00 [ksoftirqd/0]
root          11  0.0  0.0      0     0 ?          I    06:20   0:05 [rcu_sched]
root          12  0.0  0.0      0     0 ?          S    06:20   0:00 [migration/0]
[ ... ]
root          18  0.0  0.0      0     0 ?          S    06:20   0:00 [ksoftirqd/1]
[ ... ]
```

大多数内核线程都是为了明确的目的而创建的；通常，它们是在系统启动时创建的，并且永远运行(在无限循环中)。他们让自己进入睡眠状态，当一些工作需要完成时，醒来，执行它，然后马上回到睡眠状态。一个很好的例子是`ksoftirqd/n`内核线程(每个 CPU 内核通常有一个；这就是`n`的含义——它是核心数字)；当软 irq 负载变得太重时，它们会被内核唤醒，以帮助消耗未决的软 IRQ，从而提供帮助(我们在 [第 4 章](4.html)、*处理硬件中断*中，在*使用 ksoftirqd 内核线程*一节中讨论了这一点；在前面的`ps`输出中，您可以在双核虚拟机上看到它们；他们有 PID 10 和 18)。类似地，内核也使用*“kworker”工作线程*，它们是动态的——它们根据工作需要来来去去(一个快速的`ps aux | grep kworker`应该会显示其中的几个)。

让我们来看看内核线程的一些特性:

*   它们总是在内核 VAS 中执行，在内核模式下具有内核特权。
*   它们总是在进程上下文中运行(参考配套指南 *Linux 内核编程-* *第 6 章*、*内核内部本质–进程和线程*、*理解进程和中断上下文*部分)，它们有一个任务结构(因此有一个 PID 和所有其他典型的线程属性，尽管它们的*凭证*总是设置为`0`，意味着根访问)。
*   它们通过 CPU 调度器与其他线程(包括用户模式线程)争夺 CPU 资源；内核线程(通常缩写为 **kthreads** )的优先级确实会略有提升。
*   因为它们纯粹在内核增值服务中运行，所以它们对用户增值服务视而不见；因此，它们的`current->mm`值总是`NULL`(确实，这是识别 kthread 的快速方法)。

*   所有内核线程都是从名为`kthreadd`的内核线程派生而来的，其 PID 为`2`。这是内核(技术上是第一个 PID 为`0`的`swapper/0` kthread)在早期启动时创建的；你可以通过做`pstree -t -p 2`来验证这一点(查看`pstree(1)`的手册页了解使用细节)。
*   他们有命名惯例。kthreads 的命名不同，尽管遵循了一些约定。通常，名称以`/n`结尾；这意味着它是一个每 CPU 内核线程。该数字指定了与其关联运行的 CPU 内核(我们在配套指南 *Linux 内核编程-* *第 11 章*、*CPU 调度程序–第 2 部分*、在*理解、查询和设置 CPU 关联掩码*一节中介绍了 CPU 关联)。此外，内核线程用于特定的目的，它们的名字反映了这一点；例如，`irq/%d-%s`(其中`%d`是 PID，`%s`是名称)是线程中断处理程序(包含在*第 4 章*、*处理硬件中断*中)。你可以通过阅读内核文档*减少每 cpu kthreads 引起的 OS 抖动*，在[https://www . kernel . org/doc/Documentation/kernel-per-CPU-kthreads . txt](https://www.kernel.org/doc/Documentation/kernel-per-CPU-kthreads.txt)上，了解如何找到 kthread 名称以及 kthread 的许多实际用途(以及如何调整它们以减少抖动)。

我们感兴趣的是，内核模块和设备驱动程序通常需要在后台运行特定的代码路径，与它和内核例行执行的其他工作并行。假设您需要阻止正在发生的异步事件，或者需要在某个事件发生时，从内核中执行一个用户模式进程，这非常耗时。内核线程就是这里的入场券；因此，我们将关注作为模块作者的您如何创建和管理内核线程。

Yes, you can execute a user mode process or app from within the kernel! The kernel provides some**user mode helper** (**umh**) APIs to do so, with a common one being `call_usermode_helper()`. You can view its implementation here: `kernel/umh.c:int call_usermodehelper(const char *path, char **argv, char **envp, int wait)`. Be careful, though; you are not meant to abuse this API to invoke just any app from the kernel – that's simply bad design! There are very few actual use cases of using this API in the kernel; use `cscope(1)` to check it out.

太好了；有了这些，让我们学习如何创建和使用内核线程。

## 一个简单的演示——创建一个内核线程

创建内核线程(暴露给用户模块/驱动作者)的主要 API 是`kthread_create()`；这是一个调用`kthread_create_on_node()` API 的宏。事实是，仅仅调用`kthread_create()`并不足以让你的内核线程做任何有用的事情；这是因为，虽然这个宏确实创建了内核线程，但是您需要通过将它设置为运行并唤醒它来使它成为调度程序的候选线程。这可以通过`wake_up_process()`应用编程接口来实现(一旦成功，它就会排队到一个中央处理器运行队列中，这使得它可以调度，以便在不久的将来运行)。好消息是`kthread_run()`助手宏可以用来一次性调用`kthread_create()`和`wake_up_process()`。让我们看看它在内核中的实现:

```sh
// include/linux/kthread.h
/**
 * kthread_run - create and wake a thread.
 * @threadfn: the function to run until signal_pending(current).
 * @data: data ptr for @threadfn.
 * @namefmt: printf-style name for the thread.
 *
 * Description: Convenient wrapper for kthread_create() followed by
 * wake_up_process(). Returns the kthread or ERR_PTR(-ENOMEM).
 */
#define kthread_run(threadfn, data, namefmt, ...) \
({ \
    struct task_struct *__k \
        = kthread_create(threadfn, data, namefmt, ## __VA_ARGS__); \
    if (!IS_ERR(__k)) \
        wake_up_process(__k); \
    __k; \
})
```

前面代码片段中的注释清楚了`kthread_run()`的参数和返回值。

为了演示如何创建和使用内核线程，我们将编写一个名为`kthread_simple`的内核模块。以下是其`init`方法的相关代码:

```sh
// ch5/kthread_simple/kthread_simple.c
static int kthread_simple_init(void)
{   [ ... ]
    gkthrd_ts = kthread_run(simple_kthread, NULL, "llkd/%s", KTHREAD_NAME);
    if (IS_ERR(gkthrd_ts)) {
        ret = PTR_ERR(gkthrd_ts); // it's usually -ENOMEM
        pr_err("kthread creation failed (%d)\n", ret);
        return ret;
    } 
    get_task_struct(gkthrd_ts); // inc refcnt, marking the task struct as in use
    [ ... ]
```

`kthread_run()`的第一个参数是新 kthread 的命脉——它的功能！在这里，我们不打算将任何数据传递给我们的新生儿 kthread，这就是为什么第二个参数是`NULL`。其余参数是指定其名称的 printf 样式的格式字符串。一旦成功，它将返回指向新 kthread 任务结构的指针(我们在配套指南 *Linux 内核编程-* *第 6 章*、*内核内部要素–进程和线程*中的*理解和访问内核任务结构*一节中详细介绍了任务结构)。现在，`get_task_struct()`内联函数很重要——它增加传递给它的任务结构的引用计数。这将任务标记为正在使用(稍后，在清理代码中，我们将发出`kthread_stop()`助手例程；它将执行相反的操作，从而减少(并最终释放)任务结构的引用计数)。

现在，让我们看看内核线程本身(我们将只显示相关的代码片段):

```sh
static int simple_kthread(void *arg)
{
    PRINT_CTX();
    if (!current->mm)
        pr_info("mm field NULL, we are a kernel thread!\n");
```

`kthread_run()`成功创建内核线程的那一刻，它将开始与系统的其他部分并行运行它的代码:它现在是一个可调度的线程！我们的`PRINT_CTX()`宏揭示了它在进程上下文中运行，确实是一个内核线程。(我们模仿了将其名称括在方括号中的传统，以展示这一点。验证当前`mm`指针为`NULL`的检查也证实了这一点。)可以在*图 5.5* 中看到输出。内核线程例程中的所有代码都将在*进程上下文*中运行；因此，您可以执行阻塞操作(与中断上下文不同)。

接下来，默认情况下，内核线程以根所有权运行，所有信号都被屏蔽。然而，作为一个简单的测试案例，我们可以通过`allow_signal()`助手例程打开几个信号。之后，我们简单地循环(我们将很快进入`kthread_should_stop()`例程)；在循环体中，我们通过将任务的状态设置为`TASK_INTERRUPTIBLE`(暗示睡眠可以被信号打断)并调用`schedule()`来使自己进入睡眠状态:

```sh
    allow_signal(SIGINT);
    allow_signal(SIGQUIT);

    while (!kthread_should_stop()) {
        pr_info("FYI, I, kernel thread PID %d, am going to sleep now...\n",
            current->pid);
        set_current_state(TASK_INTERRUPTIBLE);
        schedule(); // yield the processor, go to sleep...
        /* Aaaaaand we're back! Here, it's typically due to either the
         * SIGINT or SIGQUIT signal hitting us! */
        if (signal_pending(current))
            break;
    }
```

因此，只有当我们被唤醒时——当你向内核线程发送`SIGINT`或`SIGQUIT`信号时就会发生这种情况——我们才会恢复执行。当这种情况发生时，我们打破循环(注意我们如何首先验证这确实是`signal_pending()`助手例程的情况！).现在，我们的 kthread 在循环之外继续执行，只是(故意地，非常戏剧性地)死去:

```sh
    set_current_state(TASK_RUNNING);
    pr_info("FYI, I, kernel thread PID %d, have been rudely awoken; I shall"
            " now exit... Good day Sir!\n", current->pid);
    return 0;
}
```

内核模块的清理代码如下:

```sh
static void kthread_simple_exit(void)
{
    kthread_stop(gkthrd_ts);   /* waits for our kthread to terminate; 
                                * it also internally invokes 
                                * the put_task_struct() to decrement task's  
                                * reference count
                                */
    pr_info("kthread stopped, and LKM removed.\n");
}
```

在这里，在清理代码路径中，您需要调用`kthread_stop()`，执行必要的清理。在内部，它实际上等待 kthread 死亡(通过`wait_for_completion()`例程)。所以，如果你在没有通过发送`SIGINT`或`SIGQUIT`信号杀死 kthread 的情况下调用`rmmod`，那么`rmmod`过程将会出现在这里；是(这个`rmmod`过程，就是)等待(嗯，`kthread_stop()`真的是那个等待)kthread 死掉！这就是为什么，如果 kthread 还没有信号，这可能会导致问题。

应该有更好的方法来处理停止内核线程，而不是从用户空间向它发送信号。确实有:正确的方法是使用`kthread_should_stop()`例程作为它运行的`while`循环的(逆)条件，所以这正是我们要做的！在前面的代码中，我们有以下内容:

```sh
while (!kthread_should_stop()) {
```

`kthread_should_stop()`例程返回一个布尔值，如果 kthread 现在应该停止(终止)，则该值为真！在清理代码路径中调用`kthread_stop()`将导致`kthread_should_stop()`返回真，从而导致我们的 kthread 脱离`while`循环并通过简单的`return 0;`终止。该值(`0`)传递回`kthread_stop()`。因此，内核模块成功卸载，*即使没有信号发送到我们的内核线程*。我们将把测试这个案例作为一个简单的练习留给你！

请注意，`kthread_stop()`的返回值可能很有用:它是一个整数，是运行的线程函数的结果——实际上，它表示您的 kthread 是否成功(返回的`0`)工作。如果你的 kthread 从未被唤醒，它将是有价值的`-EINTR`。

## 运行 kthread_simple 内核线程演示

现在，让我们试一试(`ch5/kthread_simple`)！我们可以通过`insmod(8)`进行模块插入；模块按计划插入内核。下面截图中显示的内核日志，以及一个快速的`ps`，证明我们全新的内核线程确实已经创建。此外，从代码(`ch5/kthread_simple/kthread_simple.c`)中可以看到，我们的 kthread 使自己进入睡眠状态(将其状态设置为`TASK_INTERRUPTIBLE`，然后调用`schedule()`):

![](assets/9c126689-076e-44b5-9a88-b20b3004f4d4.png)

Figure 5.5 – A partial screenshot showing that our kernel thread is born, alive – and, well, asleep

通过名字快速运行我们的内核线程`ps(1) grep`表明我们的 kthread 是活的并且很好(并且睡着了):

```sh
$ ps -e |grep kt_simple
 11372   ?        00:00:00 llkd/kt_simple
$
```

让我们稍微改变一下，向我们的 kthread 发送`SIGQUIT`信号。这让它醒来(因为我们已经设置了它的信号屏蔽以允许`SIGINT`和`SIGQUIT`信号)，将其状态设置为`TASK_RUNNING`，然后，嗯，简单地退出。然后我们使用`rmmod(8)`移除内核模块，如下图所示:

![](assets/c3f089f4-57d3-4048-a739-8fe84c1e7292.png)

Figure 5.6 – A partial screenshot showing our kernel thread waking up and the module successfully unloaded

现在您已经理解了如何创建和使用内核线程，让我们继续设计和实现我们的`sed`驱动程序的第二个版本。

## sed2 驱动程序-设计和实施

在这一节中(正如在*中提到的“sed”驱动程序——演示内核定时器、kthreads 和工作队列*一节)，我们将编写`sed1` 驱动程序的下一个进化，称为`sed2`。

### sed 2–设计

我们的`sed`v2(`sed2`)T4；代码:`ch5/sed2/`)迷你项目和我们的`sed1`项目非常相似。关键区别在于，这一次，我们将通过驱动程序为此目的创建的内核线程来执行“工作”。此版本与上一版本的主要区别如下:

*   只有一个全局共享内存缓冲区来保存元数据和有效负载；也就是要加密/解密的消息。这是我们的驱动程序上下文结构`struct stMyCtx`中的`struct sed_ds->shmem`成员。
*   加密/解密的工作现在在内核线程(这个驱动程序产生的)中执行；我们保持内核线程休眠。只有当工作出现时，驱动程序才会唤醒 kthread 并让它消耗(执行)工作。
*   我们现在在 kthread 的上下文中运行内核计时器，并显示它是否过早过期(表示没有达到截止日期)。
*   一个快速测试显示，在内核线程的关键部分消除几个`pr_debug()`打印对于减少完成工作所花费的时间大有帮助！(如果您希望消除此开销，您可以随时更改 Makefile 的`EXTRA_CFLAGS`变量来取消`DEBUG`符号的定义(通过使用`EXTRA_CFLAGS += -UDEBUG`)！).因此，这里的截止时间更长(10 毫秒)。

因此，简而言之，这里的整个想法主要是演示如何使用一个定制的内核线程以及一个内核定时器来超时一个操作。理解改变整体设计(尤其是用户空间应用与我们的`sed2`驱动程序交互的方式)的一个关键点是，因为我们在内核线程的上下文中运行工作，所以它与`ioctl()`被发布到的进程的上下文不同。因此，认识到以下几点非常重要:

*   您不能简单地将数据从内核线程的进程上下文转移到用户空间进程——它们是完全不同的(它们运行在不同的虚拟地址空间:用户模式进程有自己完整的 VAS 和 PID 等等；内核线程实际上存在于内核 VAS 中，具有自己的 PID 和内核模式堆栈)。因此，使用`copy_{from|to}_user()`(及类似)例程从 kthread 向用户模式应用进行通信是不可能的。
*   危险的*比赛*的可能性很大；内核线程相对于用户进程上下文异步运行；因此，如果我们不小心，我们最终会产生与并发相关的错误。这就是本书最后两章的全部原因，在这两章中，我们将介绍内核同步、锁定(以及相关)概念和技术。现在，请耐心等待——我们通过使用一些简单的轮询技巧来代替适当的同步，从而使事情尽可能简单。

我们的`sed2`项目中有四个操作:

*   **加密**消息(这也将消息从用户空间获取到驱动程序中；因此，这必须首先完成)。
*   **解密** 的消息。
*   **检索**消息(从司机发送到用户空间 app)。
*   **销毁** 消息(实际上，它被重置了——驱动程序中的内存和元数据被清除了)。

重要的是要意识到，由于潜在的种族，我们*不能简单地*将数据直接从 kthread 传输到用户空间 app。因此，我们必须做到以下几点:

*   我们必须通过发出`ioctl()`系统调用，在用户空间进程的进程上下文中执行检索和销毁操作。
*   我们必须在我们的内核线程的进程上下文中执行加密和解密操作，相对于用户空间应用是异步的(我们在内核线程中运行它，不是因为我们*必须*而是因为我们想要；这毕竟是这个话题的重点！).

这个设计可以用一个简单的 ASCII 艺术图来概括:

![](assets/f2d0e4db-7478-467d-b76a-16fcd637e48b.png)

Figure 5.7 – The high-level design of our sed2 mini-project

好了，现在来看看`sed2`的相关代码实现。

### sed2 驱动程序-代码实现

在代码方面，`ioctl()`方法在`sed2` 驱动程序内进行加密操作的代码如下(为清晰起见，这里不显示所有的错误检查代码；我们将只显示最相关的部分)。您可以在`ch5/sed2/`找到完整的代码:

```sh
// ch5/sed2/sed2_driver/sed2_drv.c
[ ... ]
#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 36)
static long ioctl_miscdrv(struct file *filp, unsigned int cmd, unsigned long arg)
#else
static int ioctl_miscdrv(struct inode *ino, struct file *filp, unsigned int cmd, unsigned long arg)
#endif
{
    struct stMyCtx *priv = gpriv;

[ ... ]
switch (cmd) {
    case IOCTL_LLKD_SED_IOC_ENCRYPT_MSG: /* kthread: encrypts the msg passed in */
        [ ... ]
        if (atomic_read(&priv->msg_state) == XF_ENCRYPT) { // already encrypted?
            pr_notice("encrypt op: message is currently encrypted; aborting op...\n");
            return -EBADRQC; /* 'Invalid request code' */
        }
        if (copy_from_user(priv->kdata, (struct sed_ds *)arg, sizeof(struct sed_ds))) {
         [ ... ]

        POLL_ON_WORK_DONE(1);
        /* Wake up our kernel thread and have it encrypt the message ! */
        if (!wake_up_process(priv->kthrd_work))
            pr_warn("worker kthread already running when awoken?\n");
        [ ... ]
```

驱动程序在其`ioctl()`方法中执行了几次有效性检查后，开始工作:对于加密操作，我们检查当前有效负载是否已经加密(显然，我们的上下文结构中有一个状态成员被更新以保存该信息；也就是`priv->msg_state`)。如果一切正常，它会从用户空间应用中复制消息(以及`struct sed_ds`中所需的元数据)。然后，它*唤醒我们的内核线程* (通过`wake_up_process()`API；该参数是指向其任务结构的指针，它是来自`kthread_create()`应用编程接口的返回值。这导致内核线程恢复执行！

In the `init` code, we created the kthread with the `kthread_create()` API (and not the `kthread_run()` macro) as we do *not* want the kthread to run immediately! Instead, we prefer to keep it asleep, only awakening it when work is required of it. This is the typical approach we should follow when employing a worker thread (the so-called manager-worker model).

我们的`init`方法中的以下代码创建内核线程:

```sh
static int __init sed2_drv_init(void)
{
    [ ... ]
    gpriv->kthrd_work = kthread_create(worker_kthread, NULL, "%s/%s", DRVNAME, KTHREAD_NAME);
    if (IS_ERR(gpriv->kthrd_work)) {
        ret = PTR_ERR(gpriv->kthrd_work); // it's usually -ENOMEM
        dev_err(dev, "kthread creation failed (%d)\n", ret);
        return ret;
    }
    get_task_struct(gpriv->kthrd_work); // inc refcnt, marking the task struct as in use
    pr_info("worker kthread created... (PID %d)\n", task_pid_nr(gpriv->kthrd_work));
    [ ... ]
```

此后，定时器被初始化(通过`timer_setup()`应用编程接口)。我们的工作线程的(截断的)代码如下所示:

```sh
static int worker_kthread(void *arg)
{
    struct stMyCtx *priv = gpriv;

    while (!kthread_should_stop()) {
        /* Start - the timer; set it to expire in TIMER_EXPIRE_MS ms */
        if (mod_timer(&priv->timr, jiffies + msecs_to_jiffies(TIMER_EXPIRE_MS)))
            pr_alert("timer already active?\n");
        priv->t1 = ktime_get_real_ns();

        /*--------------- Critical section begins --------------------------*/
        atomic_set(&priv->work_done, 0);
        switch (priv->kdata->data_xform) {
        [ ... ]
        case XF_ENCRYPT:
            pr_debug("data transform type: XF_ENCRYPT\n");
            encrypt_decrypt_payload(WORK_IS_ENCRYPT, priv->kdata);
 atomic_set(&priv->msg_state, XF_ENCRYPT);
            break;
        case XF_DECRYPT:
            pr_debug("data transform type: XF_DECRYPT\n");
            encrypt_decrypt_payload(WORK_IS_DECRYPT, priv->kdata);
            atomic_set(&priv->msg_state, XF_DECRYPT);
            break;
        [ ... ]
        priv->t2 = ktime_get_real_ns();
        // work done, cancel the timeout
        if (del_timer(&priv->timr) == 0)
        [ ... ]
```

在这里，您可以看到定时器正在启动(`mod_timer()`)，实际的加密/解密函数正在根据需要调用，时间戳正在被捕获，然后内核定时器被取消。这就是发生在`sed1` 中的事情，只不过，这一次(`sed2`)工作发生在我们内核线程的上下文中！然后，内核线程函数通过(如配套指南 *Linux 内核编程-* *第 10 章*、*CPU 调度程序-第 1 部分*、*第 11 章*、*CPU 调度程序-第 2 部分*中所述)将任务状态设置为睡眠状态(`TASK_INTERRUPTIBLE`)并调用`schedule()`来使自己进入睡眠状态，同时让出处理器。

等一下–在`ioctl()`方法中，你注意到在内核线程被唤醒之前对`POLL_ON_WORK_DONE(1);`宏的调用了吗？看看下面的代码:

```sh
        [ ... ]       
         POLL_ON_WORK_DONE(1);
        /* Wake up our kernel thread 
         * and have it encrypt the message ! 
         */
        if (!wake_up_process(priv->kthrd_work))
            pr_warn("worker kthread already running when awoken?\n");
        /*
         * Now, our kernel thread is doing the 'work'; 
         * it will either be done, or it will miss it's 
         * deadline and fail. Attempting to lookup the payload 
         * or do anything more here would be a
         * mistake, a race! Why? We're currently running in 
         * the ioctl() process context; the kernel thread runs 
         * in it's own process context! (If we must look it up, 
         * then we really require a (mutex) lock; we shall
         * discuss locking in detail in the book's last two chapters.
         */
        break;
```

轮询被用来避免可能的竞争:如果一个(用户模式)线程调用`ioctl()`来加密给定的消息，同时在另一个 CPU 内核上，另一个用户模式线程调用`ioctl()`来解密给定的消息，会怎么样？这将导致并发问题！同样，本书的最后两章致力于理解和处理这些；但是此时此地，我们能做什么呢？让我们实现一个穷人的同步解决方案:*轮询*。

这并不理想，但必须要做。我们将利用这样一个事实，即当工作完成时，驱动程序在驱动程序的上下文结构中设置一个名为`work_done`的原子变量为`1`；其价值是`0`不然。我们在这个宏中对此进行了调查:

```sh
/*
 * Is our kthread performing any ongoing work right now? poll...
 * Not ideal (but we'll live with it); ideally, use a lock (we cover locking in
 * this book's last two chapters)
 */
#define POLL_ON_WORK_DONE(sleep_ms) do { \
        while (atomic_read(&priv->work_done) == 0) \
            msleep_interruptible(sleep_ms); \
} while (0)
```

为了让这段代码更容易理解，我们没有占用处理器；如果工作还没有完成(还没有)，我们会休眠一毫秒(通过`msleep_interruptible()` API)然后再试一次。

到目前为止，我们已经介绍了`sed2`的加密和解密功能的相关代码(两者都在我们的 worker kthread 的上下文中运行)。现在，让我们看看剩下的两个功能——检索和销毁消息。这些都是在原始用户空间进程上下文中执行的——发出`ioctl()`系统调用的进程(或线程)。以下是他们的相关代码:

```sh
// ch5/sed2/sed2_driver/sed2_drv.c : ioctl() method
[ ... ]
case IOCTL_LLKD_SED_IOC_RETRIEVE_MSG: /* ioctl: retrieves the encrypted msg */
        if (atomic_read(&priv->timed_out) == 1) {
            pr_debug("the encrypt op had timed out! returning -ETIMEDOUT\n");
            return -ETIMEDOUT;
        }
        if (copy_to_user((struct sed_ds *)arg, (struct sed_ds *)priv->kdata, sizeof(struct sed_ds))) {
           //  [ ... error handling ... ]
        break;
    case IOCTL_LLKD_SED_IOC_DESTROY_MSG: /* ioctl: destroys the msg */
        pr_debug("In ioctl 'destroy' cmd option\n");
        memset(priv->kdata, 0, sizeof(struct sed_ds));
        atomic_set(&priv->msg_state, 0);
        atomic_set(&priv->work_done, 1);
        atomic_set(&priv->timed_out, 0);
```

```sh
        priv->t1 = priv->t2 = 0;
        break;
[ ... ]
```

既然已经看到了(相关)`sed2`代码，那就来试试吧！

### sed 2–尝试一下

让我们通过几个截图来看一下我们的`sed2`迷你项目的运行示例；确保您仔细查看它们:

![](assets/a00d3a39-6d42-400c-aa33-930747f6a037.png)

Figure 5.8 – Our sed2 mini-project showing off an interactive menu system. Here, a message has been successfully encrypted

那么，我们已经加密了一条消息，但是我们如何看待它呢？简单:我们用菜单！选择选项`2`检索(加密的)消息(它将被显示供您悠闲地阅读)，选择`3`解密它，选择`2`再次查看它，选择`5`查看内核日志–非常有用！以下屏幕截图显示了其中一些选项:

![](assets/35866e17-68c1-4168-abb3-8c5e3c2d856c.png)

Figure 5.9 – Our sed2 mini-project showing off an interactive menu system. Here, a message has been successfully encrypted

如内核日志所示，我们的用户模式 app ( `userapp_sed2_dbg_asan`)已经打开了设备并发出了检索操作，几秒钟后接着是加密操作(上一张截图左下角的时间戳帮助您解决了这个问题)。然后，驱动程序唤醒内核线程；可以看到它的 printk 输出，以及`PRINT_CTX()`的输出，这里:

```sh
[41178.885577] sed2_drv:worker_kthread(): 001) [sed2_drv/worker]:24117   |  ...0   /* worker_kthread() */
```

加密操作随后完成(成功且在期限内；计时器被取消):

```sh
[41178.888875] sed2_drv:worker_kthread(): processing complete, timeout cancelled
```

类似地，执行其他操作。我们将避免在这里显示用户空间应用的代码，因为它是一个简单的用户模式“C”程序。这一次(不同寻常的是)，是一款交互 app，菜单简单(截图所示)；一定要看看。详细阅读理解`sed2` 代码，自己试一试，就交给你了。

## 查询和设置内核线程的调度策略/优先级

最后，如何查询和/或更改内核线程的调度策略和(实时)优先级？内核为此提供了 API(内核内部经常使用`sched_setscheduler_nocheck()` API)。作为一个实际的例子，内核将需要内核线程来服务中断——我们在[第 4 章](4.html)、*处理硬件中断、*内部实现线程中断*一节中介绍的*线程中断*模型。*

它创建这些线程(通过`kthread_create()`)并通过`sched_setscheduler_nocheck()`应用编程接口改变它们的调度策略和实时优先级。我们不会在这里明确介绍它们的用法，因为我们已经在配套指南 *Linux 内核编程-* *第 11 章**中央处理器调度程序–第 2 部分*中介绍过了。有趣的是:`sched_setscheduler_nocheck()`应用编程接口只是底层`_sched_setscheduler()`例程的简单包装。为什么呢？`_sched_setscheduler()`应用编程接口根本没有导出，因此模块作者无法使用；`sched_setscheduler_nocheck()`包装器通过`EXPORT_SYMBOL_GPL()`宏导出(暗示只有 GPL 许可的代码才能实际使用！).

What about querying and/or changing the scheduling policy and (real-time) priority of **user space threads**? The Pthreads library provides wrapper APIs to do just this; the `pthread_[get|set]schedparam(3)` pair can be used here since they're wrappers around system calls such as `sched_[get|set]scheduler(2)` and `sched_[get|set]attr(2)`. They require root access and, for security purposes, have the `CAP_SYS_NICE` capability bit set in the binary executable file.

Though this book only covers kernel programming, I've mentioned this here as it's a really powerful thing: in effect, the user space app designer/developer has the ability to create and deploy application threads perfectly suited to their purpose: real-time threads at differing scheduling policies, real-time priorities between 1 and 99, non-RT threads (with the base nice value of `0`), and so on. Indiscriminately creating kernel threads is frowned upon, and the reason is clear – every additional kernel thread adds overhead, both in terms of memory and CPU cycles. When you're in the design phase, pause and think: do you really require one or more kernel threads? Or is there a better way of doing things? Workqueues are often exactly that – a better way!

现在，让我们看看工作队列！

# 使用内核工作队列

一个**工作队列**是内核工作线程创建和管理的一个抽象层。它们有助于解决一个至关重要的问题:直接使用内核线程，尤其是当涉及几个线程时，不仅很困难，而且很容易导致危险的 bug，如争用(从而导致死锁的可能性)，以及糟糕的线程管理，从而导致效率损失。工作队列是 Linux 内核中使用的*下半部分*机制(与小任务和软任务一起使用)。

Linux 内核中的现代工作队列实现——称为**并发管理工作队列**(**cmwq**)——实际上是一个相当复杂的框架，具有各种基于特定需求动态高效地供应内核线程的策略。

In this book, we prefer to focus on the *usage* of the kernel-global workqueue rather than its internal design and implementation. If you'd like to learn more about the internals, I recommend that you read the "official" kernel documentation here: [https://www.kernel.org/doc/Documentation/core-api/workqueue.rst](https://www.kernel.org/doc/Documentation/core-api/workqueue.rst). The *Further reading* section also contains some useful resources.

工作队列的主要特征如下:

*   工作队列任务(回调)总是在可抢占的进程上下文中执行。一旦您意识到它们是由运行在可抢占的进程上下文中的内核(工作)线程执行的，这是显而易见的。
*   默认情况下，所有中断都被使能，并且没有锁定。
*   上述几点意味着您可以在工作队列函数中进行冗长的、阻塞的、I/O 限制的工作(这与原子上下文(如 hardirq、小任务或 softirq)完全相反！).
*   正如您了解内核线程一样，在用户空间之间传输数据(通过典型的`copy_[to|from]_user()`和类似的例程)是*而不是*可能的；这是因为您的工作队列处理程序(函数)在其自己的进程上下文(内核线程的上下文)中执行。我们知道，内核线程没有用户映射。
*   内核工作队列框架维护工作池。这些实际上是几个内核工作线程，根据它们的需要以不同的方式组织。内核处理管理它们的所有复杂性，以及并发问题。下面的截图显示了几个工作队列内核工作线程(这是在我的 x86_64 Ubuntu 20.04 来宾虚拟机上拍摄的):

![](assets/3c46ac29-5b59-49ec-bb67-8209b7f52082.png)

Figure 5.10 – Several kernel threads serving the kernel workqueue's bottom-half mechanism

正如我们在*创建和使用内核线程*一节中提到的，找出 kthread 的名称并了解 kthread 的许多实际用途(以及如何调整它们以减少抖动)的一种方法是阅读相关的内核文档；也就是说，*减少由于每 CPU kthreads*([https://www . kernel . org/doc/Documentation/kernel-per-CPU-kthreads . txt](https://www.kernel.org/doc/Documentation/kernel-per-CPU-kthreads.txt))造成的操作系统抖动。

In terms of how to use workqueues (and the other bottom-half mechanisms), refer back to [*Chapter 4*](4.html), *Handling Hardware Interrupts*, the *Hardirqs, tasklets, and threaded handlers – what to use when* section,especially the table there.

重要的是要理解内核有一个随时可用的默认工作队列；它被称为 ***内核-全局工作队列*** 或 ***系统工作队列*** 。为了避免给系统带来压力，强烈建议您使用它。我们将使用内核全局工作队列，在上面查询我们的工作任务，并让它消耗我们的工作。

您甚至可以使用和创建其他类型的工作队列！内核提供了复杂的 *cmwq* 框架，以及一组 API 来帮助您创建特定类型的工作队列。我们将在下一节中更详细地了解这一点。

## 最基本的工作队列内部

我们在这里不深入讨论工作队列的内部；事实上，我们将仅仅触及表面(正如我们之前提到的，我们在这里的目的只是专注于使用内核全局工作队列)。

总是建议您使用默认的内核全局(系统)工作队列来使用您的异步后台工作。如果这被认为是不够的，不要担心——某些接口会暴露出来，让你创建你的工作队列。(切记这样做会增加系统的压力！)要分配新的工作队列实例，可以使用`alloc_workqueue()`API；这是用于创建(分配)工作队列的主要应用编程接口(通过现代的 *cmwq* 框架):

```sh
include/linux/workqueue.h
struct workqueue_struct *alloc_workqueue(const char *fmt, unsigned int flags, int max_active, ...);
```

请注意，它是通过`EXPORT_SYMBOL_GPL()`导出的，这意味着它只对使用 GPL 许可证的模块和驱动程序可用。`fmt`(以及`max_active`后面的参数)指定如何命名池中的工作队列线程。`flags`参数指定特殊行为值或其他特征的位掩码，例如:

*   当工作队列在内存压力下需要前进保证时，使用`WQ_MEM_RECLAIM`标志。
*   当工作项由 kthreads 的工作池以较高的优先级提供服务时，使用`WQ_HIGHPRI`标志。
*   使用`WQ_SYSFS`标志，通过 sysfs 使用户空间可以看到一些工作队列细节(实际上，在`/sys/devices/virtual/workqueue/`下查看)。
*   同样，还有其他几个标志。更多详情请看官方内核文档([https://www . kernel . org/doc/Documentation/core-API/workqueue . rst](https://www.kernel.org/doc/Documentation/core-api/workqueue.rst)；它提供了一些有趣的内容来减少由于内核中的工作队列执行而导致的“抖动”。

`max_active`参数用于指定每个 CPU 可以分配给一个工作项的最大内核线程数。

一般来说，有两种类型的工作队列:

*   **单线程** ( **ST** ) **工作队列或有序工作队列**:这里，在整个系统的任何给定时间点，只能有一个线程处于活动状态。它们可以通过`alloc_ordered_workqueue()`来创建(这实际上只是`alloc_workqueue()`的包装，指定了`max_active`精确设置为`1`的有序标志)。
*   **多线程** ( **MT** ) **工作队列**:这是默认选项。确切的`flags`指定行为；`max_active`指定工作项在每个 CPU 上可以拥有的最大工作内核线程数。

所有工作队列都可以通过`alloc_workqueue()`应用编程接口创建。创建它们的代码如下:

```sh
// kernel/workqueue.c
​int __init workqueue_init_early(void)
{
    [ ... ]
    system_wq = alloc_workqueue("events", 0, 0);
    system_highpri_wq = alloc_workqueue("events_highpri", WQ_HIGHPRI, 0);
    system_long_wq = alloc_workqueue("events_long", 0, 0);
    system_unbound_wq = alloc_workqueue("events_unbound", WQ_UNBOUND, WQ_UNBOUND_MAX_ACTIVE);
    system_freezable_wq = alloc_workqueue("events_freezable", WQ_FREEZABLE, 0);
    system_power_efficient_wq = alloc_workqueue("events_power_efficient", WQ_POWER_EFFICIENT, 0);
    system_freezable_power_efficient_wq = alloc_workqueue("events_freezable_power_efficient",
                          WQ_FREEZABLE | WQ_POWER_EFFICIENT, 0);
[ ... ]
```

这发生在引导过程的早期(实际上是在早期的 init 内核代码路径中)。第一个用粗体突出显示；这是正在创建的内核全局工作队列或系统工作队列。它的工人池被命名为`events`。(属于这个池的内核线程的名称遵循这个命名约定，并且名称中有单词`events`；再次参见*图 5.10* 。属于其他工作池的 kthreads 也会发生同样的情况。)

底层框架已经发展了很多；早期的*遗留*工作队列框架(2010 年之前)用于使用`create_workqueue()`和 friends APIs 但是，这些现在被认为是不推荐使用的。有趣的是，现代的**并发管理工作队列** ( **cmwq** )框架(大约从 2010 年开始)与旧框架向后兼容。下表总结了旧工作队列 API 到现代 cmwq API 的映射:

| **遗留(旧的和不推荐使用的)工作队列应用编程接口** | **现代(cmwq)工作队列 API** |
| `create_workqueue(name)` | `alloc_workqueue(name,WQ_MEM_RECLAIM, 1)` |
| `create_singlethread_workqueue(name)` | `alloc_ordered_workqueue(name, WQ_MEM_RECLAIM)` |
| `create_freezable_workqueue(name)` | `alloc_workqueue(name, WQ_FREEZABLE &#124; WQ_UNBOUND &#124; WQ_MEM_RECLAIM, 1)` |

Table 5.3 – Mapping of the older workqueue APIs to the modern cmwq ones

下图(以简单的概念性方式)总结了内核工作队列子系统:

![](assets/6a2e40f7-3546-45d4-843f-520afe0c298b.png)

Figure 5.11 – A simple conceptual view of the workqueue subsystem within the kernel

内核的工作队列框架动态维护这些(内核线程的)工作池；有些，如`events`工作队列(对应内核-全局工作队列)是通用的，而另一些则是为特定目的而创建和维护的(根据其内核线程的名称，如块 I/O、`kworker*blockd`、内存控制、`kworker*mm_percpu_wq`、设备特定的，如 tpm、`tpm_dev_wq`、CPU 频率调节器驱动程序、`devfreq_wq`等)。

请注意，内核工作队列子系统自动、优雅且高效地维护所有这些工作队列(及其相关的内核线程工作池)。

那么，你实际上如何利用工作队列呢？下一节将向您展示如何使用内核全局工作队列。接下来是一个演示内核模块，它清楚地展示了它的用法。

## 使用内核全局工作队列

在本节中，我们将学习如何使用内核全局(也称为系统或事件工作队列，这是默认的)工作队列。这通常包括用您的工作任务初始化工作队列，让它消耗您的工作，最后执行清理。

### 为您的任务初始化内核全局工作队列–INIT _ WORK()

将工作排入这个工作队列实际上非常容易:使用`INIT_WORK()`宏！这个宏有两个参数:

```sh
#include <linux/workqueue.h>
INIT_WORK(struct work_struct *_work, work_func_t _func);
```

`work_struct`结构是工作队列的主力结构(至少从模块/驱动作者的角度来看)；您将为它分配内存，并将指针作为第一个参数传递。`INIT_WORK()`的第二个参数是指向工作队列回调函数的指针，该函数将被工作队列的工作线程使用！`work_func_t`是指定此功能签名的`typedef`，即`void (*work_func_t)(struct work_struct *work)`。

### 让您的工作任务执行–schedule _ work()

调用`INIT_WORK()`向内部默认的内核全局工作队列注册指定的工作结构和功能。但是它还没有执行它——还没有！您必须通过在适当的时候调用`schedule_work()`应用编程接口来告诉它何时执行您的“工作”:

```sh
bool schedule_work(struct work_struct *work);
```

很明显，`schedule_work()`的参数是指向`work_struct`结构的指针(您之前通过`INIT_WORK()`宏初始化了该结构)。它返回一个布尔值(直接引用来源):`%false if @work was already on the kernel-global workqueue and %true otherwise True`。实际上，`schedule_work()`检查指定的函数(通过工作结构)是否已经在内核全局工作队列中；如果没有，它就在那里排队；如果它已经在那里了，它就把它单独留在同一个位置(它不会再添加一个实例)。然后，它标记要执行的工作项。这通常会在对应于工作队列的底层内核线程被调度后立即发生，从而为您提供了运行工作的机会。

To have two work items (functions) within your module or driver execute via the (default) kernel-global workqueue, simply call the `INIT_WORK()` macro twice, each time passing different work structures and functions. Similarly, for more work items, call `INIT_WORK()` for each of them... (For example, take this kernel block driver (`drivers/block/mtip32xx/mtip32xx.c`): apparently, for Micron PCIe SSDs, it calls `INIT_WORK()` eight times in a row (!) with its probe method, using arrays to hold all the items).

注意可以在原子上下文中调用`schedule_work()`！呼叫是非阻塞的；它只是安排工作项在稍后的、延迟的(并且安全的)时间点被使用，此时它将在流程上下文中运行。

#### 安排工作任务的不同方式

我们刚才描述的`schedule_work()` API 有一些变体，所有这些都可以通过`schedule[_delayed]_work[_on]()`API 获得。让我们简单列举一下。首先来看看`schedule_delayed_work()`内联函数，其签名如下:

```sh
bool schedule_delayed_work(struct delayed_work *dwork, unsigned long delay);
```

当您想要将工作队列处理程序函数的执行延迟指定的时间量时，请使用此例程；第二个参数`delay`，是你要等待的`jiffies`的个数。现在，我们知道`jiffies`变量每秒增加`HZ`次；因此，要让您的工作任务延迟`n`秒，请指定`n * jiffies`。同样，您可以始终将`msecs_to_jiffies(n)`值作为第二个参数传递，让它从现在开始执行`n`毫秒。

接下来，注意第一个参数`schedule_delayed_work()`不同；这是一个`delayed_work`结构，它本身包含现在熟悉的`work_struct`结构作为一个成员，以及其他内务成员(一个内核定时器、一个指向工作队列结构的指针和一个中央处理器号)。要初始化它，只需给它分配内存，然后使用`INIT_DELAYED_WORK()`宏(语法与`INIT_WORK()`相同)；它将处理所有初始化。

主题上的另一个细微变化是`schedule[_delayed]_work_on()`套路；名称中的`on`允许您指定您的工作任务在执行时将被安排在哪个 CPU 核心上。以下是`schedule_delayed_work_on()`内联函数的签名:

```sh
bool schedule_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
```

第一个参数指定执行工作任务的中央处理器内核，而其余两个参数与`schedule_delayed_work()`例程的参数相同。(您可以使用`schedule_delayed_work()`例程在给定的中央处理器内核上立即安排您的任务)。

### 清理–取消或刷新工作任务

在某些时候，您可能希望确保您的工作任务已经实际完成执行。您可能希望在销毁您的工作队列之前这样做(假设它是一个自定义创建的工作队列，而不是内核全局工作队列)，或者更有可能的是，在 LKM 或驱动程序的清理方法中使用内核全局工作队列时。这里使用的典型 API 是`cancel_[delayed_]work[_sync]()`。其变体和签名如下:

```sh
bool cancel_work_sync(struct work_struct *work);
bool cancel_delayed_work(struct delayed_work *dwork);
bool cancel_delayed_work_sync(struct delayed_work *dwork);
```

挺简单的，真的:用过`INIT_WORK()``schedule_work()`套路就用`cancel_work_sync()`；当你推迟了工作任务时，使用后两种方法。请注意，其中两个例程以`_sync`为后缀；这意味着取消是*同步*–内核会等到你的工作任务完成执行，这些函数才会返回！这通常是我们想要的。这些例程返回一个布尔值:`True`如果有工作等待处理，否则返回`False`。

Within a kernel module, not canceling (or flushing) your work task(s) in your cleanup (`rmmod`) code path is a sure-fire way to cause serious issues; ensure you do so!

内核工作队列子系统还提供了一些`flush_*()`例程(包括`flush_scheduled_work()`、`flush_workqueue()`和`flush_[delayed_]work()`)。内核文档([https://www . kernel . org/doc/html/latest/core-API/workqueue . html](https://www.kernel.org/doc/html/latest/core-api/workqueue.html))清楚地警告我们，这些例程不是最容易使用的，因为您很容易使用它们导致死锁问题。建议您改用前面提到的`cancel_[delayed_]work[_sync]()`API。

### 工作流程的快速摘要

使用内核全局工作队列时，会出现一个简单的模式(工作流):

1.  *初始化*工作任务。
2.  在适当的时间点，*安排*执行它(可能有延迟和/或在特定的中央处理器内核上)。
3.  清理干净。通常，在内核模块(或驱动程序)清理代码路径中，*取消*它。(最好同步进行，以便首先完成任何未完成的工作任务。在这里，我们将坚持使用推荐的`cancel*work*()`套路，避免使用`flush_*()`套路。

让我们用一个表格来总结一下:

| **使用内核全局工作队列** | **常规工作任务** | **延迟工作任务** | **在给定的 CPU 上执行工作任务** |
| 1.初始化 | `INIT_WORK()` | `INIT_DELAYED_WORK()` | *<要么即刻要么推迟的罚款>* |
| 2.计划要执行的工作任务 | `schedule_work()` | `schedule_delayed_work()` | `schedule_delayed_work_on()` |
| 3.取消(或冲洗)它； *foo_sync()* 确保完成 | `cancel_work_sync()` | `cancel_delayed_work_sync()` | *<要么即刻要么推迟的罚款>* |

Table 5.4 – Using the kernel-global workqueue – summary of the workflow

在接下来的几节中，我们将使用内核默认工作队列编写一个简单的内核模块，以便执行工作任务。

## 我们简单的工作队列内核模块——代码视图

让我们用工作队列来动手吧！在接下来的部分中，我们将编写一个简单的演示内核模块(`ch5/workq_simple`)，演示如何使用内核默认的工作队列来执行工作任务。它实际上建立在我们早期的 LKM 之上，我们用它来演示内核定时器(`ch5/timer_simple`)。让我们从代码的角度来检查它(像往常一样，我们不会在这里显示完整的代码，只显示最相关的部分)。我们将从查看它的私有上下文数据结构和*初始化*方法开始:

```sh
static struct st_ctx {
    struct work_struct work;
    struct timer_list tmr;
    int data;
} ctx;
[ ... ]
static int __init workq_simple_init(void)
{
    ctx.data = INITIAL_VALUE;
    /* Initialize our work queue */
 INIT_WORK(&ctx.work, work_func);
    /* Initialize our kernel timer */
    ctx.tmr.expires = jiffies + msecs_to_jiffies(exp_ms);
    ctx.tmr.flags = 0;
    timer_setup(&ctx.tmr, ding, 0);
    add_timer(&ctx.tmr); /* Arm it; let's get going! */
    return 0;
}
```

需要思考的一个关键点是:我们将如何设法将一些有用的数据项传递给我们的工作功能？`work_struct`结构只有一个用于内部目的的原子长整数。a 好(而且很典型！)诀窍是让你的`work_struct`结构嵌入到你的驱动的上下文结构中；然后，在工作任务回调函数中，使用`container_of()`宏访问父上下文数据结构！这是经常采用的策略。(第`container_of()`是一个强大的宏，但不是真的容易破译！我们已经在*进一步阅读*部分提供了一些有用的链接。)因此，在前面的代码中，我们让驱动程序的上下文结构在其中嵌入了一个`struct work_struct`。您可以在`INIT_WORK()`宏中看到我们工作任务的初始化。

一旦定时器准备好了(这里`add_timer()`起作用)，它将在大约 420 毫秒后到期，定时器回调函数将在定时器 softirq 上下文中运行(这非常像一个原子上下文):

```sh
static void ding(struct timer_list *timer)
{ 
    struct st_ctx *priv = from_timer(priv, timer, tmr);
    pr_debug("timed out... data=%d\n", priv->data--);
    PRINT_CTX();

    /* until countdown done, fire it again! */
    if (priv->data)
        mod_timer(&priv->tmr, jiffies + msecs_to_jiffies(exp_ms));
    /* Now 'schedule' our work queue function to run */
    if (!schedule_work(&priv->work))
        pr_notice("our work's already on the kernel-global workqueue!\n");
}
```

递减`data`变量后，它设置计时器再次触发(420 ms 内，通过`mod_timer()`)，之后，通过`schedule_work()` API，它调度我们的工作队列回调运行！内核将认识到工作队列函数现在必须尽快执行(消耗)。但是等等——工作队列回调必须并且将仅通过全局内核工作线程(所谓的事件线程)在进程上下文中运行*。因此，只有当我们脱离了这个 softirq 上下文，并且(其中一个)“事件”内核工作线程在一个 CPU 运行队列上并且实际运行时，我们的工作队列回调函数才会被调用。*

放松——这很快就会发生...使用工作队列的全部意义在于，不仅线程管理完全由内核负责，而且该函数还在进程上下文中运行，这样就有可能执行冗长的阻塞或 I/O 操作。

再说一遍，多快就是多快？让我们尝试测量一下:我们将紧接在`schedule_work()`之后的时间戳(通过通常的`ktime_get_real_ns()`内联函数)作为工作队列函数中的第一行代码。我们值得信赖的`SHOW_DELTA()`宏观显示了时间上的差异。不出所料，它很小，通常在百分之几微秒的范围内(当然，这取决于几个因素，包括硬件平台、内核版本等)。高负载系统会导致上下文切换到事件内核线程花费更长的时间，这可能会导致工作队列的功能执行延迟。您将在以下部分的截图中看到它的示例运行(*图 5.12* )。

下面的代码是我们的工作任务函数。这就是我们使用`container_of()`宏来访问模块上下文结构的地方:

```sh
/* work_func() - our workqueue callback function! */
static void work_func(struct work_struct *work)
{
    struct st_ctx *priv = container_of(work, struct st_ctx, work);

    t2 = ktime_get_real_ns();
    pr_info("In our workq function: data=%d\n", priv->data);
    PRINT_CTX();
    SHOW_DELTA(t2, t1);
}
```

此外，我们的`PRINT_CTX()`宏的输出最终表明该函数在流程上下文中运行。

Be careful when you're using `container_of()` within a *delayed* work task callback function – you'll have to specify the third parameter as a `work` member of `struct delayed_work` (one of our exercise questions has you try out this very thing! There's a solution provided as well...). I suggest that you master the basics first before trying this out for yourself.

在下一节中，我们将运行我们的内核模块。

## 我们简单的工作队列内核模块——运行它

我们去兜兜风吧！请看下面的截图:

![](assets/a9d89aad-617f-47e8-88d5-37443a49ce5b.png)

Figure 5.12 – Our workq_simple.ko LKM with the work queue function execution highlighted

让我们更详细地看看这段代码:

*   通过我们的`lkm`助手脚本，我们构建然后`insmod(8)`内核模块；也就是`workq_simple.ko`。
*   内核日志通过`dmesg(1)`显示:
    *   这里，工作队列和内核定时器在 init 方法中被初始化和武装。
    *   定时器到期(大约 420 毫秒)；你可以看到它的 printks(显示`timed out...`和我们的`data`变量的值)。
    *   它调用`schedule_work()` API，导致我们的工作队列函数运行。
    *   如前一张截图中突出显示的，我们的工作队列函数`work_func()`确实在运行；它显示数据变量的当前值，证明它正确地获得了对我们的“上下文”或私有数据结构的访问。

请注意，我们在这个 LKM 中使用了我们的`PRINT_CTX()`宏(它在我们的`convenient.h`头中)来揭示一些有趣的事情:

接下来，`SHOW_DELTA()`宏计算并输出正在调度的工作队列和实际执行的工作队列之间的时间差。如您所见(这里，至少在我们的轻负载 x86_64 来宾虚拟机上)，它在几百微秒的范围内。

为什么不查找用于消耗我们的工作队列的实际内核工作线程呢？PID 上一个简单的`ps(1)`就是这里所需要的。在这种特殊情况下，它恰好是内核的每个 CPU 内核通用工作队列使用者线程之一——一个内核工作者(`kworker/...`)线程:

```sh
$ ps -el | grep -w 55200
 1 I     0   55200       2  0  80  0 -    0 -    ?       00:00:02 kworker/1:0-mm_percpu_wq
 $
```

当然，内核代码库充满了工作队列的使用(尤其是许多设备驱动程序)。请使用`cscope(1)`查找并浏览此类代码的实例。

## sed3 迷你项目–非常简单的外观

让我们在本章结束时，简单回顾一下我们的`sed2`项目到`sed3`的演变。这个小项目和`sed2`一模一样，只是更简单！(en/de)加密工作**现在由我们的工作任务(函数)通过内核的工作队列功能**或下半部分机制来执行。我们使用工作队列——默认的内核全局工作队列——来完成工作，而不是手动创建和管理 kthreads(就像我们在`sed2`中所做的那样)！

下面的截图显示了我们访问一个示例运行的内核日志；在运行中，我们让用户模式应用加密，然后解密，然后检索消息进行查看。我们在这里强调了两个红色矩形中有趣的部分——通过内核全局工作队列的工作线程执行我们的工作任务:

![](assets/670779a5-067a-4d15-be9b-d4dcd7b862b5.png)

Figure 5.13 – Kernel log when running our sed3 driver; the work task running via the default kernel-global workqueue is highlighted

顺便说一下，用户模式应用和我们在`sed2`中使用的是一样的。前面的截图显示了(通过我们信任的`PRINT_CTX()`宏)内核全局工作队列用来运行我们的加密和解密工作的实际内核工作线程；在这种特殊情况下，加密工作是`[kworker/1:0]` PID 9812，解密工作是`[kworker/0:2]` PID 9791。请注意它们都是如何在流程上下文中运行的。我们将让您浏览`sed3` ( `ch5/sed3`)的代码。

这就结束了这一部分。在这里，您了解了内核工作队列基础设施如何成为模块/驱动程序作者的福音，因为它帮助您在内核线程、它们的创建以及复杂的管理和操作的底层细节上添加了一个强大的抽象层。它使您可以非常容易地在内核中执行工作，尤其是通过使用预先存在的内核全局(默认)工作队列，而不必担心血淋淋的细节。

# 摘要

干得好！我们在这一章中谈了很多。首先，您学习了如何在内核空间中创建延迟，包括原子和阻塞类型(分别通过`*delay()`和`*sleep()`例程)。接下来，您学习了如何在 LKM(或驱动程序)中设置和使用内核定时器——这是一项非常常见且必需的任务。直接创建和使用内核线程可能是一种令人兴奋(甚至困难)的体验，这就是为什么您学习了这样做的基础。之后，您看了内核工作队列子系统，它解决了复杂性(和并发性)问题。您了解了它是什么，以及如何实际利用内核全局(默认)工作队列来使您的工作任务在需要时执行。

我们设计和实现的三个`sed`(简单加密解密)演示驱动程序系列向您展示了这些有趣技术的一个更复杂的用例:`sed1`使用超时实现，`sed2`添加到内核线程来执行工作，`sed3`使用内核全局工作队列在需要时消耗工作。

请花一些时间来完成本章的以下*问题*/练习，并浏览*进一步阅读*资源。完成后，我建议你好好休息一下，跳回去。我们就快到了:最后两章涵盖了一个非常关键的主题——内核同步！

# 问题

1.  找出以下伪代码中的错误:

```sh
static my_chip_tasklet(void)
{
    // ... process data
    if (!copy_to_user(to, from, count)) {
        pr_warn("..."); [...]
    }
}
static irqreturn_t chip_hardisr(int irq, void *data)
{
    // ack irq
    // << ... fetch data into kfifo ... >>
    // << ... call func_a(), delay, then call func_b() >>
    func_a();
    usleep(100); // 100 us delay required here! see datasheet pg ...
    func_b();
    tasklet_schedule(...);
    return IRQ_HANDLED;
}
my_chip_probe(...)
{
    // ...
    request_irq(CHIP_IRQ, chip_hardisr, ...);
    // ...
    tasklet_init(...);
}
```

2.  `timer_simple_check`:增强`timer_simple`内核模块，使其检查设置超时和实际服务之间经过的时间。
3.  `kclock`:写一个内核模块，设置一个内核定时器，让它每秒都超时。然后，使用它将时间戳打印到内核日志中，实际上在内核中获得一个简单的“时钟应用”。

4.  `mutlitime` *:* 开发一个内核模块，以发出定时器回调的秒数作为参数。将其默认为零(意味着没有计时器，因此存在有效性错误)。它应该是这样工作的:如果传递的数字是 3，它应该创建三个内核定时器；第一个将在 3 秒后过期，第二个在 2 秒后过期，最后一个在 1 秒后过期。换句话说，如果传递的数字是“n”，它应该创建“n”个内核定时器；第一个将在“n”秒内过期，第二个在“n-1”秒内过期，第三个在“n-2”秒内过期，以此类推，直到计数达到零。
5.  构建并运行本章中提供的`sed[123]`小项目，并验证(通过查看内核日志)它们以应有的方式工作。
6.  `workq_simple2`:我们提供的`ch5/workq_simple` LKM 通过内核-全局工作队列设置并“消耗”一个工作项(函数)；增强它，以便它设置和执行两个“工作”任务。验证它是否正常工作。
7.  `workq_delayed`:在前一个任务(`workq_simple2`)的基础上执行两个工作任务，再加上一个任务(来自初始化代码路径)。这一个(第三个)应该延迟；延迟的时间量应该作为名为`work_delay_ms`的模块参数传递(以毫秒为单位；默认值应为 500 毫秒)。
    【*提示:*在延时工作任务回拨功能内使用`container_of()`时要小心；您必须指定第三个参数作为`struct delayed_work`的`work`成员；查看我们提供的解决方案]。

You will find some of the questions answered in the book's GitHub repo: [https://github.com/PacktPublishing/Linux-Kernel-Programming-Part-2/tree/main/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming-Part-2/tree/main/solutions_to_assgn).

# 进一步阅读

*   内核文档:*延迟，休眠机制*:[https://www . kernel . org/doc/Documentation/timers/timers-how to . tx](https://www.kernel.org/doc/Documentation/timers/timers-howto.txt)
*   内核定时器系统:[https://elinux.org/Kernel_Timer_Systems#Timer_information](https://elinux.org/Kernel_Timer_Systems#Timer_information)

*   工作队列:
    *   这是一个非常好的演示:*与工作队列的异步执行*，Bhaktipriya Shridhar:[https://events . static . linuxfind . org/sites/events/file/slides/Async % 20 execution % 20 with % 20 wqs . pdf](https://events.static.linuxfound.org/sites/events/files/slides/Async%20execution%20with%20wqs.pdf)
    *   内核文档:*并发管理工作队列(cmwq)*:[https://www . kernel . org/doc/html/latest/core-API/Workqueue . html #并发管理工作队列-cmwq](https://www.kernel.org/doc/html/latest/core-api/workqueue.html#concurrency-managed-workqueue-cmwq)

*   `container_of()`宏解释道:
    *   *宏*的神奇容器 _ 2012 年 11 月:[https://radek.io/2012/11/10/magical-container_of-macro/](https://radek.io/2012/11/10/magical-container_of-macro/)
    *   *对 linux 内核中宏容器的理解*:[https://embetronix . com/tutories/Linux/c-programming/对 Linux 内核中宏容器的理解/](https://embetronicx.com/tutorials/linux/c-programming/understanding-of-container_of-macro-in-linux-kernel/)