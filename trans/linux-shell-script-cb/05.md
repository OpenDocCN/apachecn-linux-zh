# 纠结网？一点也不！

在本章中，我们将介绍以下食谱:

*   从网页下载
*   以纯文本形式下载网页
*   cURL 入门
*   从命令行访问未读的 Gmail 电子邮件
*   解析来自网站的数据
*   图像爬虫和下载器
*   网络相册生成器
*   Twitter 命令行客户端
*   通过网络服务器访问单词定义
*   在网站中查找断开的链接
*   跟踪网站的更改
*   发布到网页并阅读回复
*   从互联网下载视频
*   与 OTS 一起总结文本
*   从命令行翻译文本

# 介绍

网络已经成为技术的表面和数据处理的中心接入点。Shell 脚本不能做像 PHP 这样的语言在 Web 上能做的所有事情，但是有很多任务是 shell 脚本非常适合的。我们将探索下载和解析网站数据、将数据发送到表单以及自动化网站使用任务和类似活动的方法。我们可以通过浏览器用几行脚本自动执行许多交互活动。HTTP 协议和命令行实用程序提供的功能使我们能够编写脚本来解决许多 web 自动化需求。

# 从网页下载

下载文件或网页很简单。一些命令行下载实用程序可用于执行此任务。

# 准备好

`wget`是一个灵活的文件下载命令行实用程序，可以配置很多选项。

# 怎么做...

使用`wget`可以下载网页或远程文件:

```
$ wget URL

```

例如:

```
$ wget knopper.net
--2016-11-02 21:41:23--  http://knopper.net/
Resolving knopper.net... 85.214.68.145
Connecting to knopper.net|85.214.68.145|:80...
connected.
HTTP request sent, awaiting response... 200 OK
Length: 6899 (6.7K) [text/html]
Saving to: "index.html.1"

100% [=============================]45.5K=0.1s

2016-11-02 21:41:23 (45.5 KB/s) - "index.html.1" saved
[6899/6899]

```

也可以指定多个下载网址:

```
$ wget URL1 URL2 URL3 ..

```

# 它是如何工作的...

默认情况下，下载的文件与网址同名，下载信息和进度写入`stdout`。

`-O`选项指定输出文件名。如果同名文件已经存在，它将被下载的文件替换:

```
$ wget http://www.knopper.net -O knopper.html.

```

`-o`选项指定了一个`logfile`，而不是将日志打印到`stdout`:

```
$ wget ftp://ftp.example.com/somefile.img -O dloaded_file.img -o log

```

使用前面的命令不会在屏幕上打印任何内容。日志或进度将被写入日志，输出文件为`dloaded_file.img`。

由于互联网连接不稳定，下载可能会中断。`-t`选项指定在放弃之前实用程序将重试多少次:

```
$ wget -t 5 URL

```

Use a value of `0` to force `wget` to keep trying infinitely:

```
$ wget -t 0 URL

```

# 还有更多...

`wget`实用程序有微调行为和解决问题的选项。

# 限制下载速度

当许多应用程序共享有限的带宽时，一个大文件可能会吞噬所有带宽，并使其他进程(可能是交互式用户)挨饿。`wget`选项`-limit-rate`将指定下载作业的最大带宽，允许所有应用程序公平访问互联网:

```
$ wget  --limit-rate 20k http://example.com/file.iso

```

在该命令中，`k`(千字节)指定速度限制。也可以使用`m`进行兆字节。

`-quota`(或`-Q`)选项指定下载的最大大小。`wget`超过配额将停止。这在将多个文件下载到空间有限的系统时非常有用:

```
$ wget -Q 100m http://example.com/file1 http://example.com/file2

```

# 继续下载并继续

如果`wget`在下载完成前被中断，可以使用`-c`选项从中断处恢复:

```
$ wget -c URL

```

# 复制完整的网站(镜像)

`wget`可以递归收集 URL 链接，像爬虫一样下载，就可以下载一个完整的网站。要下载页面，请使用`--mirror`选项:

```
$ wget --mirror --convert-links exampledomain.com

```

或者，使用以下命令:

```
$ wget -r -N -l -k DEPTH URL

```

`-l`选项将网页的深度指定为级别。这意味着它将只遍历该数量的级别。它与`-r`(递归)一起使用。`-N`参数用于启用文件的时间戳。`URL`是需要启动下载的网站的基本网址。`-k`或`--convert-links`选项指示`wget`将其他页面的链接转换为本地副本。

Exercise discretion when mirroring other websites. Unless you have permission, only perform this for your personal use and don't do it too frequently.

# 使用 HTTP 或 FTP 身份验证访问页面

`--user`和`--password`参数为需要认证的网站提供用户名和密码。

```
$ wget --user username --password pass URL

```

也可以在不内嵌指定密码的情况下要求输入密码。为此，请使用`--ask-password`代替`--password`参数。

# 以纯文本形式下载网页

网页只是带有 HTML 标签、JavaScript 和 CSS 的文本。HTML 标签定义了网页的内容，我们可以对其进行解析以获得特定的内容。Bash 脚本可以解析网页。可以在网络浏览器中查看 HTML 文件，以查看其格式是否正确，或者使用上一章中描述的工具进行处理。

解析文本文档比解析 HTML 数据更简单，因为我们不需要剥离 HTML 标签。 **Lynx** 是一款命令行网页浏览器，可以以纯文本方式下载网页。

# 准备好

Lynx 并未在所有发行版中安装，但可以通过软件包管理器获得。

```
# yum install lynx

```

或者，您可以执行以下命令:

```
 apt-get install lynx

```

# 怎么做...

`-dump`选项以纯 ASCII 形式下载网页。下一个方法显示了如何将页面的 ASCII 版本发送到文件:

```
$ lynx URL -dump > webpage_as_text.txt

```

该命令将在标题`References`下单独列出所有超链接(`<a href="link">`)，作为文本输出的页脚。这让我们可以用正则表达式分别解析链接。

考虑这个例子:

```
$lynx -dump http://google.com > plain_text_page.txt

```

您可以使用`cat`命令查看`text`的纯文本版本:

```
    $ cat plain_text_page.txt
 Search [1]Images [2]Maps [3]Play [4]YouTube [5]News [6]Gmail   
    [7]Drive
 [8]More »
 [9]Web History | [10]Settings | [11]Sign in

 [12]St. Patrick's Day 2017

 _______________________________________________________
 Google Search  I'm Feeling Lucky    [13]Advanced search
 [14]Language tools

 [15]Advertising Programs     [16]Business Solutions     [17]+Google
 [18]About Google

 © 2017 - [19]Privacy - [20]Terms

References
...

```

# cURL 入门

**cURL** 使用 HTTP、HTTPS 或 FTP 协议将数据传输到服务器或从服务器传输数据。它支持`POST`、cookies、身份验证、从指定偏移量下载部分文件、引用者、用户代理字符串、额外的头、限制速度、最大文件大小、进度条等等。cURL 对于维护网站、检索数据和检查服务器配置非常有用。

# 准备好了

与`wget`不同，cURL 并不包含在所有的 Linux 发行版中；您可能需要使用软件包管理器来安装它。

默认情况下，cURL 将下载的文件转储到`stdout`，将进度信息转储到`stderr`。要禁用显示进度信息，请使用`--silent`选项。

# 怎么做...

`curl`命令执行很多功能，包括下载、发送不同的 HTTP 请求、指定 HTTP 头。

*   要将下载的文件转储到`stdout`，请使用以下命令:

```
        $ curl URL

```

*   `-O`选项指定将下载的数据发送到一个文件中，该文件的文件名由网址解析而来。请注意，网址必须是一个完整的网页网址，而不仅仅是一个网站名称。

```
        $ curl www.knopper.net/index.htm --silent -O

```

*   `-o`选项指定输出文件名。使用此选项，您可以仅指定网站名称来检索主页。

```
        $curl www.knopper.net -o knoppix_index.html
 % Total % Received % Xferd  Avg  Speed Time   Time  Time  
        Current
 Dload Upload Total Spent Left  Speed
 100 6889 100 6889  0 0     10902  0     --:-- --:-- --:-- 26033

```

*   `-silent`选项阻止`curl`命令显示进度信息:

```
        $ curl URL --silent

```

*   下载时`-progress`选项显示进度条:

```
        $ curl http://knopper.net -o index.html --progress
 ################################## 100.0% 

```

# 它是如何工作的...

cURL 将网页或远程文件下载到您的本地系统。您可以使用`-O`和`-o`选项控制目标文件名，使用`-silent`和`-progress`选项控制详细程度。

# 还有更多...

在前面几节中，您学习了如何下载文件。cURL 支持更多选项来微调其行为。

# 继续和恢复下载

cURL 可以从给定的偏移量恢复下载。如果您有每日数据限制和要下载的大文件，这很有用。

```
$ curl URL/file -C offset

```

offset 是以字节为单位的整数值。

如果我们想继续下载文件，cURL 不要求我们知道确切的字节偏移量。如果您希望 cURL 找出正确的恢复点，请使用`-C -`选项，如下所示:

```
$ curl -C - URL

```

cURL 会自动找出在哪里重新开始下载指定的文件。

# 用 cURL 设置引用字符串

The **Referer** field in the HTTP header identifies the page that led to the current web page. When a user clicks on a link on web page A to go to web page B, the referer header string for page B will contain the URL of page A.

一些动态页面在返回 HTML 数据之前会检查引用字符串。例如，当用户从谷歌导航到网站时，网页可能会显示谷歌徽标，而当用户键入网址时，网页可能会显示不同的页面。

如果推荐人是 www.google.com，网络开发者可以写一个条件返回谷歌页面，如果不是，返回一个不同的页面。

您可以使用`--referer`和`curl`命令来指定引用字符串，如下所示:

```
$ curl --referer Referer_URL target_URL

```

考虑这个例子:

```
$ curl --referer http://google.com http://knopper.org

```

# 带 cURL 的饼干

`curl`可以指定和存储 HTTP 操作时遇到的 cookies。

`-cookie` `COOKIE_IDENTIFER`选项指定提供哪些 cookies。饼干被定义为`name=value`。多个 cookies 应该用分号(`;`)分隔:

```
$ curl http://example.com --cookie "user=username;pass=hack"

```

`-cookie-jar`选项指定存储 cookies 的文件:

```
$ curl URL --cookie-jar cookie_file

```

# 使用 cURL 设置用户代理字符串

如果没有指定用户代理，某些检查用户代理的网页将无法工作。比如一些老网站需要**互联网浏览器** ( **IE** )。如果使用不同的浏览器，它们会显示一条消息，说明必须使用 IE 查看该网站。这是因为网站会检查用户代理。可以用`curl`设置用户代理。

`--user-agent`或`-A`选项设置用户代理:

```
$ curl URL --user-agent "Mozilla/5.0"

```

附加的头可以用 cURL 传递。使用`-H "Header"`传递附加标题:

```
$ curl -H "Host: www.knopper.net" -H "Accept-language: en" URL

```

There are many different user agent strings across multiple browsers and crawlers on the Web. You can find a list of some of them at [http://www.useragentstring.com/pages/useragentstring.php](http://www.useragentstring.com/pages/useragentstring.php).

# 在 cURL 上指定带宽限制

当多个用户共享带宽时，我们可以通过`--limit-rate`选项限制下载速率:

```
$ curl URL --limit-rate 20k

```

速率可以用`k`(千字节)或`m`(兆字节)指定。

# 指定最大下载大小

`--max-filesize`选项指定最大文件大小:

```
$ curl URL --max-filesize bytes

```

The `curl` command will return a non-zero exit code if the file size exceeds the limit or a zero if the download succeeds.

# 使用 cURL 进行身份验证

`curl`命令的`-u`选项执行 HTTP 或 FTP 身份验证。

可以使用`-u username:password`指定用户名和密码:

```
$ curl -u user:pass http://test_auth.com

```

如果您希望系统提示您输入密码，请仅提供用户名:

```
$ curl -u user http://test_auth.com 

```

# 打印不含数据的响应标题

检查标题对于许多检查和统计来说已经足够了。例如，我们不需要下载整个页面来确认它是可到达的。仅仅阅读 HTTP 响应就足够了。

检查 HTTP 头的另一个用例是在下载前检查`Content-Length`字段以确定文件大小，或者检查`Last-Modified`字段以查看文件是否比当前副本新。

`-I`或`-head`选项只输出 HTTP 头，不下载远程文件:

```
$ curl -I http://knopper.net 
HTTP/1.1 200 OK
Date: Tue, 08 Nov 2016 17:15:21 GMT
Server: Apache
Last-Modified: Wed, 26 Oct 2016 23:29:56 GMT
ETag: "1d3c8-1af3-b10500"
Accept-Ranges: bytes
Content-Length: 6899
Content-Type: text/html; charset=ISO-8859-1

```

# 请参见

*   将*发布到网页并阅读本章中的响应*食谱

# 从命令行访问未读的 Gmail 电子邮件

Gmail 是谷歌广泛使用的免费电子邮件服务:[http://mail.google.com/](http://mail.google.com/)。它允许您通过浏览器或经过验证的 RSS 源阅读邮件。我们可以解析 RSS 提要来报告发送者的姓名和主题。这是一种无需打开网络浏览器即可扫描未读电子邮件的快速方法。

# 怎么做...

让我们通过一个 shell 脚本来解析 Gmail 的 RSS 提要，以显示未读邮件:

```
#!/bin/bash 
#Desc: Fetch gmail tool 

username='PUT_USERNAME_HERE' 
password='PUT_PASSWORD_HERE' 

SHOW_COUNT=5 # No of recent unread mails to be shown 

echo 
curl -u $username:$password --silent \
    "https://mail.google.com/mail/feed/atom" | \
     tr -d '\n' | sed 's:</entry>:\n:g' |\ 
     sed -n 's/.*<title>\(.*\)<\/title.*<author><name>\([^<]*\)<\/name><email>
 \([^<]*\).*/From: \2 [\3] \nSubject: \1\n/p' | \ 
head -n $(( $SHOW_COUNT * 3 ))

```

输出如下所示:

```
$ ./fetch_gmail.sh
From: SLYNUX [ slynux@slynux.com ]
Subject: Book release - 2

From: SLYNUX [ slynux@slynux.com ]
Subject: Book release - 1 
.
... 5 entries

```

If you use a Gmail account with two-factor authentication, you will have to generate a new key for this script and use it. Your regular password won't work.

# 它是如何工作的...

该脚本使用 cURL 下载 RSS 提要。您可以通过登录您的 Gmail 帐户并查看[https://mail.google.com/mail/feed/atom](https://mail.google.com/mail/feed/atom)来查看传入数据的格式。

cURL 使用`-u user:pass`参数提供的用户认证读取 RSS 提要。当你使用`-u user`而没有密码 cURL 时，它会交互询问密码。

*   `tr -d '\n'`:这将删除换行符
*   `sed 's:</entry>:\n:g'`:这将每个`</entry>`元素替换为一个换行符，因此每个电子邮件条目都由一个新行分隔，因此邮件可以被逐个解析。

需要作为单个表达式执行的下一个脚本块使用`sed`提取相关字段:

```
 sed 's/.*<title>\(.*\)<\/title.*<author><name>\([^<]*\)<\/name><email>
 \([^<]*\).*/Author: \2 [\3] \nSubject: \1\n/' 

```

该脚本将标题与`<title>\(.*\)<\/title`正则表达式匹配，将发件人姓名与`<author><name>\([^<]*\)<\/name>`正则表达式匹配，并使用`<email>\([^<]*\)`发送电子邮件。Sed 使用反向引用将电子邮件的作者、标题和主题显示为易于阅读的格式:

```
Author: \2 [\3] \nSubject: \1\n 

```

`\1`对应第一个子串匹配(标题)`\2`为第二个子串匹配(名称)，以此类推。

`SHOW_COUNT=5`变量用于取终端上待打印的未读邮件条目数。

`head`用于仅显示第一行开始的`SHOW_COUNT*3`行。`SHOW_COUNT`乘以 3，以显示三行输出。

# 请参见

*   本章中的*cURL*食谱入门解释了`curl`命令
*   使用 sed 执行文本替换的*在[第 4 章](04.html)、*文本和驾驶中*解释了`sed`命令*

# 解析来自网站的数据

`lynx`、`sed`和`awk`命令可用于从网站中挖掘数据。你可能会在*中看到一个女演员排名列表，在[第 4 章](04.html)、*发短信和开车*的 grep* 食谱文件中搜索和挖掘文本；它是通过解析[http://www.johntorres.net/BoxOfficefemaleList.html](http://www.johntorres.net/BoxOfficefemaleList.html)网页生成的。

# 怎么做...

让我们从网站上浏览用于解析女演员详细信息的命令:

```
$ lynx -dump -nolist \
    http://www.johntorres.net/BoxOfficefemaleList.html
    grep -o "Rank-.*" | \
    sed -e 's/ *Rank-\([0-9]*\) *\(.*\)/\1\t\2/' | \
    sort -nk 1 > actresslist.txt 

```

输出如下:

```
# Only 3 entries shown. All others omitted due to space limits
1   Keira Knightley 
2   Natalie Portman 
3   Monica Bellucci 

```

# 它是如何工作的...

Lynx 是一个命令行 web 浏览器；它可以像我们在网络浏览器中看到的那样转储网站的文本版本，而不是像`wget`或 cURL 那样返回原始 HTML。这省去了移除 HTML 标签的步骤。`-nolist`选项显示没有编号的链接。解析和格式化包含等级的行是通过`sed`完成的:

```
sed -e 's/ *Rank-\([0-9]*\) *\(.*\)/\1\t\2/'

```

然后根据等级对这些行进行排序。

# 请参见

*   使用 sed 执行文本替换的*[第 4 章](04.html)、*短信和驾驶*中的*食谱解释了`sed`命令
*   本章中的*以纯文本方式下载网页*食谱解释了`lynx`命令

# 图像爬虫和下载器

**图片爬虫**下载网页中出现的所有图片。我们可以使用脚本来识别图像并自动下载它们，而不是手动浏览 HTML 页面来挑选图像。

# 怎么做...

这个 Bash 脚本将识别并从网页下载图像:

```
#!/bin/bash 
#Desc: Images downloader 
#Filename: img_downloader.sh 

if [ $# -ne 3 ];
then
  echo "Usage: $0 URL -d DIRECTORY"
  exit -1
fi
while [ $# -gt 0 ]
do
  case $1 in
  -d) shift; directory=$1; shift ;;
  *) url=$1; shift;;
  esac
done

mkdir -p $directory;
baseurl=$(echo $url | egrep -o "https?://[a-z.\-]+")

echo Downloading $url 
curl -s $url | egrep -o "<img[^>]*src=[^>]*>" | \
  sed 's/<img[^>]*src=\"\([^"]*\).*/\1/g' | \
  sed "s,^/,$baseurl/," > /tmp/$$.list

cd $directory;

while read filename;
do
  echo Downloading $filename
  curl -s -O "$filename" --silent
done < /tmp/$$.list

```

示例用法如下:

```
$ url=https://commons.wikimedia.org/wiki/Main_Page
$ ./img_downloader.sh $url -d images

```

# 它是如何工作的...

图像下载器脚本读取一个 HTML 页面，剥离除`<img>`以外的所有标签，从`<img>`标签中解析`src="URL"`，并将它们下载到指定的目录。该脚本接受网页网址和目标目录作为命令行参数。

`[ $# -ne 3 ]`语句检查脚本的参数总数是否为三，否则退出并返回一个用法示例。否则，此代码将解析 URL 和目标目录:

```
while [ -n "$1" ] 
do  
  case $1 in 
  -d) shift; directory=$1; shift ;; 
   *) url=${url:-$1}; shift;; 
esac 
done 

```

`while`循环运行，直到处理完所有参数。`shift`命令将参数向左移动，以便`$1`取下一个参数的值；也就是`$2`等等。因此，我们可以通过`$1`本身来评价所有的论点。

`case`语句检查第一个参数(`$1`)。如果匹配`-d`，下一个参数必须是一个目录名，所以参数被移动，目录名被保存。如果参数是任何其他字符串，它就是一个网址。

以这种方式解析参数的优势在于，我们可以将-d 参数放在命令行的任何地方:

```
$ ./img_downloader.sh -d DIR URL

```

或者:

```
$ ./img_downloader.sh URL -d DIR

```

`egrep -o "<img src=[^>]*>"`将只打印匹配的字符串，即包含其属性的`<img>`标签。`[^>]*`短语匹配除结尾`>`即`<img src="image.jpg">`以外的所有字符。

`sed's/<img src=\"\([^"]*\).*/\1/g'`从`src="url"`弦中提取`url`。

有两种类型的图像源路径:相对和绝对。**绝对路径**包含以`http://`或`https://`开头的完整网址。相对网址从`/`或`image_name`本身开始。一个绝对网址的例子是`http://example.com/image.jpg`。相对网址的一个例子是`/image.jpg`。

对于相对网址，开始的`/`应替换为基础网址，将其转换为`http://example.com/image.jpg`。该脚本通过以下命令从初始网址中提取`baseurl`来初始化它:

```
baseurl=$(echo $url | egrep -o "https?://[a-z.\-]+") 

```

先前描述的`sed`命令的输出通过管道传输到另一个 sed 命令中，用`baseurl`替换前导的`/`，结果保存在一个以脚本的 PID 命名的文件中:(`/tmp/$$.list`)。

```
sed "s,^/,$baseurl/," > /tmp/$$.list 

```

最后的`while`循环遍历列表的每一行，并使用 curl 下载图像。`--silent`参数与`curl`一起使用，以避免在屏幕上打印额外的进度信息。

# 请参见

*   本章中的*cURL*食谱入门解释了`curl`命令
*   使用 sed 执行文本替换的*[第 4 章](04.html)、*短信和驾驶*中的*食谱解释了`sed`命令
*   [第 4 章](04.html)、*发短信和开车*中的*使用 grep* 配方搜索和挖掘文件中的文本，解释了`grep`命令

# 网络相册生成器

网络开发人员经常创建全尺寸和缩略图的相册。单击缩略图时，会显示图片的大版本。这需要调整大小和放置许多图像。这些动作可以通过一个简单的 Bash 脚本自动完成。脚本创建缩略图，将它们放在精确的目录中，并自动生成`<img>`标签的代码片段。

# 准备好

该脚本使用`for`循环来迭代当前目录中的每个图像。使用了常用的 Bash 工具，如`cat`和`convert`(来自 Image Magick 包)。这些将生成一个 HTML 相册，使用所有的图像，在`index.html`。

# 怎么做...

这个 Bash 脚本将生成一个 HTML 相册页面:

```
#!/bin/bash 
#Filename: generate_album.sh 
#Description: Create a photo album using images in current directory 

echo "Creating album.." 
mkdir -p thumbs 
cat <<EOF1 > index.html 
<html> 
<head> 
<style> 

body  
{  
  width:470px; 
  margin:auto; 
  border: 1px dashed grey; 
  padding:10px;  
}  

img 
{  
  margin:5px; 
  border: 1px solid black; 

}  
</style> 
</head> 
<body> 
<center><h1> #Album title </h1></center> 
<p> 
EOF1 

for img in *.jpg; 
do  
  convert "$img" -resize "100x" "thumbs/$img" 
  echo "<a href=\"$img\" >" >>index.html 
  echo "<img src=\"thumbs/$img\" title=\"$img\" /></a>" >> index.html 
done 

cat <<EOF2 >> index.html 

</p> 
</body> 
</html> 
EOF2  

echo Album generated to index.html 

```

按如下方式运行脚本:

```
$ ./generate_album.sh
Creating album..
Album generated to index.html

```

# 它是如何工作的...

脚本的初始部分用于编写 HTML 页面的标题部分。

以下脚本将直到`EOF1`的所有内容重定向到`index.html`:

```
cat <<EOF1 > index.html 
contents... 
EOF1 

```

标题包括 HTML 和 CSS 样式。

`for img in *.jpg *.JPG;`迭代文件名并计算循环体。

`convert "$img" -resize "100x" "thumbs/$img"`创建 100 像素宽的缩略图。

以下语句生成所需的`<img>`标记并将其附加到`index.html`中:

```
echo "<a href=\"$img\" >" 
echo "<img src=\"thumbs/$img\" title=\"$img\" /></a>" >> index.html 

```

最后，如脚本的第一部分一样，页脚 HTML 标记被附加了`cat`。

# 请参见

*   本章中的*网络相册生成器*配方解释了`EOF`和`stdin`重定向

# Twitter 命令行客户端

**Twitter** 是最热门的微博平台，也是现在网络社交媒体的最新流行语。我们可以使用 Twitter API 从命令行读取我们时间轴上的推文！

让我们看看怎么做。

# 准备好了

最近，推特停止允许人们使用普通的 HTTP 身份验证登录，所以我们必须使用 OAuth 来验证自己。关于 OAuth 的完整解释不在本书的讨论范围之内，因此我们将使用一个库，它使得从 Bash 脚本中使用 OAuth 变得很容易。请执行以下步骤:

1.  从[https://github.com/livibetter/bash-oauth/archive/master.zip](https://github.com/livibetter/bash-oauth/archive/master.zip)下载`bash-oauth`库，解压到任意目录。
2.  转到该目录，然后在子目录`bash-oauth-master`中，以 root 用户身份运行`make install-all`。
3.  去[https://apps.twitter.com/](https://apps.twitter.com/)注册一个新的应用。这将使使用 OAuth 成为可能。
4.  注册新应用程序后，转到应用程序的设置，并将访问类型更改为读写。
5.  现在，转到应用程序的详细信息部分，注意两件事，消费者密钥和消费者秘密，这样您就可以在我们将要编写的脚本中替换它们。

太好了，现在让我们编写使用这个的脚本。

# 怎么做...

这个 Bash 脚本使用 OAuth 库来读取推文或发送您自己的更新:

```
#!/bin/bash 
#Filename: twitter.sh 
#Description: Basic twitter client 

oauth_consumer_key=YOUR_CONSUMER_KEY 
oauth_consumer_scret=YOUR_CONSUMER_SECRET 

config_file=~/.$oauth_consumer_key-$oauth_consumer_secret-rc  

if [[ "$1" != "read" ]] && [[ "$1" != "tweet" ]]; 
then  
  echo -e "Usage: $0 tweet status_message\n   OR\n      $0 read\n" 
  exit -1; 
fi 

#source /usr/local/bin/TwitterOAuth.sh 
source bash-oauth-master/TwitterOAuth.sh 
TO_init 

if [ ! -e $config_file ]; then 
 TO_access_token_helper 
 if (( $? == 0 )); then 
   echo oauth_token=${TO_ret[0]} > $config_file 
   echo oauth_token_secret=${TO_ret[1]} >> $config_file 
 fi 
fi 

source $config_file 

if [[ "$1" = "read" ]]; 
then 
TO_statuses_home_timeline '' 'YOUR_TWEET_NAME' '10' 
  echo $TO_ret |  sed  's/,"/\n/g' | sed 's/":/~/' | \ 
    awk -F~ '{} \ 
      {if ($1 == "text") \ 
        {txt=$2;} \ 
       else if ($1 == "screen_name") \ 
        printf("From: %s\n Tweet: %s\n\n", $2, txt);} \ 
      {}' | tr '"' ' ' 

elif [[ "$1" = "tweet" ]]; 
then  
  shift 
  TO_statuses_update '' "$@" 
  echo 'Tweeted :)' 
fi 

```

按如下方式运行脚本:

```
$./twitter.sh read
Please go to the following link to get the PIN:   
https://api.twitter.com/oauth/authorize?
oauth_token=LONG_TOKEN_STRING
PIN: PIN_FROM_WEBSITE
Now you can create, edit and present Slides offline.
- by A Googler 
$./twitter.sh tweet "I am reading Packt Shell Scripting Cookbook"
Tweeted :)
$./twitter.sh read | head -2
From: Clif Flynt
Tweet: I am reading Packt Shell Scripting Cookbook 

```

# 它是如何工作的...

首先，我们使用 source 命令来包含`TwitterOAuth.sh`库，这样我们就可以使用它的函数来访问 Twitter。`TO_init`功能初始化库。

每个应用程序都需要在第一次使用时获得一个 OAuth 令牌和令牌秘密。如果这些不存在，我们使用`TO_access_token_helper`库函数来获取它们。一旦我们有了令牌，我们就将它们保存到一个`config`文件中，这样我们就可以在下一次运行脚本时简单地获取它。

`TO_statuses_home_timeline`库函数从推特上获取推文。该数据以 JSON 格式作为单个长字符串返回，如下所示:

```
[{"created_at":"Thu Nov 10 14:45:20 +0000    
"016","id":7...9,"id_str":"7...9","text":"Dining...

```

每条推文都以`"created_at"`标签开头，包括一个`text`和一个`screen_name`标签。该脚本将提取文本和屏幕名称数据，并只显示这些字段。

脚本将长字符串分配给`TO_ret`变量。

JSON 格式使用带引号的字符串作为键，并且可以给值加引号，也可以不加引号。键/值对用逗号分隔，键和值用冒号(`:`)分隔。

第一个`sed`用换行符替换每个`"`字符集，使每个键/值成为单独的一行。这些行通过管道连接到另一个`sed`命令，用波浪号(~)替换每个出现的`":`，这样就创建了一行:

```
screen_name~"Clif_Flynt" 

```

最后的`awk`脚本读每一行。`-F~`选项将行分割成波浪号处的字段，因此`$1`是关键，`$2`是值。`if`命令检查`text`或`screen_name`。文本首先在推文中，但如果我们先报告发件人，会更容易阅读；所以脚本保存一个`text`返回，直到看到一个`screen_name`，然后打印`$2`的当前值和文本的保存值。

`TO_statuses_update`库函数生成推文。空的第一个参数将我们的消息定义为默认格式，消息是第二个参数的一部分。

# 请参见

*   使用 sed 执行文本替换的*[第 4 章](04.html)、*短信和驾驶*中的*食谱解释了`sed`命令
*   [第 4 章](04.html)、*发短信和开车*中的*使用 grep* 配方搜索和挖掘文件中的文本，解释了`grep`命令

# 通过网络服务器访问单词定义

网络上的一些词典提供了一个通过脚本与网站交互的应用编程接口。这个食谱演示了如何使用一个受欢迎的。

# 准备好

我们将使用`curl`、`sed`和`grep`来定义效用。有很多字典网站可以免费注册和使用它们的 API 供个人使用。在这个例子中，我们使用的是韦氏词典的字典 API。请执行以下步骤:

1.  去[http://www.dictionaryapi.com/register/index.htm](http://www.dictionaryapi.com/register/index.htm)，给自己注册一个账号。选择大学词典和学习词典:
2.  使用新创建的帐户登录，并转到我的密钥以访问密钥。记下学习词典的钥匙。

# 怎么做...

该脚本将显示单词定义:

```
#!/bin/bash 
#Filename: define.sh 
#Desc: A script to fetch definitions from dictionaryapi.com 

key=YOUR_API_KEY_HERE 

if  [ $# -ne 2 ]; 
then 
  echo -e "Usage: $0 WORD NUMBER" 
  exit -1; 
fi 

curl --silent \
http://www.dictionaryapi.com/api/v1/references/learners/xml/$1?key=$key | \ 
  grep -o \<dt\>.*\</dt\> | \ 
  sed 's$</*[a-z]*>$$g' | \ 
  head -n $2 | nl 

```

像这样运行脚本:

```
    $ ./define.sh usb 1
 1  :a system for connecting a computer to another device (such as   
    a printer, keyboard, or mouse) by using a special kind of cord a   
    USB cable/port USB is an abbreviation of "Universal Serial Bus."How 
    it works...

```

# 它是如何工作的...

我们使用`curl`通过指定我们的 API `Key ($apikey)`和我们想要定义的单词(`$1`)从字典 API 网页中获取数据。结果包含`<dt>`标签中的定义，用`grep`选择。`sed`命令删除标签。脚本从定义中选择所需的行数，并使用`nl`为每行添加行号。

# 请参见

*   第 4 章中的*使用 sed 执行文本替换*配方解释了`sed`命令
*   [第 4 章](04.html)、*发短信和开车*中的*使用 grep* 配方搜索和挖掘文件中的文本，解释了`grep`命令

# 在网站中查找断开的链接

网站必须测试断开的链接。对于大型网站，手动这样做是不可行的。幸运的是，这是一个容易自动化的任务。我们可以通过 HTTP 操作工具找到断开的链接。

# 准备好

我们可以使用`lynx`和`curl`来识别链接并找到断开的链接。Lynx 有`-traversal`选项，可以递归访问网站上的页面，并建立所有超链接的列表。cURL 用于验证每个链接。

# 怎么做...

这个脚本使用`lynx`和`curl`来查找网页上断开的链接:

```
#!/bin/bash  
#Filename: find_broken.sh 
#Desc: Find broken links in a website 

if [ $# -ne 1 ];  
then  
  echo -e "$Usage: $0 URL\n"  
  exit 1;  
fi  

echo Broken links:  

mkdir /tmp/$$.lynx  
cd /tmp/$$.lynx  

lynx -traversal $1 > /dev/null  
count=0;  

sort -u reject.dat > links.txt  

while read link;  
do  
  output=`curl -I $link -s \ 
| grep -e "HTTP/.*OK" -e "HTTP/.*200"` 
  if [[ -z $output ]];  
  then  
    output=`curl -I $link -s | grep -e "HTTP/.*301"` 
    if [[ -z $output ]];  
      then  
      echo "BROKEN: $link" 
      let count++  
    else  
      echo "MOVED: $link" 
    fi 
  fi  
done < links.txt  

[ $count -eq 0 ] && echo No broken links found. 

```

# 它是如何工作的...

`lynx -traversal URL`会在工作目录中产生多个文件。它包括一个`reject.dat`文件，该文件将包含网站中的所有链接。`sort -u`用于通过避免重复来建立列表。然后，我们遍历每个链接，并使用`curl -I`检查标题响应。如果标题的第一行包含 HTTP/和`OK`或`200`，则表示链接有效。如果链接无效，则重新检查并测试`301` - *链接移动*-回复。如果测试也失败了，断开的链接会显示在屏幕上。

From its name, it might seem like `reject.dat` should contain a list of URLs that were broken or unreachable. However, this is not the case, and lynx just adds all the URLs there.
Also note that `lynx` generates a file called `traverse.errors`, which contains all the URLs that had problems in browsing. However, `lynx` will only add URLs that return `HTTP 404 (not found)`, and so we will lose other errors (for instance, `HTTP 403 Forbidden`). This is why we manually check for statuses.

# 请参见

*   本章中的*以纯文本方式下载网页*食谱解释了`lynx`命令
*   本章中的*cURL*食谱入门解释了`curl`命令

# 跟踪网站的更改

跟踪网站变化对网络开发者和用户都很有用。手动检查网站是不切实际的，但是变更跟踪脚本可以定期运行。当发生更改时，它会生成通知。

# 准备好

在 Bash 脚本方面跟踪变化意味着在不同的时间抓取网站，并使用`diff`命令获取差异。我们可以用`curl`和`diff`来做这个。

# 怎么做...

这个 Bash 脚本结合了不同的命令来跟踪网页中的变化:

```
#!/bin/bash 
#Filename: change_track.sh 
#Desc: Script to track changes to webpage 

if [ $# -ne 1 ]; 
then  
  echo -e "$Usage: $0 URL\n" 
  exit 1; 
fi 

first_time=0 
# Not first time 

if [ ! -e "last.html" ]; 
then 
  first_time=1 
  # Set it is first time run 
fi 

curl --silent $1 -o recent.html 

if [ $first_time -ne 1 ]; 
then 
  changes=$(diff -u last.html recent.html) 
  if [ -n "$changes" ]; 
  then 
    echo -e "Changes:\n" 
    echo "$changes" 
  else 
    echo -e "\nWebsite has no changes" 
  fi 
else 
  echo "[First run] Archiving.." 

fi 

cp recent.html last.html 

```

让我们看看`track_changes.sh`脚本在你控制的网站上的输出。首先，我们将看到网页不变时的输出，然后进行更改后的输出。

请注意，您应该将`MyWebSite.org`更改为您的网站名称。

*   首先，运行以下命令:

```
        $ ./track_changes.sh http://www.MyWebSite.org
 [First run] Archiving..

```

*   其次，再次运行命令:

```
        $ ./track_changes.sh http://www.MyWebSite.org
 Website has no changes 

```

*   第三，在对网页进行更改后，运行以下命令:

```
        $ ./track_changes.sh http://www.MyWebSite.org

 Changes: 

 --- last.html    2010-08-01 07:29:15.000000000 +0200 
 +++ recent.html    2010-08-01 07:29:43.000000000 +0200 
 @@ -1,3 +1,4 @@ 
 <html>
 +added line :)
 <p>data</p>
 </html>

```

# 它是如何工作的...

脚本使用`[ ! -e "last.html" ];`检查脚本是否是第一次运行。如果`last.html`不存在，说明是第一次，网页必须下载保存为`last.html`。

如果不是第一次，则下载新副本(`recent.html`)并用 diff 实用程序检查差异。任何更改都将显示为差异输出。最后`recent.html`被复制到`last.html`。

请注意，更改您正在检查的网站将在您第一次检查时生成一个巨大的差异文件。如果你需要跟踪多个页面，你可以为你想看的每个网站创建一个文件夹。

# 请参见

*   本章中的*cURL*食谱入门解释了`curl`命令

# 发布到网页并阅读回复

`POST`和`GET`是 HTTP 中向网站发送信息或从网站检索信息的两种请求。在`GET`请求中，我们通过网页 URL 本身发送参数(名称-值对)。开机自检命令将键/值对放在消息正文中，而不是网址中。`POST`常用于提交长表格或隐藏提交的信息。

# 准备好

对于本食谱，我们将使用包含在 **tclhttpd** 包中的样本`guestbook`网站。您可以从[http://sourceforge.net/projects/tclhttpd](http://sourceforge.net/projects/tclhttpd)下载 tclhttpd，然后在本地系统上运行它来创建本地 web 服务器。留言簿页面需要一个名字和网址，当用户点击将我添加到你的留言簿按钮时，它会将这个名字和网址添加到留言簿中，以显示谁访问过某个网站。

这个过程可以通过单个`curl`(或`wget`)命令实现自动化。

# 怎么做...

将 tclhttpd 包和`cd`下载到`bin`文件夹。使用以下命令启动 tclhttpd 守护程序:

```
    tclsh httpd.tcl

```

从通用网站发布和读取 HTML 响应的格式类似于这样:

```
    $ curl URL -d "postvar=postdata2&postvar2=postdata2"

```

考虑以下示例:

```
    $ curl http://127.0.0.1:8015/guestbook/newguest.html \
 -d "name=Clif&url=www.noucorp.com&http=www.noucorp.com"

```

curl 命令打印如下响应页面:

```
<HTML> 
<Head> 
<title>Guestbook Registration Confirmed</title> 
</Head> 
<Body BGCOLOR=white TEXT=black> 
<a href="www.noucorp.com">www.noucorp.com</a> 

<DL> 
<DT>Name 
<DD>Clif 
<DT>URL 
<DD> 
</DL> 
www.noucorp.com 

</Body> 

```

`-d`是用于过账的参数。`-d`的字符串参数类似于`GET`请求语义。`var=value`对由`&`划定。

您可以使用`--post-data "string"`使用`wget`发布数据。考虑以下示例:

```
    $ wget http://127.0.0.1:8015/guestbook/newguest.cgi \
 --post-data "name=Clif&url=www.noucorp.com&http=www.noucorp.com" \
 -O output.html

```

对名称-值对使用与 cURL 相同的格式。output.html 的文本与 cURL 命令返回的文本相同。

The string to the post arguments (for example, to `-d` or `--post-data`) should always be given in quotes. If quotes are not used, `&` is interpreted by the shell to indicate that this should be a background process.

如果您查看网站源(使用网络浏览器中的“查看源”选项)，您将看到一个已定义的 HTML 表单，类似于以下代码:

```
<form action="newguest.cgi" " method="post" > 
<ul> 
<li> Name: <input type="text" name="name" size="40" > 
<li> Url: <input type="text" name="url" size="40" > 
<input type="submit" > 
</ul> 
</form> 

```

这里，`newguest.cgi`是目标 URL。当用户输入详细信息并点击提交按钮时，名称和网址输入作为`POST`请求发送到`newguest.cgi`，响应页面返回浏览器。

# 请参见

*   本章中的*cURL*食谱入门解释了`curl`命令
*   本章中的*从网页下载*食谱解释了`wget`命令

# 从互联网下载视频

下载视频的原因有很多。如果您使用的是计量服务，您可能希望在收费较低的非工作时间下载视频。你可能想看带宽不支持流式传输的视频，或者你可能只想确保你总是有可爱的猫的视频给你的朋友看。

# 准备好了

下载视频的一个程序是`youtube-dl`。这不包括在大多数发行版中，并且存储库可能不是最新的，所以最好去位于[http://yt-dl.org](http://yt-dl.org)的`youtube-dl`主站点。

你会在那个页面上找到下载和安装`youtube-dl`的链接和信息。

# 怎么做...

使用`youtube-dl`很容易。打开浏览器，找到自己喜欢的视频。然后将该网址复制/粘贴到`youtube-dl`命令行:

```
    youtube-dl https://www.youtube.com/watch?v=AJrsl3fHQ74

```

当`youtube-dl`正在下载文件时，它将在您的终端上生成一个状态行。

# 它是如何工作的...

`youtube-dl`程序通过向服务器发送`GET`消息来工作，就像浏览器一样。它伪装成浏览器，这样 YouTube 或其他视频提供商就会下载一段视频，就好像设备正在流式传输一样。

`-list-formats` ( `-F`)选项将列出视频可用的格式，而`-format` ( `-f`)选项将指定下载的格式。如果您想要下载的视频分辨率高于互联网连接能够可靠传输的分辨率，这将非常有用。

# 与 OTS 一起总结文本

**打开文本摘要器** ( **OTS** )是一个应用程序，它从一段文本中去除多余的内容，创建一个简洁的摘要。

# 准备好

`ots`包不是大多数 Linux 标准发行版的一部分，但是可以通过以下命令安装:

```
    apt-get install libots-devel

```

# 怎么做...

`OTS`应用程序很容易使用。它从文件或`stdin`中读取文本，并生成到`stdout`的摘要。

```
    ots LongFile.txt | less

```

或者

```
    cat LongFile.txt | ots | less

```

`OTS`应用也可以和`curl`一起使用，汇总网站的信息。例如，你可以用`ots`来概括长篇大论的博客:

```
    curl http://BlogSite.org | sed -r 's/<[^>]+>//g' | ots | less

```

# 它是如何工作的...

`curl`命令从博客站点检索页面并将页面传递给`sed`。`sed`命令使用一个正则表达式来替换所有的 HTML 标签，一个以小于符号开始，以大于符号结束的字符串，用一个空格代替。剥离的文本被传递给`ots`，T3 生成一个显示较少的摘要。

# 从命令行翻译文本

谷歌提供在线翻译服务，你可以通过浏览器访问。Andrei Neculau 创建了一个 **awk** 脚本，该脚本将访问该服务并从命令行进行翻译。

# 准备好了

大多数 Linux 发行版都不包含命令行翻译器，但是可以直接从 Git 安装，如下所示:

```
    cd ~/bin
 wget git.io/trans 
 chmod 755 ./trans

```

# 怎么做...

默认情况下，`trans`应用程序将翻译成您的区域环境变量中的语言。

```
    $> trans "J'adore Linux"

 J'adore Linux

 I love Linux

 Translations of J'adore Linux
 French -> English

 J'adore Linux 
 I love Linux

```

您可以使用文本前的选项来控制要翻译的语言。选项的格式如下:

```
    from:to

```

要从英语翻译成法语，请使用以下命令:

```
    $> trans en:fr "I love Linux" 
 J'aime Linux

```

# 它是如何工作的...

`trans`程序是大约 5000 行 awk 代码，使用`curl`与谷歌、必应和 Yandex 翻译服务进行通信。