# 二、编写良好的命令

在本章中，我们将介绍以下食谱:

*   与`cat`相连
*   录制和播放终端会话
*   查找文件和文件列表
*   玩`xargs`
*   用`tr`翻译
*   校验和和验证
*   加密工具和哈希
*   对唯一行和重复行进行排序
*   临时文件命名和随机数
*   拆分文件和数据
*   基于扩展名分割文件名
*   批量重命名和移动文件
*   拼写检查和词典操作
*   自动交互输入
*   通过运行并行进程使命令更快
*   检查目录、其中的文件和子目录

# 介绍

类似 Unix 的系统拥有最好的命令行工具。每个命令都执行一个简单的功能，使我们的工作更容易。这些简单的功能可以与其他命令相结合来解决复杂的问题。组合简单的命令是一门艺术；随着你练习和获得经验，你会在这方面做得更好。本章介绍一些最有趣、最有用的命令，包括`grep`、`awk`、`sed`和`find`。

# 与猫连接

`cat`命令显示或连接文件的内容，但`cat`功能更多。例如，`cat`可以将标准输入数据与文件中的数据相结合。将`stdin`数据与文件数据相结合的一种方法是将`stdin`重定向到一个文件，然后追加两个文件。`cat`命令可以在一次调用中做到这一点。接下来的食谱展示了`cat`的基本用法和高级用法。

# 怎么做...

`cat`命令是一个简单而常用的命令，它代表**连接**。

`cat`读取内容的一般语法如下:

```sh
$ cat file1 file2 file3 ...

```

该命令将指定为命令行参数的文件中的数据连接起来，并将该数据发送到`stdout`。

*   要打印单个文件的内容，请执行以下命令:

```sh
        $ cat file.txt
 This is a line inside file.txt
 This is the second line inside file.txt

```

*   要打印多个文件的内容，请执行以下命令:

```sh
        $ cat one.txt two.txt 
 This line is from one.txt
 This line is from two.txt

```

`cat`命令不仅读取文件和连接数据，还读取标准输入。

管道操作员将数据重定向到 cat 命令的标准输入，如下所示:

```sh
OUTPUT_FROM_SOME COMMANDS | cat

```

`cat`命令还可以将文件内容与终端输入连接起来。

将`stdin`和另一个文件中的数据合并，如下所示:

```sh
$ echo 'Text through stdin' | cat - file.txt

```

在本例中，`-`充当`stdin`文本的文件名。

# 还有更多...

`cat`命令还有许多其他查看文件的选项。您可以通过在终端会话中键入`man cat`来查看完整列表。

# 去掉多余的空行

有些文本文件包含两行或多行空行。如果需要删除多余的空行，请使用以下语法:

```sh
$ cat -s file

```

考虑以下示例:

```sh
$ cat multi_blanks.txt
line 1

line 2

line 3

line 4

$ cat -s multi_blanks.txt # Squeeze adjacent blank lines
line 1

line 2

line 3

line 4

```

我们可以用`tr`删除所有空白行，正如本章中*用*配方翻译所讨论的。

# 将选项卡显示为^I

很难区分制表符和重复的空格字符。Python 等语言对制表符和空格的处理可能会有所不同。制表符和空格的混合在编辑器中可能看起来相似，但在解释器中显示为不同的缩进。在文本编辑器中查看文件时，很难识别制表符和空格之间的区别。`cat`也可以识别标签页。这有助于您调试缩进错误。

`cat`命令的`-T`选项将制表符显示为`^I`:

```sh
$ cat file.py 
def function(): 
    var = 5 
        next = 6 
    third = 7 

$ cat -T file.py 
def function():
^Ivar = 5
^I^Inext = 6
^Ithird = 7^I

```

# 行号

cat 命令的`-n`标志为每一行加上一个行号的前缀。考虑以下示例:

```sh
$ cat lines.txt
line 
line
line

$ cat -n lines.txt
 1 line
 2 line
 3 line

```

The `cat` command never changes a file. It sends output to `stdout` after modifying the input according to the options. Do not attempt to use redirection to overwrite your input file. The shell creates the new output file before it opens the input file. The `cat` command will not let you use the same file as input and redirected output. Trying to trick `cat` with a pipe and redirecting the output will empty the input file.

```sh
$> echo "This will vanish" > myfile
$> cat -n myfile >myfile
cat: myfile: input file is output file
$> cat myfile | cat -n >myfile
$> ls -l myfile
-rw-rw-rw-. 1 user user 0 Aug 24 00:14 myfile   ;# myfile has 0
bytes

```

The `-n` option generates line numbers for all lines, including blank lines. If you want to skip numbering blank lines, use the `-b` option.

# 录制和播放终端会话

将屏幕会话录制为视频很有用，但是视频对于调试终端会话或提供 shell 教程来说是一种过度的手段。

Shell 提供了另一种选择。`script`命令记录您的击键和击键的时间，并将您的输入和结果输出保存在一对文件中。`scriptreplay`命令将重放会话。

# 准备好

大多数 GNU/Linux 发行版中都有`script`和`scriptreplay`命令。您可以通过记录终端会话来创建命令行技巧教程。您还可以共享录制的文件供他人回放，并查看如何使用命令行执行特定任务。您甚至可以调用其他解释器并记录发送给该解释器的击键。您不能录制 vi、emacs 或其他将字符映射到屏幕上特定位置的应用。

# 怎么做...

使用以下命令开始记录终端会话:

```sh
$ script -t 2> timing.log -a output.session

```

完整的示例如下所示:

```sh
$ script -t 2> timing.log -a output.session

# This is a demonstration of tclsh
$ tclsh
% puts [expr 2 + 2]
4
% exit
$ exit

```

Note that this recipe will not work with shells that do not support redirecting only `stderr` to a file, such as the `csh` shell.

`script`命令接受文件名作为参数。这个文件将保存击键和命令结果。当您使用`-t`选项时，脚本命令将定时数据发送到`stdout`。计时数据可以被重定向到一个文件(`timing.log`)，该文件记录每次按键和输出的计时信息。前面的例子使用了`2>`将`stderr`重定向到`timing.log`。

使用两个文件`timing.log`和`output.session`，我们可以重放命令执行的顺序如下:

```sh
$ scriptreplay timing.log output.session
# Plays the sequence of commands and output

```

# 它是如何工作的...

我们经常录制桌面视频来准备教程。然而，视频需要相当大的存储量，而终端脚本文件只是一个文本文件，通常只有千字节的数量级。

您可以将`timing.log`和`output.session`文件共享给任何想要在终端中重放终端会话的人。

# 查找文件和文件列表

`find`命令是 Unix/Linux 命令行工具箱中最棒的实用程序之一。它在命令行和 shell 脚本中都很有用。像`cat`和`ls`一样，`find`有很多功能，大部分人都没有发挥到极致。本食谱介绍了一些利用`find`定位文件的常用方法。

# 准备好了

`find`命令使用以下策略:`find`在文件的层次结构中下降，匹配满足指定标准的文件，并执行一些操作。默认操作是打印文件和文件夹的名称，可以用`-print`选项指定。

# 怎么做...

要列出从给定目录降序排列的所有文件和文件夹，请使用以下语法:

```sh
$ find base_path

```

`base_path`可以是`find`开始下降的任何位置(例如`/home/slynux/`)。

下面是这个命令的一个例子:

```sh
$ find . -print
.history
Downloads
Downloads/tcl.fossil
Downloads/chapter2.doc
…

```

`.`指定当前目录，`..`指定父目录。整个 Unix 文件系统都遵循这个惯例。

打印选项用`\n`(换行符)分隔每个文件或文件夹名称。`-print0`选项用空字符`'\0'`分隔每个名称。`-print0`的主要用途是将包含换行符或空白字符的文件名传递给`xargs`命令。`xargs`命令将在后面详细讨论:

```sh
$> echo "test" > "file name"
$> find . -type f -print | xargs ls -l
ls: cannot access ./file: No such file or directory
ls: cannot access name: No such file or directory
$> find . -type f -print0 | xargs -0 ls -l
-rw-rw-rw-. 1 user group 5  Aug 24 15:00 ./file name

```

# 还有更多...

前面的例子演示了使用`find`列出文件系统层次结构中的所有文件和文件夹。`find`命令可以根据全局或正则表达式规则、文件系统树的深度、日期、文件类型等选择文件。

# 基于名称或正则表达式匹配进行搜索

`-name`参数指定名称的选择模式。`-name`参数接受 glob 风格的通配符和正则表达式。在以下示例中，`'*.txt'`匹配所有以`.txt`结尾的文件或文件夹名称并打印它们。

Note the single quotes around `*.txt`. The shell will expand glob wildcards with no quotes or using double-quotes (`"`). The single quotes prevent the shell from expanding the `*.txt` and passes that string to the `find` command.

```sh
$ find /home/slynux -name '*.txt' -print

```

`find`命令有一个选项`-iname`(忽略大小写)，类似于`-name`，但是它匹配文件名而不管大小写。

考虑以下示例:

```sh
$ ls
example.txt  EXAMPLE.txt  file.txt
$ find . -iname "example*" -print
./example.txt
./EXAMPLE.txt

```

`find`命令支持带有选择选项的逻辑操作。`-a`和`-and`选项执行逻辑**和**，而`-o`和`-or`选项执行逻辑**或**。

```sh
$ ls
new.txt  some.jpg  text.pdf   stuff.png
$ find . \( -name '*.txt' -o -name '*.pdf' \) -print
./text.pdf
./new.txt

```

之前的命令将打印所有的`.txt`和`.pdf`文件，因为`find`命令同时匹配`.txt`和`.pdf`文件。`\(`和`\)`用于将`-name "*.txt" -o -name "*.pdf"`作为一个单元来处理。

下面的命令演示了如何使用`-and`操作符只选择以`s`开头并且名称中有`e`的文件。

```sh
$ find . \( -name '*e*' -and -name 's*' \) 
./some.jpg

```

`-path`参数将匹配限制为匹配路径和名称的文件。比如`$ find /home/users -path '*/slynux/*' -name '*.txt' -print`会找`/home/users/slynux/readme.txt`，但不会找`/home/users/slynux.txt`。

The `-regex` argument is similar to `-path`, but `-regex` matches the file paths based on regular expressions.

正则表达式比 glob 通配符更复杂，并且支持更精确的模式匹配。文本与正则表达式匹配的一个典型例子是识别所有电子邮件地址。电子邮件地址采用`name@host.root`形式。可以概括为`[a-z0-9]+@[a-z0-9]+\.[a-z0-9]+`。方括号内的字符代表一组字符。在这种情况下，`a-z`和`0-9``+`符号表示前一类字符可以出现一次或多次。句点是一个单字符通配符(就像 glob 通配符中的`?`)，因此必须使用反斜杠对其进行转义，以匹配电子邮件地址中的实际点。所以，这个正则表达式翻译成‘一个字母或数字序列，后跟一个`@`，后跟一个字母或数字序列，后跟一个句点，最后是一个字母或数字序列’。详见[第四章](04.html)、*发短信和开车*中的*使用正则表达式*配方。

该命令匹配`.py`或`.sh`文件:

```sh
$ ls
new.PY  next.jpg  test.py script.sh $ find . -regex '.*\.(py\|sh\)$'
./test.py
script.sh

```

`-iregex`选项忽略正则表达式匹配的大小写。

考虑这个例子:

```sh
$ find . -iregex '.*\(\.py\|\.sh\)$'
./test.py
./new.PY
./script.sh

```

# 否定论点

`find`命令也可以使用`!`排除符合模式的事物:

```sh
$ find . ! -name "*.txt" -print

```

这将匹配所有名称不以`.txt`结尾的文件。以下示例显示了该命令的结果:

```sh
$ ls
list.txt  new.PY  new.txt  next.jpg  test.py

$ find . ! -name "*.txt" -print
.
./next.jpg
./test.py
./new.PY

```

# 基于目录深度进行搜索

`find`命令遍历所有子目录，直到到达每个子目录树的底部。默认情况下，`find`命令不会跟随符号链接。`-L`选项将迫使它遵循符号链接。如果某个链接引用了指向原始链接的链接，`find`将陷入循环。

`-maxdepth`和`-mindepth`参数限制了`find`命令将穿越的距离。这将使`find`命令从原本无限的搜索中中断。

`/proc`文件系统包含关于您的系统和正在运行的任务的信息。任务的文件夹层次结构相当深，并且包括可以循环的符号链接。运行系统的每个进程在`proc`中都有一个条目，以进程标识命名。每个进程标识下都有一个名为`cwd`的文件夹，它是该任务当前工作目录的链接。

下面的例子展示了如何列出在一个名为`bundlemaker.def`的文件夹中运行的所有任务:

```sh
$ find -L /proc -maxdepth 3 -name 'bundlemaker.def' 2>/dev/null

```

*   `-L`选项告诉`find`命令跟随符号链接
*   `/proc`是开始搜索的文件夹
*   `-maxdepth 3`选项将搜索限制在当前文件夹，而不是子文件夹
*   `-name 'bundlemaker.def'`选项是要搜索的文件
*   `2>/dev/null`将关于递归循环的错误消息重定向到空设备

`-mindepth`选项类似于`-maxdepth`，但它设置了`find`报告匹配的最小深度。它可用于查找和打印距离基本路径最小深度的文件。例如，要打印名称以`f`开头且距离当前目录至少两个子目录的所有文件，请使用以下命令:

```sh
$ find . -mindepth 2 -name "f*" -print
./dir1/dir2/file1
./dir3/dir4/f2

```

当前目录下或`dir1`和`dir3`中以`f`开头的文件将不会被打印。

The `-maxdepth` and `-mindepth` option should be early in the `find` command. If they are specified as later arguments, it may affect the efficiency of `find` as it has to do unnecessary checks. For example, if `-maxdepth` is specified after a `-type` argument, the `find` command will first find the files having the specified `-type` and then filter out the files that don't match the proper depth. However, if the depth was specified before the `-type`, `find` will collect the files having the specified depth and then check for the file type, which is the most efficient way to search.

# 基于文件类型搜索

类似 Unix 的操作系统将每个对象都视为一个文件。有不同种类的文件，如常规文件、目录、字符设备、块设备、符号链接、硬链接、套接字、先进先出等等。

`find`命令使用`-type`选项过滤文件搜索。使用`-type`，我们可以告诉`find`命令只匹配指定类型的文件。

仅列出包括后代的目录:

```sh
$ find . -type d -print

```

很难分别列出目录和文件。但是`find`有助于做到。仅列出常规文件如下:

```sh
$ find . -type f -print

```

仅列出如下符号链接:

```sh
$ find . -type l -print

```

下表显示了`find`识别的类型和参数:

| **文件类型** | **类型参数** |
| 常规文件 | `f` |
| 符号链接 | `l` |
| 目录 | `d` |
| 字符专用设备 | `c` |
| 闭塞装置 | `b` |
| 窝 | `s` |
| 先进先出。比较 LIFO | `p` |

# 按文件时间戳搜索

Unix/Linux 文件系统在每个文件上都有三种类型的时间戳。它们如下:

*   **访问时间** ( `-atime`):上次访问文件的时间戳
*   **修改时间** ( `-mtime`):上次修改文件的时间戳
*   **更改时间** ( `-ctime`):上次修改文件元数据(如权限或所有权)的时间戳

Unix does not store file creation time by default; however, some filesystems (`ufs2`, `ext4`, `zfs`, `btrfs`, `jfs`) save the creation time. The creation time can be accessed with the stat command.
Given that some applications modify a file by creating a new file and then deleting the original, the creation date may not be accurate.
The `-atime`, `-mtime`, and `-ctime` option are the time parameter options available with `find`. They can be specified with integer values in *number of days*. The number may be prefixed with `-` or `+` signs. The `-` sign implies less than, whereas the `+` sign implies greater than.

考虑以下示例:

*   打印过去七天内访问过的文件:

```sh
        $ find . -type f -atime -7 -print

```

*   打印访问时间正好为七天的文件:

```sh
        $ find . -type f -atime 7 -print

```

*   打印访问时间超过七天的文件:

```sh
        $ find . -type f -atime +7 -print

```

`-mtime`参数将根据修改时间搜索文件；`-ctime`根据变更时间进行搜索。

`-atime`、`-mtime`和`-ctime`使用以天为单位的时间。`find`命令还支持以分钟为单位的选项。这些措施如下:

*   `-amin`(访问时间)
*   `-mmin`(修改时间)
*   `-cmin`(变更时间)

要打印所有访问时间超过七分钟的文件，请使用以下命令:

```sh
$ find . -type f -amin +7 -print

```

`-newer`选项指定一个修改时间的参考文件，用于选择比参考文件更新的文件。

查找所有比`file.txt`文件更新的文件:

```sh
$ find . -type f -newer file.txt -print

```

`find`命令的时间戳标志对于编写备份和维护脚本非常有用。

# 基于文件大小的搜索

基于文件的文件大小，可以执行搜索:

```sh
# Files having size greater than 2 kilobytes
$ find . -type f -size +2k

# Files having size less than 2 kilobytes
$ find . -type f -size -2k

# Files having size 2 kilobytes
$ find . -type f -size 2k

```

代替`k`，我们可以使用这些不同大小的单位:

*   `b` : 512 字节块
*   `c`字节
*   `w`:双字节字
*   `k`:千字节(1，024 字节)
*   `M`:兆字节(1，024 千字节)
*   `G`:千兆字节(1024 兆字节)

# 基于文件权限和所有权的匹配

可以根据文件权限匹配文件。我们可以列出具有指定文件权限的文件:

```sh
$ find . -type f -perm 644 -print
# Print files having permission 644

```

`-perm`选项指定`find`应该只匹配权限设置为特定值的文件。权限在*处理文件权限、所有权、* *和[第 3 章](02.html)、*文件输入、文件输出*中的粘性位*配方中有更详细的解释。

作为一个示例使用案例，我们可以考虑 Apache web 服务器的案例。网络服务器中的 PHP 文件需要适当的权限才能执行。我们可以找到没有适当执行权限的 PHP 文件:

```sh
$ find . -type f -name "*.php" ! -perm 644 -print
PHP/custom.php
$ ls -l PHP/custom.php
-rw-rw-rw-.  root   root   513 Mar 13  2016  PHP/custom.php

```

我们还可以根据所有权搜索文件。可以通过`-user USER`选项找到特定用户拥有的文件。

`USER`参数可以是用户名或 UID。

例如，要打印`slynux`用户拥有的所有文件的列表，可以使用以下命令:

```sh
$ find . -type f -user slynux -print

```

# 使用查找对文件执行操作

find 命令可以对它识别的文件执行操作。您可以删除文件，或者对文件执行任意的 Linux 命令。

# 基于文件匹配删除

`find`命令的`-delete`标志删除匹配的文件，而不是显示它们。从当前目录中删除`.swp`文件:

```sh
$ find . -type f -name "*.swp" -delete

```

# 执行命令

使用`-exec`选项，可以将`find`命令与许多其他命令结合使用。

考虑前面的例子。我们使用`-perm`查找没有适当权限的文件。同样，如果我们需要将某个用户(例如`root`)拥有的所有文件的所有权更改为另一个用户(例如`www-data`，网络服务器中默认的 Apache 用户)，我们可以使用`-user`选项找到`root`拥有的所有文件，并使用`-exec`执行所有权更改操作。

You must run the `find` command as root if you want to change the ownership of files or directories.

`find`命令使用开/闭花括号对`{}`来表示文件名。在下一个例子中，每次`find`识别一个文件，它会用文件名替换`{}`并改变文件的所有权。例如，如果`find`命令找到两个拥有者为`root`的文件，它将改变这两个文件，使它们都归`slynux`所有:

```sh
# find . -type f -user root -exec chown slynux {} \;

```

Note that the command is terminated with `\;`. The semicolon must be escaped or it will be grabbed by your command shell as the end of the `find` command instead of the end of the `chown` command.

为每个文件调用一个命令是很大的开销。如果命令接受多个参数(如`chown`所做的)，您可以用加号(`+`)而不是分号来终止命令。加号使`find`列出所有匹配搜索参数的文件，并在一条命令行上用所有文件执行一次应用。

另一个用法示例是将给定目录中的所有 C 程序文件连接起来，并将它们写入单个文件，比如说`all_c_files.txt`。这些示例中的每一个都将执行此操作:

```sh
$ find . -type f -name '*.c' -exec cat {} \;>all_c_files.txt
$ find . -type f -name '*.c' -exec cat {} > all_c_files.txt \;
$ fine . -type f -name '*.c' -exec cat {} >all_c_files.txt +

```

为了将数据从`find`重定向到`all_c_files.txt`文件，我们使用了`>`操作符而不是`>>`(追加)，因为来自`find`命令的整个输出是单个数据流(`stdin`)；当要将多个数据流附加到单个文件时,`>>`是必要的。

以下命令将所有 10 天以上的`.txt`文件复制到一个目录`OLD`:

```sh
$ find . -type f -mtime +10 -name "*.txt" -exec cp {} OLD  \;

```

`find`命令可以与许多其他命令结合使用。

We cannot use multiple commands along with the `-exec` parameter. It accepts only a single command, but we can use a trick. Write multiple commands in a shell script (for example, `commands.sh`) and use it with `-exec` as follows:

```sh
-exec ./commands.sh {} \;

```

`-exec`参数可以与`printf`耦合产生`joutput`。考虑这个例子:

```sh
$ find . -type f -name "*.txt" -exec printf "Text file: %s\n" {} \;
Config file: /etc/openvpn/easy-rsa/openssl-1.0.0.cnf
Config file: /etc/my.cnf

```

# 使用 find 命令时跳过指定的目录

跳过某些子目录可以提高`find`运行时的性能。例如，当在版本控制系统(如`Git`)下的开发源代码树中搜索文件时，文件系统在存储版本控制相关信息的每个子目录中包含一个目录。这些目录可能不包含有用的文件，应该从搜索中排除。

排除文件和目录的技术被称为**修剪**。以下示例显示了如何使用`-prune`选项排除符合模式的文件。

```sh
$ find devel/source_path  -name '.git' -prune -o -type f -print

```

`-name ".git" -prune`是修剪部分，指定`.git`目录应该排除。`-type f -print`部分描述了要执行的操作。

# 玩 xargs

Unix 命令接受来自标准输入(`stdin`)或作为命令行参数的数据。前面的例子已经展示了如何用管道将数据从一个应用的标准输出传递到另一个应用的标准输入。

我们可以以其他方式调用接受命令行参数的应用。最简单的方法是使用 back-tic 符号运行命令，并将其输出用作命令行:

```sh
$ gcc `find '*.c'`

```

这个解决方案在很多情况下都可以正常工作，但是如果有很多文件需要处理，你会看到可怕的`Argument list too long`错误消息。`xargs`程序解决了这个问题。

`xargs`命令从`stdin`读取参数列表，并在命令行中使用这些参数执行命令。`xargs`命令还可以将任何单行或多行文本输入转换为其他格式，例如多行(指定列数)或单行，反之亦然。

# 准备好了

`xargs`命令应该是管道操作员之后出现的第一个命令。它使用标准输入作为主要数据源，并使用从`stdin`读取的值作为新命令的命令行参数来执行另一个命令。本示例将在一组 C 文件中搜索主字符串:

```sh
ls *.c | xargs grep main

```

# 怎么做...

`xargs`命令通过重新格式化通过`stdin`接收的数据，为目标命令提供参数。默认情况下，`xargs`将执行`echo`命令。在许多方面，`xargs`命令类似于`find`命令的`-exec`选项所执行的动作:

*   将多行输入转换为单行输出:

Xarg 的默认`echo`命令可用于将多行输入转换为单行文本，如下所示:

```sh
          $ cat example.txt # Example file
 1 2 3 4 5 6 
 7 8 9 10 
 11 12

 $ cat example.txt | xargs
 1 2 3 4 5 6 7 8 9 10 11 12

```

*   将单线输出转换为多线输出:

`xargs`的`-n`参数限制了每个命令行调用中的元素数量。该配方将输入分成多行 *N* 项，每一行:

```sh
          $ cat example.txt | xargs -n 3
 1 2 3 
 4 5 6 
 7 8 9 
 10 11 12

```

# 它是如何工作的...

`xargs`命令的工作原理是接受来自`stdin`的输入，将数据解析成单个元素，并调用以这些元素作为最终命令行参数的程序。默认情况下，`xargs`会根据空白分割输入并执行`/bin/echo`。

当文件名和文件夹名中有空格(甚至换行符)时，根据空白将输入拆分成元素就成了一个问题。`My Documents`文件夹将被解析成两个元素`My`和`Documents`，两者都不存在。

大多数问题都有解决方案，这也不例外。

我们可以定义用于分隔参数的分隔符。要为输入指定自定义分隔符，请使用`-d`选项:

```sh
$ echo "split1Xsplit2Xsplit3Xsplit4" | xargs -d X
split1 split2 split3 split4

```

在前面的代码中，`stdin`包含一个由多个`X`字符组成的字符串。我们用`-d option`定义`X`作为输入分隔符。

使用`-n`和前面的命令，我们可以将输入分成多行，每行两个单词，如下所示:

```sh
$ echo "splitXsplitXsplitXsplit" | xargs -d X -n 2
split split
split split

```

`xargs`命令与查找命令很好地结合在一起。find 的输出可以通过管道传输到`xargs`，以执行比`-exec`选项所能处理的更复杂的操作。如果文件系统的名称中有空格，find 命令的`-print0`选项将使用`0`(空)来分隔元素，这与`xargs -0`选项一起解析这些元素。以下示例在 Samba 安装的文件系统上搜索`.docx`文件，其中带有大写字母和空格的名称很常见。它使用`grep`报告带有图像的文件:

```sh
$ find /smbMount -iname '*.docx' -print0 | xargs -0 grep -L image

```

# 还有更多...

前面的例子展示了如何使用`xargs`来组织一组数据。接下来的示例显示了如何在命令行上格式化数据集。

# 通过读取 stdin 将格式化的参数传递给命令

这里有一个小的`echo`脚本，可以清楚地看到`xargs`是如何提供命令参数的:

```sh
#!/bin/bash 
#Filename: cecho.sh 

echo $*'#'  

```

当参数被传递到`cecho.sh` shell 时，它将打印以`#`字符结束的参数。考虑这个例子:

```sh
    $ ./cecho.sh arg1 arg2
 arg1 arg2 #

```

这里有一个常见的问题:

*   我有一个文件中的元素列表(每行一个)要提供给一个命令(比方说，`cecho.sh`)。我需要以几种方式应用这些参数。在第一个方法中，我需要每个调用一个参数，如下所示:

```sh
 ./cecho.sh arg1 
 ./cecho.sh arg2 
 ./cecho.sh arg3 

```

*   接下来，我需要为命令的每次执行提供一个或两个参数，如下所示:

```sh
 ./cecho.sh arg1 arg2 
 ./cecho.sh arg3 

```

*   最后，我需要一次性向命令提供所有参数:

```sh
 ./cecho.sh arg1 arg2 arg3 

```

运行`cecho.sh`脚本并记录输出，然后继续下一节。`xargs`命令可以格式化每个需求的参数。争论的清单在一份名为`args.txt`的文件中:

```sh
$ cat args.txt
arg1
arg2
arg3

```

对于第一种形式，我们多次执行该命令，每次执行一个参数。`xargs -n`选项可以将命令行参数的数量限制为一个:

```sh
$ cat args.txt | xargs -n 1 ./cecho.sh
arg1 #
arg2 #
arg3 #

```

要将参数数量限制为两个或更少，请执行以下操作:

```sh
$ cat args.txt | xargs -n 2 ./cecho.sh 
arg1 arg2 #
arg3 #

```

最后，要用所有参数同时执行命令，不要使用任何`-n`参数:

```sh
$ cat args.txt | xargs ./cecho.sh
arg1 arg2 arg3 #

```

在前面的例子中，`xargs`添加的参数放在命令的末尾。但是，我们可能需要在命令的末尾有一个常量短语，并希望`xargs`在中间替换它的参数，如下所示:

```sh
./cecho.sh -p arg1 -l

```

在前面的命令执行中，`arg1`是唯一的变量文本。所有其他的都应该保持不变。`args.txt`的论点应该这样应用:

```sh
./cecho.sh -p arg1 -l
./cecho.sh -p arg2 -l
./cecho.sh -p arg3 -l

```

`xargs-I`选项指定要用 xargs 从输入中解析的参数替换的替换字符串。当`-I`与`xargs`一起使用时，它将作为每个参数的一个命令执行来执行。这个例子解决了这个问题:

```sh
$ cat args.txt | xargs -I {} ./cecho.sh -p {} -l
-p arg1 -l #
-p arg2 -l #
-p arg3 -l #

```

`-I {}`指定替换字符串。对于为命令提供的每个参数，`{}`字符串将被替换为通过`stdin`读取的参数。

When used with `-I`, the command is executed in a loop. When there are three arguments, the command is executed three times along with the `{}` command. Each time, `{}` is replaced with arguments one by one.

# 将 xargs 与 find 一起使用

`xargs`和`find`命令可以组合执行任务。但是，请注意仔细组合它们。考虑这个例子:

```sh
$ find . -type f -name "*.txt"  -print | xargs rm -f 

```

这很危险。这可能会导致意外文件的删除。我们无法预测`find`命令输出的定界字符(无论是`'\n'`还是`' '`)。如果任何文件名包含空格字符(`' '` ) `xargs`可能会将其误解为分隔符。例如`bashrc text.txt`会被`xargs`误解为`bashrc`和`text.txt`。之前的命令不会删除`bashrc text.txt`，但会删除`bashrc`。

使用`find`的`-print0`选项产生由空字符(`'\0'`)分隔的输出；您使用`find`输出作为`xargs`输入。

该命令将`find`并删除所有`.txt`文件，不删除任何其他内容:

```sh
$ find . -type f -name "*.txt" -print0 | xargs -0 rm -f

```

# 计算源代码目录中 C 代码的行数

在某些时候，大多数程序员需要计算他们的 C 程序文件中的**行代码** ( **LOC** )这个任务的代码如下:

```sh
$ find source_code_dir_path -type f -name "*.c" -print0 | xargs -0 wc -l 

```

If you want more statistics about your source code, a utility called `SLOCCount`, is very useful. Modern GNU/Linux distributions usually have packages or you can get it from [http://www.dwheeler.com/sloccount/](http://www.dwheeler.com/sloccount/).

# 而与 stdin 的子壳技巧

`xargs`命令将参数放在命令的末尾；因此，`xargs`不能为多组命令提供参数。我们可以创建一个子 Shell 来处理复杂的情况。子 Shell 可以使用`while`循环来读取参数，并以更复杂的方式执行命令，如下所示:

```sh
$ cat files.txt  | ( while read arg; do cat $arg; done )
# Equivalent to cat files.txt | xargs -I {} cat {}

```

这里，通过使用`while`循环用任意数量的命令替换`cat $arg`，我们可以用相同的参数执行许多命令动作。我们可以不使用管道将输出传递给其他命令。地下`( )`技巧可以在各种有问题的环境中使用。当包含在子 Shell 操作符中时，它充当一个内部有多个命令的单元，如下所示:

```sh
$ cmd0 | ( cmd1;cmd2;cmd3) | cmd4

```

如果`cmd1`是子壳内的`cd /`，工作目录的路径会改变。然而，这种变化只存在于子 Shell 内部。`cmd4`命令不会看到目录改变。

shell 接受一个`-c`选项，用命令行脚本调用一个子 shell。这可以结合`xargs`解决需要多次替换的问题。以下示例查找所有`C`文件并回显每个文件的名称，前面加一个换行符(`-e`选项允许反斜杠替换)。紧随文件名之后的是该文件中出现的所有时间的列表`main`:

```sh
find . -name '*.c' | xargs -I ^ sh -c "echo -ne '\n ^: '; grep main ^"

```

# 用 tr 翻译

`tr`命令是 Unix 命令-战士工具包中的一个通用工具。它被用来创建优雅的单行命令。它执行字符替换，删除选定的字符，并可以从标准输入中压缩重复的字符。Tr 是**翻译**的缩写，因为它将一组字符翻译成另一组。在这个食谱中，我们将看到如何使用`tr`来执行集合之间的基本翻译。

# 准备好

`tr`命令通过**标准输入** ( **标准输入**)接受输入，不能通过命令行参数接受输入。它具有以下调用格式:

```sh
tr [options] set1 set2

```

从`stdin`输入的字符从`set1`中的第一个字符映射到`set2`中的第一个字符，以此类推，输出写入`stdout`(标准输出)。`set1`和`set2`是字符类或一组字符。如果集合的长度不相等，`set2`通过重复最后一个字符扩展到`set1`的长度；否则如果`set2`的长度大于`set1`的长度，所有超过`set1`长度的字符将从`set2`中忽略。

# 怎么做...

要将输入中的字符从大写转换为小写，请使用以下命令:

```sh
$ echo "HELLO WHO IS THIS" | tr 'A-Z' 'a-z'
hello who is this

```

`'A-Z'`和`'a-z'`为套装。我们可以根据需要通过附加字符或字符类来指定自定义集。

`'ABD-}'`、`'aA.,'`、`'a-ce-x'`、`'a-c0-9'`等为有效集合。我们可以很容易地定义集合。我们可以使用`'startchar-endchar'`格式来代替书写连续的字符序列。它还可以与任何其他字符或字符类组合。如果`startchar-endchar`不是有效的连续字符序列，则它们被视为一组三个字符(例如，`startchar`、`-`和`endchar`)。您也可以使用特殊字符，如`'\t'`、`'\n'`或任何 ASCII 字符。

# 它是如何工作的...

使用带有集合概念的`tr`，我们可以很容易地将字符从一个集合映射到另一个集合。让我们看一个使用`tr`加密和解密数字字符的例子:

```sh
$ echo 12345 | tr '0-9' '9876543210'
87654 #Encrypted

$ echo 87654 | tr '9876543210' '0-9'
12345 #Decrypted

```

`tr`命令可用于加密文本。 **ROT13** 是一个众所周知的加密算法。在 ROT13 方案中，字符移动了 13 个位置，因此相同的函数可以加密和解密文本:

```sh
$ echo "tr came, tr saw, tr conquered." | tr 'a-zA-Z' 'n-za-mN-ZA-M'

```

输出如下:

```sh
ge pnzr, ge fnj, ge pbadhrerq.

```

通过将加密的文本再次发送到同一个 ROT13 函数，我们得到如下结果:

```sh
$ echo ge pnzr, ge fnj, ge pbadhrerq. | tr 'a-zA-Z' 'n-za-mN-ZA-M'

```

输出如下:

```sh
tr came, tr saw, tr conquered.

```

`tr`可以将每个制表符转换为一个空格，如下所示:

```sh
$ tr '\t' ' ' < file.txt

```

# 还有更多...

我们看到了一些使用`tr`命令的基本翻译。让我们看看`tr`还能帮助我们实现什么。

# 使用 tr 删除字符

`tr`命令有一个选项`-d`可以使用要删除的指定字符集来删除出现在`stdin`上的一组字符，如下所示:

```sh
$ cat file.txt | tr -d  '[set1]'
#Only set1 is used, not set2

```

考虑这个例子:

```sh
$ echo "Hello 123 world 456" | tr -d '0-9'
Hello world
# Removes the numbers from stdin and print

```

# 补充字符集

我们可以使用一个集合来使用`-c`标志来补充`set1`。`set2`在以下命令中是可选的:

```sh
tr -c [set1] [set2]

```

如果只有`set1`存在，`tr`将删除所有不在`set1`的字符。如果`set2`也存在，`tr`将把不在`set1`中的字符翻译成`set2`中的值。如果单独使用`-c`选项，则必须使用`set1`和`set2`。如果组合`-c`和`-d`选项，只使用`set1`，其他字符全部删除。

以下示例删除输入文本中的所有字符，补码集中指定的字符除外:

```sh
$ echo hello 1 char 2 next 4 | tr -d -c '0-9 \n'
124

```

本示例用空格替换所有不在`set1`中的字符:

```sh
$ echo hello 1 char 2 next 4 | tr -c '0-9' ' '
 1      2     4

```

# 用 tr 压缩字符

`tr`命令可以执行许多文本处理任务。例如，它可以删除字符串中一个字符的多次出现。其基本形式如下:

```sh
tr -s '[set of characters to be squeezed]' 

```

如果您通常在句点后放两个空格，则需要在不删除重复字母的情况下删除多余的空格:

```sh
$ echo "GNU is       not     UNIX.  Recursive   right ?" | tr -s ' '
GNU is not UNIX. Recursive right ?

```

`tr`命令也可以用来删除多余的换行符:

```sh
$ cat multi_blanks.txt | tr -s '\n'
line 1
line 2
line 3
line 4

```

在`tr`的前面用法中，去掉了多余的`'\n'`字符。让我们以一种巧妙的方式使用`tr`从文件中添加给定的数字列表，如下所示:

```sh
$ cat sum.txt
1
2
3
4
5

$ cat sum.txt | echo $[ $(tr '\n' '+' ) 0 ]
15

```

这个黑客是如何工作的？

在这里，`tr`命令将`'\n'`替换为`'+'`字符，因此，我们形成了字符串`1+2+3+..5+` `,`，但是在字符串的末尾我们有一个额外的`+`运算符。为了取消`+`操作符的效果，附加了`0`。

`$[ operation ]`执行数字操作。因此，它形成了这个字符串:

```sh
echo $[ 1+2+3+4+5+0 ]

```

如果我们使用循环从文件中读取数字来执行加法，将需要几行代码。有了`tr`，一个一个的班轮就可以了。

更棘手的是，当我们有一个包含字母和数字的文件时，我们想要对这些数字求和:

```sh
$ cat test.txt
first 1
second 2
third 3

```

我们可以使用`tr`用`-d`选项去掉字母，然后用`+`替换空格:

```sh
$ cat test.txt | tr -d [a-z] | echo "total: $[$(tr ' ' '+')]"
total: 6

```

# 字符类

`tr`命令可以使用不同的字符类作为集合。以下是支持的字符类:

*   `alnum`:字母数字字符
*   `alpha`:字母字符
*   `cntrl`:控制(非打印)字符
*   `digit`:数字字符
*   `graph`:图形字符
*   `lower`:小写字母字符
*   `print`:可打印字符
*   `punct`:标点符号
*   `space`:空白字符
*   `upper`:大写字符
*   `xdigit`:十六进制字符

我们可以选择所需的类，如下所示:

```sh
tr [:class:] [:class:]

```

考虑这个例子:

```sh
tr '[:lower:]' '[:upper:]'

```

# 校验和和验证

校验和程序用于从文件中生成相对较小的唯一密钥。我们可以重新计算密钥来确认文件没有更改。文件可能会被故意修改(添加新用户会更改密码文件)、意外修改(从光驱读取数据时出错)或恶意修改(插入病毒)。校验和让我们验证文件是否包含我们期望的数据。

备份应用使用校验和来检查文件是否已被修改并需要备份。

大多数软件发行版也有可用的校验和文件。即使是可靠的协议，如 TCP，也允许在传输过程中修改文件。因此，我们需要通过应用某种测试来知道接收到的文件是否是原始文件。

通过将我们下载的文件的校验和与分发器计算的校验和进行比较，我们可以验证接收到的文件是否正确。如果从源位置的原始文件计算的校验和与在目标位置计算的校验和相匹配，则文件已成功接收。

一些系统验证套件维护关键文件的校验和。如果恶意软件修改了文件，我们可以从更改的校验和中检测到这一点。

在本食谱中，我们将看到如何计算校验和来验证数据的完整性。

# 准备好

Unix 和 Linux 支持多种校验和程序，但最健壮、使用最广泛的算法是 **MD5** 和 **SHA-1** 。 **ms5sum** 和 **sha1sum** 程序通过对数据应用相应的算法来生成校验和字符串。让我们看看如何从文件中生成校验和，并验证该文件的完整性。

# 怎么做...

要计算 md5sum，请使用以下命令:

```sh
$ md5sum filename
68b329da9893e34099c7d8ad5cb9c940 filename

```

`md5sum`是给定的 32 个字符的十六进制字符串。

我们可以将校验和输出重定向到一个文件供以后使用，如下所示:

```sh
$ md5sum filename > file_sum.md5

```

# 它是如何工作的...

`md5sum`校验和计算的语法如下:

```sh
$ md5sum file1 file2 file3 ..

```

当使用多个文件时，输出将包含每个文件的校验和，每行一个校验和报告:

```sh
[checksum1]   file1
[checksum1]   file2
[checksum1]   file3

```

可以用生成的文件验证文件的完整性，如下所示:

```sh
$ md5sum -c file_sum.md5
# It will output a message whether checksum matches or not

```

如果我们需要使用所有可用的`.md5`信息检查所有文件，请使用以下内容:

```sh
$ md5sum -c *.md5

```

SHA-1 是另一种常用的校验和算法。它根据输入生成一个 40 个字符的十六进制代码。`sha1sum`命令计算一个 SHA-1 `checksum`。其用法与`md5sum`相似。只需在前面提到的所有命令中将`md5sum`替换为`sha1sum`。将输出文件名改为`file_sum.sha1`，而不是`file_sum.md5`。

校验和对于验证从互联网下载的文件的完整性非常有用。ISO 图像易受错误位的影响。一些错误的位和国际标准化组织可能是不可读的，或者，更糟糕的是，它可能会安装以奇怪的方式失败的应用。大多数文件存储库都包含一个`md5`或`sha1`文件，您可以用它来验证文件是否被正确下载。

![](assets/B05265_02_01_New.png)

这是创建的 MD5 总和校验和:

```sh
3f50877c05121f7fd8544bef2d722824 *ubuntu-16.10-desktop-amd64.iso
e9e9a6c6b3c8c265788f4e726af25994 *ubuntu-16.10-desktop-i386.iso
7d6de832aee348bacc894f0a2ab1170d *ubuntu-16.10-server-amd64.iso
e532cfbc738876b353c7c9943d872606 *ubuntu-16.10-server-i386.iso

```

# 还有更多...

校验和在用于许多文件时也很有用。让我们看看如何将校验和应用于文件集合并验证其准确性。

# 目录的校验和

计算文件的校验和。计算目录的校验和需要递归计算目录中所有文件的校验和。

`md5deep`或`sha1deep`命令遍历文件树并计算所有文件的校验和。这些程序可能没有安装在您的系统上。使用`apt-get`或`yum`安装`md5deep`套件。该命令的示例如下:

```sh
$ md5deep -rl directory_path > directory.md5

```

`-r`选项允许 md5deep 递归到子目录中。`-l`选项允许显示相对路径，而不是默认的绝对路径。

```sh
# `-r` to enable recursive traversal
# `-l` to use relative path. By default it writes absolute file
path in output

```

`find`和`md5sum`命令可用于递归计算校验和:

```sh
$ find directory_path -type f -print0 | xargs -0 md5sum >> directory.md5

```

要验证，请使用以下命令:

```sh
$ md5sum -c directory.md5

```

*   **md5** 和 **SHA-1 校验和**是单向哈希算法，不能反过来形成原始数据。这些也用于从给定数据生成唯一密钥:

```sh
        $ md5sum file
 8503063d5488c3080d4800ff50850dc9  file
 $ sha1sum file
 1ba02b66e2e557fede8f61b7df282cd0a27b816b  file

```

这些哈希通常用于存储密码。仅存储密码的哈希。当需要对用户进行身份验证时，会读取密码并将其转换为哈希，然后将该哈希与存储的哈希进行比较。如果它们相同，则验证密码并提供访问权限。存储纯文本密码字符串有风险，并且会带来安全风险。

Although commonly used, md5sum and SHA-1 are no longer considered secure. This is because the rise in computing power in recent times that makes it easier to crack them. It is recommended that you use tools such as `bcrypt` or **sha512sum** instead. Read more about this at [http://codahale.com/how-to-safely-store-a-password/](http://codahale.com/how-to-safely-store-a-password/).

*   类似影子的散列(盐散列)

下一个食谱展示了如何为密码生成一个类似影子的盐散列。Linux 中用户密码的哈希存储在`/etc/shadow`文件中。`/etc/shadow`中的一句典型台词是这样的:

```sh
 test:$6$fG4eWdUi$ohTKOlEUzNk77.4S8MrYe07NTRV4M3LrJnZP9p.qc1bR5c.
EcOruzPXfEu1uloBFUa18ENRH7F70zhodas3cR.:14790:0:99999:7::: 

```

`$6$fG4eWdUi$ohTKOlEUzNk77.4S8MrYe07NTRV4M3LrJnZP9p.qc1bR5c.EcOruzPXfEu1uloBFUa18ENRH7F70zhodas3cR`是其密码对应的哈希。

在某些情况下，我们需要编写脚本来编辑密码或添加用户。在这种情况下，我们必须生成一个影子密码字符串，并向影子文件中写入一个与前面类似的行。我们可以使用`openssl`生成一个影子密码。

影子密码通常是加盐密码。`SALT`是一个额外的字符串，用于混淆和加强加密。Salt 由随机位组成，这些位被用作密钥派生函数的输入之一，该函数为密码生成盐散列。

For more details on salt, refer to this Wikipedia page at [h t t p ://e n . w i k i p e d i a . o r g /w i k i /S a l t _ (c r y p t o g r a p h y )](http://en.wikipedia.org/wiki/Salt_(cryptography)).

```sh
$ opensslpasswd -1 -salt SALT_STRING PASSWORD
$1$SALT_STRING$323VkWkSLHuhbt1zkSsUG.

```

用随机字符串替换`SALT_STRING`，用想要使用的密码替换`PASSWORD`。

# 加密工具和哈希

加密技术用于保护数据免受未经授权的访问。与我们刚刚讨论的校验和算法不同，加密程序可以无损地重建原始数据。有许多可用的算法，我们将讨论 Linux/Unix 世界中最常用的算法。

# 怎么做...

来看看如何使用`crypt`、`gpg`、`base64`等工具:

*   `crypt`命令在 Linux 系统上并不常见。这是一个简单且相对不安全的加密工具，它接受来自`stdin`的输入，请求一个`passphrase`，并将加密输出发送到`stdout`:

```sh
        $ crypt <input_file >output_file
 Enter passphrase:

```

我们可以在命令行上提供一个密码:

```sh
        $ crypt PASSPHRASE <input_file >encrypted_file

```

要解密文件，请使用以下命令:

```sh
        $ crypt PASSPHRASE -d <encrypted_file >output_file

```

*   `gpg` (GNU 隐私卫士)是一个广泛使用的保护文件的工具，以确保数据在到达预期目的地之前不会被读取。

`gpg` signatures are also widely used in e-mail communications to "sign" e-mail messages, proving the authenticity of the sender.

要用`gpg`加密文件，请使用以下命令:

```sh
          $ gpg -c filename

```

该命令交互读取密码短语并生成`filename.gpg`。要解密`gpg`文件，请使用以下命令:

```sh
          $ gpg filename.gpg

```

该命令读取密码并解密文件。

We are not covering `gpg` in much detail in this book. For more information, refer to [http://en.wikipedia.org/wiki/GNU_Privacy_Guard](http://en.wikipedia.org/wiki/GNU_Privacy_Guard).

*   **Base64** 是一组类似的编码方案，通过将二进制数据转换为**基数-64** 表示，以 ASCII 字符串格式表示二进制数据。这些程序用于通过电子邮件传输二进制数据。`base64`命令对 Base64 字符串进行编码和解码。要将二进制文件编码为 Base64 格式，请使用以下命令:

```sh
        $ base64 filename > outputfile

```

或者，使用以下命令:

```sh
        $ cat file | base64 > outputfile

```

可以从`stdin`读取。

解码 Base64 数据，如下所示:

```sh
        $ base64 -d file > outputfile

```

或者，使用以下方法:

```sh
        $ cat base64_file | base64 -d > outputfile

```

# 对唯一行和重复行进行排序

对文本文件进行排序是一项常见的任务。`sort`命令对文本文件和`stdin`进行排序。它可以与其他命令相结合，以产生所需的输出。`uniq`常用于`sort`提取唯一(或重复)的线条。以下食谱说明了一些排序和`uniq`用例。

# 准备好

`sort`和`uniq`命令接受作为文件名或来自`stdin`(标准输入)的输入，并通过写入`stdout`输出结果。

# 怎么做...

1.  我们可以对一组文件(例如`file1.txt`和`file2.txt`)进行排序，如下所示:

```sh
        $ sort file1.txt file2.txt > sorted.txt

```

或者，使用以下方法:

```sh
        $ sort file1.txt file2.txt -o sorted.txt

```

2.  对于数字排序，我们使用这个:

```sh
        $ sort -n file.txt

```

3.  要按相反的顺序排序，我们使用以下命令:

```sh
        $ sort -r file.txt

```

4.  按月排序(按 1 月、2 月、3 月，...)，使用这个:

```sh
        $ sort -M months.txt

```

5.  要合并两个已经排序的文件，请使用以下命令:

```sh
        $ sort -m sorted1 sorted2

```

6.  要从已排序的文件中查找唯一的行，请使用以下命令:

```sh
        $ sort file1.txt file2.txt | uniq

```

7.  要检查文件是否已经排序，请使用以下代码:

```sh
        #!/bin/bash 
        #Desc: Sort 
        sort -C filename ; 
        if [ $? -eq 0 ]; then 
           echo Sorted; 
        else 
           echo Unsorted; 
        fi 

```

将`filename`替换为您想要检查并运行脚本的文件。

# 它是如何工作的...

如示例所示，`sort`接受许多参数来定义如何对数据进行排序。排序命令对于期望排序输入的`uniq`命令很有用。

有许多可以使用`sort`和`uniq`命令的场景。让我们来看看各种选项和使用技巧。

为了检查文件是否已经被排序，我们利用了这样一个事实，即如果文件被排序并且非零，则`sort`返回退出代码(`$?`)为 0。

```sh
if sort -c fileToCheck ; then echo sorted ; else echo unsorted ; fi

```

# 还有更多...

这些是`sort`命令的一些基本用法。以下是使用它来完成复杂任务的部分:

# 根据键或列排序

如果输入数据的格式如下，我们可以使用带有排序的列:

```sh
$ cat data.txt
1  mac    2000
2  winxp    4000
3  bsd    1000
4  linux    1000

```

我们可以通过多种方式对此进行分类；目前，它是按照序列号(第一列)进行数字排序的。我们也可以按第二列或第三列排序。

`-k`选项指定排序依据的字符。单个数字指定列。`-r`选项指定按相反顺序排序。考虑这个例子:

```sh
# Sort reverse by column1
$ sort -nrk 1  data.txt
4  linux    1000 
3  bsd    1000 
2  winxp    4000 
1  mac    2000 
# -nr means numeric and reverse

# Sort by column 2
$ sort -k 2  data.txt
3  bsd    1000 
4  linux    1000 
1  mac    2000 
2  winxp    4000

```

Always be careful about the -n option for numeric sort. The sort command treats alphabetical sort and numeric sort differently. Hence, in order to specify numeric sort, the `-n` option should be provided.

当`-k`后跟单个整数时，它指定文本文件中的一列。列由空格字符分隔。如果我们需要将键指定为一组字符(例如，第 2 列的字符 4-5)，我们将范围定义为由句点分隔的两个整数来定义字符位置，并用逗号连接第一个和最后一个字符位置:

```sh
$ cat data.txt

1 alpha 300
2 beta 200
3 gamma 100
$ sort -bk 2.3,2.4 data.txt   ;# Sort m, p, t
3 gamma 100
1 alpha 300
2 beta 200

```

突出显示的字符将用作数字键。要提取它们，使用它们在行中的位置作为关键格式(在前面的示例中，它们是`2`和`3`)。

要使用第一个字符作为密钥，请使用以下命令:

```sh
$ sort -nk 1,1 data.txt

```

要使排序输出`xargs`与`\0`终止符兼容，请使用以下命令:

```sh
$ sort -z data.txt | xargs -0
# Use zero terminator to make safe use with xargs

```

有时，文本可能包含不必要的无关字符，如空格。要按照字典顺序对它们进行排序，忽略标点符号和折叠，请使用以下命令:

```sh
$ sort -bd unsorted.txt

```

`-b`选项用于忽略文件中的前导空行，`-d`选项指定按字典顺序排序。

# 金圣柱

`uniq`命令在给定的输入(`stdin`或文件名命令行参数)中找到唯一的行，并报告或删除重复的行。

此命令仅适用于已排序的数据。因此，`uniq`经常与`sort`命令一起使用。

要生成唯一的行(打印输入中的所有行，并打印一次重复的行)，请使用以下命令:

```sh
$ cat sorted.txt
bash 
foss 
hack 
hack

$ uniq sorted.txt
bash 
foss 
hack 

```

或者，使用以下方法:

```sh
$ sort unsorted.txt | uniq

```

仅显示唯一的行(输入文件中不重复的行):

```sh
$ uniq -u sorted.txt
bash
foss

```

或者，使用以下命令:

```sh
$ sort unsorted.txt | uniq -u

```

要计算每一行在文件中出现的次数，请使用以下命令:

```sh
$ sort unsorted.txt | uniq -c
 1 bash
 1 foss
 2 hack

```

要在文件中查找重复的行，请使用以下命令:

```sh
$ sort unsorted.txt  | uniq -d
hack

```

要指定键，我们可以使用`-s`和`-w`参数的组合:

*   `-s`:指定要跳过的第一个 *N* 字符的编号
*   `-w`:指定要比较的最大字符数

以下示例描述了使用比较键作为`uniq`操作的索引:

```sh
$ cat data.txt
u:01:gnu 
d:04:linux 
u:01:bash 
u:01:hack

```

为了仅测试粗体字符(跳过前两个字符并使用后两个字符)，我们使用`-s 2`跳过前两个字符，使用`-w 2`使用后两个字符:

```sh
$ sort data.txt | uniq -s 2 -w 2
d:04:linux 
u:01:bash 

```

当一个命令的输出作为输入传递给`xargs`命令时，最好对每个数据元素使用零字节结束符。将输出从`uniq`传递到`xargs`也不例外。如果不使用零字节终止符，则使用默认的空格字符来分割`xargs`命令中的参数。例如，来自`stdin`的文本为`this is a line`的一行将被`xargs`命令视为四个独立的参数，而不是一行。当零字节结束符`\0`用作分隔符时，包含空格的整行被解释为单个参数。

`-z`选项生成零字节终止输出:

```sh
$ uniq -z file.txt

```

该命令删除所有文件，文件名从`files.txt`读取:

```sh
$ uniq -z file.txt | xargs -0 rm

```

如果文件名出现多次，`uniq`命令只将文件名写入`stdout`一次，从而避免出现`rm: cannot remove FILENAME: No such file or directory`错误。

# 临时文件命名和随机数

Shell 脚本通常需要存储临时数据。最合适的位置是`/tmp`(重启时会被系统清空)。有两种方法可以为临时数据生成标准文件名。

# 怎么做...

`mktemp`命令将创建一个唯一的临时文件或文件夹名称:

1.  创建临时文件:

```sh
        $ filename=`mktemp`
 $ echo $filename
 /tmp/tmp.8xvhkjF5fH

```

这将创建一个临时文件，将名称存储在文件名中，然后显示名称。

2.  创建临时目录:

```sh
        $ dirname=`mktemp -d`
 $ echo $dirname
 tmp.NI8xzW7VRX

```

这将创建一个临时目录，将名称存储在文件名中，并显示名称。

*   要生成文件名而不创建文件或目录，请使用以下命令:

```sh
                $ tmpfile=`mktemp -u`
 $ echo $tmpfile
 /tmp/tmp.RsGmilRpcT

```

这里，文件名存储在`$tmpfile`中，但不会创建文件。

*   要基于模板创建临时文件名，请使用以下命令:

```sh
                $mktemp test.XXX
 test.2tc

```

# 它是如何工作的...

`mktemp`命令很简单。它生成一个具有唯一名称的文件，并返回它的文件名(或者在目录的情况下是目录名)。

提供自定义模板时，`X`将被随机字母数字字符替换。还要注意的是，模板中必须至少有三个`X`字符才能让`mktemp`工作。

# 拆分文件和数据

有时有必要将一个大文件分割成更小的文件。很久以前，我们不得不分割文件来传输软盘上的大型数据集。今天，我们为了可读性、为了生成日志或为了解决电子邮件附件的大小限制而拆分文件。这些食谱将展示将文件分成不同块的方法。

# 怎么做...

创建 split 命令是为了分割文件。它接受一个文件名作为参数，并创建一组较小的文件，其中原始文件的第一部分位于按字母顺序排列的第一个新文件中，下一组位于按字母顺序排列的下一个文件中，依此类推。

例如，通过指定分割大小，可以将一个 100 KB 的文件分割成每个 10k 的较小文件。拆分命令支持`M`代表 MB，`G`代表 GB，`c`代表字节，`w`代表字。

```sh
$ split -b 10k data.file
$ ls
data.file  xaa  xab  xac  xad  xae  xaf  xag  xah  xai  xaj

```

前面的代码将把`data.file`分成十个文件，每个文件为`10k`。新文件命名为`xab`、`xac`、`xad`等。默认情况下，拆分使用字母后缀。要使用数字后缀，请使用`-d`参数。也可以使用`-a`长度指定后缀长度:

```sh
$ split -b 10k data.file -d -a 4

$ ls
data.file x0009  x0019  x0029  x0039  x0049  x0059  x0069  x0079

```

# 还有更多...

`split`命令有更多选项。让我们仔细检查一下。

# 为分割文件指定文件名前缀

所有以前的拆分文件名都以 x 开头。如果我们要拆分多个文件，我们需要给这些文件命名，所以很明显哪个文件与哪个文件相匹配。我们可以通过提供前缀作为最后一个参数来使用我们自己的文件名前缀。

让我们运行前一个带有`split_file`前缀的命令:

```sh
$ split -b 10k data.file -d -a 4 split_file
$ ls
data.file       split_file0002  split_file0005  split_file0008
strtok.c
split_file0000  split_file0003  split_file0006  split_file0009
split_file0001  split_file0004  split_file0007

```

要根据每次拆分的行数而不是块大小来拆分文件，请使用以下命令:

```sh
-l no_of_lines:
 # Split into files of 10 lines each.
 $ split -l 10 data.file 

```

`csplit`实用程序根据上下文而不是大小分割文件。它可以根据行数或正则表达式模式进行拆分。这对拆分日志文件特别有用。

查看以下示例日志:

```sh
$ cat server.log
SERVER-1 
[connection] 192.168.0.1 success 
[connection] 192.168.0.2 failed 
[disconnect] 192.168.0.3 pending 
[connection] 192.168.0.4 success 
SERVER-2 
[connection] 192.168.0.1 failed 
[connection] 192.168.0.2 failed 
[disconnect] 192.168.0.3 success 
[connection] 192.168.0.4 failed 
SERVER-3 
[connection] 192.168.0.1 pending 
[connection] 192.168.0.2 pending 
[disconnect] 192.168.0.3 pending 
[connection] 192.168.0.4 failed

```

我们可能需要根据每个文件中每个`SERVER`的内容将文件拆分为`server1.log`、`server2.log`和`server3.log`。这可以通过以下方式实现:

```sh
$ csplit server.log /SERVER/ -n 2 -s {*}  -f server -b "%02d.log"       $ rm server00.log 
$ ls
server01.log  server02.log  server03.log  server.log

```

该命令的详细信息如下:

*   `/SERVER/`:这是用来匹配要进行拆分的行的行。
*   `/[REGEX]/`:这是格式。它从当前行(第一行)复制到包含`SERVER`的匹配行，不包括匹配行。
*   `{*}`:这指定根据匹配重复分割，直到文件结束。我们可以通过在花括号之间放置一个数字来指定它要继续的次数。
*   `-s`:这是让命令静音而不是打印其他消息的标志。
*   `-n`:指定作为后缀的位数。`01`、`02`、`03`等等。
*   `-f`:指定拆分文件的文件名前缀(`server`是上例中的前缀)。
*   `-b`:指定后缀格式。`"%02d.log"`类似于 C 语言中的`printf`参数格式，这里*文件名=前缀+后缀*，即`"server" + "%02d.log"`。

我们删除`server00.log`，因为第一个分割文件是一个空文件(匹配字是文件的第一行)。

# 基于扩展名分割文件名

许多 shell 脚本执行涉及修改文件名的操作。他们可能需要重命名文件并保留扩展名，或者将文件从一种格式转换为另一种格式并更改扩展名，同时保留名称，提取文件名的一部分，等等。

该 Shell 具有操作文件名的内置功能。

# 怎么做...

`%`操作员将从`name.extension`中提取名称。本示例从`sample.jpg`中提取`sample`:

```sh
file_jpg="sample.jpg" 
name=${file_jpg%.*} 
echo File name is: $name 

```

输出是这样的:

```sh
File name is: sample

```

`#`操作员将提取分机:

从存储在`file_jpg`变量中的文件名中提取`.jpg`:

```sh
extension=${file_jpg#*.} 
echo Extension is: jpg 

```

输出如下:

```sh
Extension is: jpg

```

# 它是如何工作的...

要从格式化为`name.extension`的文件名中提取名称，我们使用`%`运算符。

`${VAR%.*}`解释如下:

*   为出现在`%`(上例中的`.*`)右侧的通配符模式删除`$VAR`中的字符串匹配。从右到左计算找到通配符匹配。
*   将文件名存储为`VAR=sample.jpg`。因此`.*`从右到左的通配符匹配是`.jpg`。因此，它从`$VAR`串中移除，并且输出是`sample`。

`%`是非热操作。它从右向左查找通配符的最小匹配。`%%`算子和`%`相似，但是贪婪。这意味着它找到了通配符字符串的最大匹配。考虑这个例子，我们有这个:

```sh
VAR=hack.fun.book.txt

```

使用`%`运算符从右向左进行非精确匹配，并匹配`.txt`:

```sh
$ echo ${VAR%.*}

```

输出将是:`hack.fun.book`。

使用`%%`运算符进行贪婪匹配，匹配`.fun.book.txt`:

```sh
$ echo ${VAR%%.*}

```

输出将是:`hack`。

`#`运算符从文件名中提取扩展名。和`%`类似，但是从左到右求值。

`${VAR#*.}`解释如下:

*   删除出现在`#`(上例中的`*.`)右侧的通配符模式匹配的字符串匹配。从左到右的计算应该会使通配符匹配。

同样的，和`%%`的情况一样，运算符##是一个相当于#的贪婪。

它通过从左到右计算进行贪婪匹配，并从指定变量中移除匹配字符串。让我们用这个例子:

```sh
VAR=hack.fun.book.txt

```

`#`操作员从左到右执行非重复匹配，并匹配`hack`:

```sh
$ echo ${VAR#*.} 

```

输出将是:`fun.book.txt`。

`##`操作者从左到右进行贪婪匹配，匹配`hack.fun.book`:

```sh
$ echo ${VAR##*.}

```

输出将是:`txt`。

The `##` operator is preferred over the `#` operator to extract the extension from a filename, since the filename may contain multiple `.` characters. Since `##` makes a greedy match, it always extracts extensions only.

这里有一个提取域名不同部分的实际例子，比如 URL= `www.google.com`:

```sh

$ echo ${URL%.*} # Remove rightmost .*
www.google

$ echo ${URL%%.*} # Remove right to leftmost  .* (Greedy operator)
www

$ echo ${URL#*.} # Remove leftmost  part before *.
google.com

$ echo ${URL##*.} # Remove left to rightmost  part before *.
(Greedy operator) com

```

# 批量重命名和移动文件

我们经常需要移动或者重命名一组文件。系统管理通常需要将具有公共前缀或文件类型的文件移动到新文件夹中。从相机下载的图像可能需要重命名和排序。音乐、视频和电子邮件文件最终都需要重组。

其中许多操作都有自定义应用，但是我们可以编写自己的自定义脚本以我们的方式来完成。

让我们看看如何编写脚本来执行这些操作。

# 准备好

`rename`命令使用 Perl 正则表达式更改文件名。通过组合`find`、`rename`和`mv`命令，我们可以执行很多事情。

# 怎么做...

以下脚本使用 find 定位 PNG 和 JPEG 文件，然后使用`##`运算符和`mv`将它们重命名为`image-1.EXT`、`image-2.EXT`等。这会更改文件的名称，但不会更改其扩展名:

```sh
#!/bin/bash 
#Filename: rename.sh 
#Desc: Rename jpg and png files 

count=1; 
for img in `find . -iname '*.png' -o -iname '*.jpg' -type f -maxdepth 1` 
do 
  new=image-$count.${img##*.} 

  echo "Renaming $img to $new" 
  mv "$img" "$new" 
  let count++ 

done  

```

输出如下:

```sh
$ ./rename.sh
Renaming hack.jpg to image-1.jpg
Renaming new.jpg to image-2.jpg
Renaming next.png to image-3.png

```

前面的脚本将当前目录中的所有`.jpg`和`.png`文件重命名为新的文件名，格式为`image-1.jpg`、`image-2.jpg`、`image-3.png`、`image-4.png`等。

# 它是如何工作的...

前一个脚本使用`for`循环遍历所有以`.jpg`或`.png`扩展名结尾的文件的名称。`find`命令执行该搜索，使用`-o`选项为不区分大小写的匹配指定多个`-iname`选项。`-maxdepth 1`选项将搜索限制在当前目录，而不是任何子目录。

`count`变量初始化为`1`以跟踪图像编号。然后脚本使用`mv`命令重命名文件。文件的新名称是使用`${img##*.}`构建的，它解析当前正在处理的文件名的扩展名(关于`${img##*.}`的解释，请参考本章中基于扩展名的*切片文件名)。*

`let count++`用于循环每次执行时增加文件号。

以下是执行重命名操作的其他方法:

*   将`*.JPG`重命名为`*.jpg`如下:

```sh
        $ rename *.JPG *.jpg

```

*   用`"_"`字符替换文件名中的空格:

```sh
        $ rename 's/ /_/g' *

```

`# 's/ /_/g'`是文件名中的替换部分，`*`是目标文件的通配符。它可以是`*.txt`或任何其他通配符模式。

*   使用这些将任何文件名从大写转换为小写，反之亦然:

```sh
        $ rename 'y/A-Z/a-z/' *
 $ rename 'y/a-z/A-Z/' *

```

*   使用这个递归移动所有`.mp3`文件到一个给定的目录:

```sh
        $ find path -type f -name "*.mp3" -exec mv {} target_dir \;

```

*   用`_`字符替换空格，递归重命名所有文件:

```sh
        $ find path -type f -exec rename 's/ /_/g' {} \;

```

# 拼写检查和词典操作

大多数 Linux 发行版都包含一个字典文件。然而，很少有人意识到这一点，因此拼写错误比比皆是。`aspell`命令行实用程序是一个拼写检查器。让我们看几个利用字典文件和拼写检查器的脚本。

# 怎么做...

`/usr/share/dict/`目录包含一个或多个字典文件，它们是带有单词列表的文本文件。我们可以使用这个列表来检查一个单词是否是字典单词:

```sh
$ ls /usr/share/dict/ 
american-english  british-english

```

要检查给定的单词是否是词典单词，请使用以下脚本:

```sh
#!/bin/bash 
#Filename: checkword.sh 
word=$1 
grep "^$1$" /usr/share/dict/british-english -q  
if [ $? -eq 0 ]; then 
  echo $word is a dictionary word; 
else 
  echo $word is not a dictionary word; 
fi 

```

用法如下:

```sh
$ ./checkword.sh ful 
ful is not a dictionary word 

$ ./checkword.sh fool 
fool is a dictionary word

```

# 它是如何工作的...

在`grep`中，`^`是单词开始标记字符，`$`字符是单词结束标记。`-q`选项抑制任何输出，使`grep`命令安静。

或者，我们可以使用拼写检查`aspell`来检查一个单词是否在字典中:

```sh
#!/bin/bash  
#Filename: aspellcheck.sh 
word=$1  

output=`echo \"$word\" | aspell list`  

if [ -z $output ]; then  
        echo $word is a dictionary word;  
else  
        echo $word is not a dictionary word;  
fi  

```

当给定的输入不是词典单词时，`aspell list`命令返回输出文本，当输入是词典单词时，不输出任何内容。一个`-z`命令检查`$output`是否为空字符串。

`look`命令将显示以给定字符串开头的行。您可以使用它在日志文件中查找以给定日期开头的行，或者在字典中查找以给定字符串开头的单词。默认情况下，`look`搜索`/usr/share/dict/words`，或者你可以提供一个文件进行搜索。

```sh
$ look word

```

或者，这可以用于:

```sh
$ grep "^word" filepath

```

考虑这个例子:

```sh
$ look android
android
android's
androids

```

用它在`/var/log/syslog`中查找给定日期的行:

```sh
$look 'Aug 30' /var/log/syslog

```

# 自动交互输入

我们在命令行上查看了接受参数的命令。Linux 还支持很多从`passwd`到`ssh`的交互应用。

我们可以创建自己的交互式 Shell 脚本。临时用户更容易与一组提示交互，而不是记住命令行标志和正确的顺序。例如，备份用户工作但不备份和锁定文件的脚本可能如下所示:

```sh
$ backupWork.sh

```

*   应该备份哪个文件夹？`notes`
*   应该备份什么类型的文件？`.docx`

当您需要重新运行同一个应用时，自动化交互式应用可以节省您的时间，并在开发应用时减少您的挫败感。

# 准备好了

自动化一项任务的第一步是运行它并记录你做了什么。前面讨论的脚本命令可能有用。

# 怎么做...

检查交互输入的顺序。从前面的代码中，我们可以这样表述序列的步骤:

```sh
notes[Return]docx[Return] 

```

除了前面的步骤，输入`notes`，按下`Return`，输入`docx`，最后按下`Return`转换成单弦，如下图:

```sh
    "notes\ndocx\n"

```

当我们按下 Return 时`\n`字符被发送。通过附加返回(`\n`)字符，我们得到传递给`stdin`(标准输入)的字符串。

通过发送用户键入的字符的等效字符串，我们可以自动将输入传递给交互过程。

# 它是如何工作的...

让我们为自动化示例编写一个交互式读取输入的脚本:

```sh
#!/bin/bash 
# backup.sh 
# Backup files with suffix. Do not backup temp files that start with ~ 
read -p " What folder should be backed up: " folder 
read -p " What type of files should be backed up: " suffix 
find $folder -name "*.$suffix" -a ! -name '~*' -exec cp {} \   
    $BACKUP/$LOGNAME/$folder 
echo "Backed up files from $folder to $BACKUP/$LOGNAME/$folder" 

```

让我们自动向命令发送输入:

```sh
$ echo -e "notes\ndocx\n" | ./backup.sh 
Backed up files from notes to /BackupDrive/MyName/notes

```

这种自动化交互式脚本的方式可以在开发和调试过程中节省大量的打字时间。它也确保了你每次都执行相同的测试，并且不会因为你输入错误而导致追踪一个幻影 bug。

我们使用`echo -e`产生输入序列。`-e`选项向`echo`发出信号，解释转义序列。如果输入很大，我们可以使用输入文件和重定向操作符来提供输入:

```sh
$ echo -e "notes\ndocx\n"  > input.data
$ cat input.data
notes
docx

```

无需手动输入`echo`命令，就可以手工制作输入文件。考虑这个例子:

```sh
$ ./interactive.sh < input.data

```

这将重定向文件中的交互式输入数据。

如果你是一个逆向工程师，你可能玩过缓冲区溢出漏洞。为了利用它们，我们需要重定向一个 Shell 代码，比如用十六进制写的`\xeb\x1a\x5e\x31\xc0\x88\x46`。这些字符不能直接在键盘上键入，因为这些字符的键不存在。因此，我们使用:

```sh
echo -e \xeb\x1a\x5e\x31\xc0\x88\x46"

```

这将把字节序列重定向到易受攻击的可执行文件。

这些回声和重定向技术使交互式输入程序自动化。然而，这些技术是脆弱的，因为没有有效性检查，并且假设目标应用总是以相同的顺序接受数据。如果程序以不断变化的顺序要求输入，或者有些输入并不总是必需的，这些方法就会失败。

预期程序可以执行复杂的交互，并适应目标应用的变化。该程序在全球范围内用于控制硬件测试、验证软件构建、查询路由器统计数据等等。

# 还有更多...

expect 应用是一个类似于 shell 的解释器。它基于 TCL 语言。我们将讨论简单自动化的产生、预期和发送命令。有了 TCL 语言的支持，expect 可以完成更复杂的任务。你可以在 [www.tcl.tk](http://www.tcl.tk) 网站上了解更多关于 TCL 语言的知识。

# 期待自动化

`expect`并不是所有的 Linux 发行版默认都有。您可能需要使用软件包管理器(`apt-get`或`yum`)安装 expect 软件包。

Expect 有三个主要命令:

| **命令** | **描述** |
| `spawn` | 运行新的目标应用。 |
| `expect` | 观察目标应用发送的模式。 |
| `send` | 向目标应用发送字符串。 |

以下示例生成备份脚本，然后查找模式`*folder*`和`*file*`，以确定备份脚本是要求文件夹名称还是文件名。然后，它将发送适当的回复。如果备份脚本被重写以首先请求文件，然后请求文件夹，这个自动化脚本仍然可以工作。

```sh
#!/usr/bin/expect  
#Filename: automate_expect.tcl 
spawn ./backup .sh  
expect { 
  "*folder*" { 
     send "notes\n" 
     exp_continue 
   } 
  "*type*" { 
     send "docx\n" 
     exp_continue 
  } 
} 

```

运行方式为:

```sh
$ ./automate_expect.tcl 

```

`spawn`命令的参数是要自动化的目标应用和参数。

`expect`命令接受一组要寻找的模式，以及当该模式匹配时要执行的动作。该操作包含在大括号中。

`send`命令是要发送的消息。这类似于回声`-n -e`，因为它不自动包含换行符，并且理解反斜杠符号。

# 通过运行并行进程使命令更快

计算能力不断提高，不仅因为处理器的时钟周期更高，还因为它们有多个内核。这意味着在单个硬件处理器中有多个逻辑处理器。就像有几台电脑，而不是只有一台。

但是，除非软件使用多核，否则多核是无用的。例如，一个进行大量计算的程序可能只在一个内核上运行，而其他内核将处于空闲状态。如果我们想让软件更快，它必须意识到并利用多核。

在这个食谱中，我们将看到如何让我们的命令运行得更快。

# 怎么做...

让我们举一个我们在前面的食谱中讨论过的`md5sum`命令的例子。该命令执行复杂的计算，使其成为 CPU 密集型。如果我们有多个要生成校验和的文件，我们可以使用如下脚本运行`md5sum`的多个实例:

```sh
#/bin/bash 
#filename: generate_checksums.sh 
PIDARRAY=() 
for file in File1.iso File2.iso 
do 
  md5sum $file & 
  PIDARRAY+=("$!") 
done 
wait ${PIDARRAY[@]} 

```

当我们运行该程序时，我们会得到以下输出:

```sh
$ ./generate_checksums.sh 
330dcb53f253acdf76431cecca0fefe7  File1.iso
bd1694a6fe6df12c3b8141dcffaf06e6  File2.iso

```

输出将与运行以下命令相同:

```sh
md5sum File1.iso File2.iso

```

然而，如果`md5sum`命令同时运行，如果您有多核处理器，您将更快地获得结果(您可以使用`time`命令验证这一点)。

# 它是如何工作的...

我们利用 Bash 操作数`&`，它指示 shell 将命令发送到后台并继续执行脚本。然而，这意味着我们的脚本将在循环完成后立即退出，而`md5sum`进程仍在后台运行。为了防止这种情况，我们使用`$!`获取进程的 PID，它在 Bash 中保存最后一个后台进程的 PID。我们将这些 PiD 附加到一个数组中，然后使用`wait`命令等待这些过程完成。

# 还有更多...

Bash `&`操作数适用于少量任务。如果您有 100 个文件要校验，脚本会尝试启动 100 个进程，并可能迫使您的系统进行交换，这将使任务运行得更慢。

GNU 并行命令不是所有安装的一部分，但是它也可以用您的包管理器加载。并行命令优化了资源的使用，而不会使任何资源过载。

并行命令读取`stdin`上的文件列表，并使用类似于查找命令的`-exec`参数的选项来处理这些文件。`{}`符号代表要处理的文件，`{.}`符号代表不带后缀的文件名。

以下命令使用 **Imagemagick 的** `convert`命令为一个文件夹中的所有图像制作新的、调整大小的图像:

```sh
ls *jpg | parallel convert {} -geometry 50x50 {.}Small.jpg

```

# 检查目录、其中的文件和子目录

我们处理的最常见的问题之一是找到放错地方的文件，并整理出混乱的文件层次结构。本节将讨论检查文件系统的一部分和呈现内容的技巧。

# 准备好了

我们讨论的`find`命令和循环为我们提供了检查和报告目录及其内容细节的工具。

# 怎么做...

接下来的食谱展示了两种检查目录的方法。首先，我们将层次结构显示为树，然后我们将看到如何在一个目录下生成文件和文件夹的摘要。

# 生成目录的树视图。

有时，如果文件系统以图形方式呈现，则更容易可视化。

下一个配方汇集了我们讨论过的几个工具。它使用 find 命令生成当前文件夹下所有文件和子文件夹的列表。

`-exec`选项创建一个子 Shell，它使用 echo 将文件名发送到`tr`命令的`stdin`。有两个`tr`命令。第一个删除所有字母数字字符，以及任何破折号(`-`)、下划线(`_`)或句点(`.`)。这仅将路径中的斜线(`/`)传递给第二个`tr`命令，该命令将这些斜线转换为空格。最后，`basename`命令从文件名中去除前导路径并显示它。

使用这些查看`/var/log`中的文件夹树:

```sh
$ cd /var/log
$ find . -exec sh -c 'echo -n {} | tr -d "[:alnum:]_.\-" | \
    tr "/" " "; basename {}' \;

```

生成以下输出:

```sh
mail
 statistics
gdm
 ::0.log
 ::0.log.1
cups
 error_log
 access_log
 ... access_l

```

# 生成文件和子目录的摘要

我们可以结合`find`命令、`echo`和`wc`命令生成子目录列表以及其中的文件数量，这将在下一章中详细讨论。

使用以下内容获取当前文件夹中文件的摘要:

```sh
for d in `find . -type d`;  
  do  
  echo `find $d -type f | wc -l` files in $d;  
done 

```

如果该脚本在`/var/log`中运行，它将生成如下输出:

```sh
103 files in .
17 files in ./cups
0 files in ./hp
0 files in ./hp/tmp

```