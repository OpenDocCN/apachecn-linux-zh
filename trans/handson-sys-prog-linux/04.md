# 动态内存分配

在本章中，我们将深入研究现代操作系统上系统编程的一个关键方面--动态(运行时)内存分配和释放的管理。 我们将首先介绍用于动态分配和释放内存的基本 glibcAPI。 然后，我们将超越这些基础知识，研究 VAS 中的程序中断以及`malloc(3)`在不同环境下的行为。

然后，我们将让读者沉浸在一些高级讨论中：请求分页、内存锁定和保护，以及`alloca`API 的使用。

代码示例为读者提供了以动手方式探索这些主题的机会。

在本章中，我们将介绍以下主题：

*   基本的 glibc 动态内存管理 API 及其在代码中的正确用法
*   程序中断(以及通过`sbrk(3)`API 进行管理)
*   分配不同内存量时`malloc(3)`的内部行为
*   高级功能：
    *   按需寻呼的概念
    *   内存锁定
    *   内存区保护
    *   使用`alloca (3)`API 替代

# Glibc malloc(3)API 家族

在[第 2 章](02.html)，*虚拟内存*中，我们了解到在使用**虚拟地址空间**(**VAS**)的过程中，有一些区域或段可以使用动态内存分配。 **堆段**就是这样一个动态区域--免费赠送内存给进程以供其运行时使用。

开发人员究竟是如何利用这种内存天赋的呢？ 不仅如此，开发人员必须非常小心地将内存*分配*与后续内存*释放*相匹配，否则系统将不会喜欢它！

**GNU C 库**(**glibc**)提供了一组很小但功能强大的 API，使开发人员能够管理动态内存；本节将详细介绍它们的用法。

正如您将看到的，内存管理 API 实际上只有几个：`malloc(3)`、`calloc`、`realloc`和`free`。 尽管如此，正确使用它们仍然是一个挑战！ 接下来的章节(和章节)将揭示为什么会出现这种情况。 继续读下去！

# Malloc(3)API

也许应用程序开发人员使用的最常见的 API 之一是著名的`malloc(3)`*。*

The `foo(3)` syntax indicates that the `foo` function is in section 3 of the manual (the man pages) – a library API, not a system call. We recommend you develop the habit of reading the man pages. The man pages are available online, and you can find them at [https://linux.die.net/man/](https://linux.die.net/man/)*.*

我们使用*`malloc(3)`在运行时动态分配内存块。 这与静态或编译时动态内存分配相反，在静态或编译时动态内存分配中，我们会做出如下声明：

```sh
char buf[256];
```

在前面的情况下，内存是静态分配的(在编译时)。

那么，你到底是如何使用`malloc(3)`的呢？ 让我们来看看它的签名：

```sh
#include <stdlib.h>
void *malloc(size_t size);
```

`malloc(3)`的参数是要分配的字节数。 但是，`size_t`数据类型是什么呢？ 显然，它不是 C 基元数据类型；它是典型 64 位平台上的`typedef – long unsigned int`(确切的数据类型因平台而异；重要的一点是它始终是无符号的-它不能是负数。 在 32 位 Linux 上，它将被命名为`unsigned int`)。 确保代码与函数签名和数据类型精确匹配对于编写健壮而正确的程序至关重要。 在此期间，请确保包括手册页与 API 签名一起显示的头文件。

To print a variable of the `size_t` type within a `printf`, use the **`%zu`** format specifier:
`size_t sz = 4 * getpagesize();`
`[...]`
`printf("size = %zu bytes\n", sz);`

在本书中，我们不会深入研究关于`malloc(3)`和 Friends 如何实际存储、分配和释放内存的内部实现细节(请参阅 GitHub 存储库的*进一步阅读*部分。)。 可以说，内部实现力求尽可能高效；使用这些 API 通常被认为是执行内存管理的正确方式。

如果成功，返回值是指向新分配的内存区第 0 个字节的指针；如果失败，返回值是指向第 0 个字节的指针。

You will come across, shall we say *optimists*, who say things such as, "Don't bother checking malloc for failure, it never fails". Well, take that sage advice with a grain of salt. While it's true that malloc would rarely fail, the fact is (as you shall see), it could fail. Writing defensive code – code that checks for the failure case immediately – is a cornerstone of writing solid, robust programs.

因此，使用 API 非常简单：例如，动态分配 256 字节的内存，并将指向新分配区域的指针存储在参数`ptr`变量中：

```sh
void *ptr;
ptr = malloc(256);
```

另一个典型的例子是，程序员需要为数据结构分配内存；让我们称其为`struct sbar`。 您可以这样做：

```sh
    struct sbar {
        int a[10], b[10];
        char buf[512];
    } *psbar;

 psbar = malloc(sizeof(struct sbar));
    // initialize and work with it
    [...]
    free(psbar);
```

嘿，精明的读者！ 检查一下故障案例怎么样？ 这是一个关键点，因此我们将这样重写前面的代码(当然，`malloc(256)`代码片段也是如此)：

```sh
struct [...] *psbar;
sbar = malloc(sizeof(struct sbar));
if (!sbar) {
 *<... handle the error ...>*
}
```

让我们使用功能强大的跟踪工具之一`ltrace`来检查这是否按预期工作；`ltrace`用于显示进程执行路径中的所有库 API(类似地，使用`strace`跟踪所有系统调用)。 假设我们编译了前面的代码，生成的二进制可执行文件名为`tst`：

```sh
$ ltrace ./tst 
malloc(592)           = 0xd60260
free(0xd60260)        = <void>
exit(0 <no return ...>
+++ exited (status 0) +++
$ 
```

我们可以清楚地看到`malloc(3)`(以及我们使用的示例结构在 x86_64 上占用了 592 字节)，以及它的返回值(跟在`=`符号后面)。 紧随其后的是`free`API，然后它简单地退出。

重要的是要理解，由`malloc(3)`分配的内存块的*内容*被认为是随机的。 因此，程序员有责任在读取内存之前对其进行初始化；如果您做不到这一点，则会导致名为**Uninitialized Memory Read**(**UMR***)和*的 bug(下一章将对此进行详细介绍)。

`malloc(3)` always returns a memory region that is aligned on an 8-byte boundary. Need larger alignment values? Use the `posix_memalign(3)` API. Deallocate its memory as usual with free(3).
Details can be found on the man page at [https://linux.die.net/man/3/posix_memalign](https://linux.die.net/man/3/posix_memalign).Examples of using the `posix_memalign(3)` API can be found in the *Locking memory* and *Memory protection* sections.

# 马洛克(3)

以下是一些常见问题解答，有助于我们更多地了解`malloc(3)`：

*   常见问题 1：`malloc(3)`一次调用可以分配多少内存？

从实际意义上讲，这是一个相当无意义的问题，但这是一个经常被问到的问题！

`malloc(3)`的参数是`size_t`数据类型的整数值，因此，从逻辑上讲，我们可以作为参数传递给`malloc(3)`的最大值是`size_t`在平台*上可以采用的最大值。* 实际上，在 64 位 Linux 上，`size_t`将是 8 个字节，当然，以位为单位是 8*8=64。 因此，在单个`malloc(3)`调用中可以分配的最大内存量是`2^64`！

那么，多少钱呢？ 让我们进行实验(请务必阅读[第 19 章](19.html)、*故障排除和最佳实践*以及其中关于*经验方法*的简要讨论)。并实际尝试一下(请注意，必须使用`-lm`开关将以下代码片段链接到数学库)：

```sh
    int szt = sizeof(size_t);
    float max=0;
    max = pow(2, szt*8);
    printf("sizeof size_t = %u; " 
            "max value of the param to malloc = %.0f\n", 
            szt, max);
```

X86_64 上的输出：

**`sizeof size_t = 8; max param to malloc = 18446744073709551616`**

啊哈！ 这是一个非常大的数字；更具可读性的是，它如下所示：

`2^64 = 18,446,744,073,709,551,616 = 0xffffffffffffffff`

那就是 16EB(艾字节，相当于 16384PB，相当于 1600 万 TB)！

因此，在 64 位操作系统上，`malloc(3)`在一次调用中最多可以分配 16 个 EB。 理论上是这样的。

As usual, there's more to it: please see *FAQ 2*; it will reveal that the *theoretical* answer to this question is **8 exabytes** (8 EB).

显然，在实践中，这是不可能的，因为，当然，这就是流程本身的整个用户模式 VAS。 实际上，可以分配的内存量受到堆上连续可用的空闲内存量的限制。 实际上，还有更多的事情要做。 正如我们很快就会了解到的(在第*节中，malloc(3)的实际行为*部分)，对 Mallloc`malloc(3)`的记忆也可以来自 VAS 的其他区域。 别忘了数据段大小是有资源限制的；默认值通常是无限制的，正如我们在本章中讨论的那样，这实际上意味着操作系统没有人为的限制。

因此，在实践中，最好是明智的，不要做任何假设，并检查返回值是否为空。

顺便说一句，`size_t`在 32 位操作系统上可以采用的最大值是多少？因此，我们通过将`-m32`开关传递给编译器，在 x86_64 上编译 32 位操作系统：

```sh
$ gcc -m32 mallocmax.c -o mallocmax32 -Wall -lm
$ ./mallocmax32
*** max_malloc() ***
sizeof size_t = 4; max value of the param to malloc = 4294967296
[...]
$ 
```

显然，它是 4 GB(GB)-同样，32 位进程的整个 VAS。

*   常见问题 2：如果我将`malloc(3)`作为否定参数传递，该怎么办？

`malloc(3)`的参数`size_t`的数据类型是一个无符号整数*和*数量-它不能为负。 但是，人类并不完美，**整数溢出**(**IOF**)bug 确实存在！ 您可以想象这样一个场景：程序尝试计算要分配的字节数，如下所示：

`num = qa * qb;`

如果`num`被声明为有符号整数变量，并且`qa`和`qb`足够大，以至于乘法运算的结果导致溢出，该怎么办？ 然后，`num`的结果会变成负值！当然，`malloc(3)`应该会失败。 但稍等一下：如果把`num`这个变量声明为`size_t`(应该是这样的)，负量就会变成一些正量！

`mallocmax`程序对此有一个测试用例。

以下是在 x86_64 Linux 计算机上运行时的输出：

```sh
*** negative_malloc() ***
size_t max    = 18446744073709551616
ld_num2alloc  = -288225969623711744
szt_num2alloc = 18158518104085839872
1\. long int used:  malloc(-288225969623711744) returns (nil)
2\. size_t used:    malloc(18158518104085839872) returns (nil)
3\. short int used: malloc(6144) returns 0x136b670
4\. short int used: malloc(-4096) returns (nil)
5\. size_t used:    malloc(18446744073709547520) returns (nil)
```

以下是相关的变量声明：

```sh
const size_t onePB    = 1125899907000000; /* 1 petabyte */
int qa = 28*1000000;
long int ld_num2alloc = qa * onePB;
size_t szt_num2alloc  = qa * onePB;
short int sd_num2alloc;
```

现在，让我们用该程序的 32 位版本尝试一下。

Note that on a default-install Ubuntu Linux box, the 32-bit compile may fail (with an error such as `fatal error: bits/libc-header-start.h: No such file or directory`*)*. Don't panic: this usually implies that the compiler support for building 32-bit binaries isn't present by default. To get it (as mentioned in the Hardware-Software List document), install the `multilib` compiler package: `sudo apt-get install gcc-multilib`.

将其编译为 32 位版本并运行：

```sh
$ ./mallocmax32 
*** max_malloc() ***
sizeof size_t = 4; max param to malloc = 4294967296
*** negative_malloc() ***
size_t max    = 4294967296
ld_num2alloc  = 0
szt_num2alloc = 1106247680
1\. long int used:  malloc(-108445696) returns (nil)
2\. size_t used:    malloc(4186521600) returns (nil)
3\. short int used: malloc(6144) returns 0x85d1570
4\. short int used: malloc(-4096) returns (nil)
5\. size_t used:    malloc(4294963200) returns (nil)
$ 
```

公平地说，编译器确实警告我们：

```sh
gcc -Wall   -c -o mallocmax.o mallocmax.c
mallocmax.c: In function ‘negative_malloc’:
mallocmax.c:87:6: warning: argument 1 value ‘18446744073709551615’ exceeds maximum object size 9223372036854775807 [-Walloc-size-larger-than=]
  ptr = malloc(-1UL);
  ~~~~^~~~~~~~~~~~~~
In file included from mallocmax.c:18:0:
/usr/include/stdlib.h:424:14: note: in a call to allocation function ‘malloc’ declared here
 extern void *malloc (size_t __size) __THROW __attribute_malloc__ __wur;
              ^~~~~~ 
[...]
```

有意思的!。 编译器现在回答我们的*常见问题 1*的问题：

```sh
[...] warning: argument 1 value ‘18446744073709551615’ *exceeds maximum object size* *9223372036854775807* [-Walloc-size-larger-than=] [...]
```

根据编译器可以分配的最大值似乎是**`9223372036854775807`**。

哇。 少量计算器时间显示这是 8192PB=8EB！ 因此，我们必须得出上一个问题的正确答案：*一次调用 malloc 可以分配多少内存？*答案：*8EB*。 再说一次，从理论上讲。

*   常见问题 3：如果我使用`malloc(0)`怎么办？

不是很多；根据实现的不同，`malloc(3)`将返回 NULL 或一个可以传递给 FREE 的非 NULL 指针。 当然，即使指针非空，也没有内存，所以不要尝试使用它。

让我们试试看：

```sh
    void *ptr;
    ptr = malloc(0);
    free(ptr);
```

我们编译它，然后通过`ltrace`运行它：

```sh
$ ltrace ./a.out 
malloc(0)                                  = 0xf50260
free(0xf50260)                                = <void>
exit(0 <no return ...>
+++ exited (status 0) +++
$ 
```

这里，`malloc(0)`确实返回了一个非空指针。

*   常见问题 4：如果我使用`malloc(2048)`并尝试读/写超过 2,048 个字节，该怎么办？

这当然是一个 bug--一种越界的内存访问 bug，进一步定义为读或写缓冲区溢出。 请稍等，有关内存错误(以及随后如何查找和修复它们)的详细讨论是[第 5 章](05.html)、*Linux 内存问题*和[第 6 章](06.html)以及*内存问题调试工具*的主题。

# Malloc(3)：快速摘要

因此，让我们总结一下关于`malloc(3)`API 使用的要点：

*   `malloc(3)`动态(在运行时)从进程堆分配内存
    *   我们很快就会了解到，情况并不总是如此。
*   `malloc(3)`的单个参数是一个无符号整数值-要分配的字节数
*   如果成功，返回值是指向新分配内存块开始的指针；如果失败，则返回值为 NULL：
    *   你必须检查失败的情况，不要只假定它会成功
    *   `malloc(3)`始终返回与 8 字节边界对齐的内存区域
*   新分配的存储区的内容被认为是随机的
    *   您必须先对其进行初始化，然后才能读取它的任何部分
*   您必须释放分配的内存

# 免费的 API

在这个生态系统中，开发的黄金规则之一是必须释放程序员分配的内存。

如果做不到这一点，就会导致一种糟糕的情况--一个 bug，实际上就是所谓的**内存泄漏**；这将在下一章中进行一些深入的讨论。 仔细匹配您的分配和自由是至关重要的。

Then again, in smaller real-world projects (utils), you do come across cases where memory is allocated exactly once; in such cases, freeing the memory is pedantic as the entire virtual address space is destroyed upon process-termination. Also, using the *alloca(3)* API implies that you do not need to free the memory region (seen later in, *Advanced features *section). Nevertheless, you are advised to err on the side of caution!

使用`free(3)`API 非常简单：

`void free(void *ptr);`

它接受一个参数：指向要释放的内存块的指针。`ptr`必须是由`malloc(3)`系列例程之一返回的指针：`malloc(3)`、`calloc`或`realloc[array]`。

`free`不返回任何值；甚至不要尝试检查它是否工作；如果使用正确，它就工作。 有关可用内存的更多信息，请参阅*一节中的*部分。 一旦释放了内存块，显然就不能再尝试使用该内存块的任何部分；这样做会导致错误(或所谓的**UB-未定义行为**)。

关于`free()`的一个常见误解有时会导致它以错误的方式使用；看看下面的伪代码片段：

```sh
void *ptr = NULL;
[...] 
while(*<some-condition-is-true>*) {
    if (!ptr)
        ptr = malloc(n);

    [...
   * <use 'ptr' here>*
    ...]

    free(ptr);
}
```

这个程序在几次迭代后可能会在循环中崩溃(在`<use 'ptr' here>`代码中)。 为什么？ 因为`ptr`的内存指针已被释放，并且正在尝试重用。 但是为什么呢？ 啊，仔细看一下：如果代码片段当前为 NULL，那么它只会指向第`ptr`个指针的`malloc(3)`，也就是说，它的程序员假设一旦我们`free()`内存，我们刚刚释放的指针就会被设置为 NULL。 不是这样的！！

在编写代码时要保持警惕和防御性。 不要假设任何事情；这是大量错误的来源。(重要的是，我们的[章](19.html)、*故障排除和最佳实践*涵盖了这些要点)

# 免费下载-提供快速摘要

因此，让我们总结一下有关使用*free*API 的要点：

*   传递给`free(3)`的参数必须是`malloc(3)`系列 API(`malloc(3)`、`calloc`或`realloc[array]`)之一返回的值。
*   `free`没有返回值。
*   调用`free(ptr)`不会将`ptr`设置为`NULL`(不过这会很好)。
*   释放后，不要尝试使用释放的内存。
*   不要多次尝试*释放*同一个内存块(这是一个错误-UB)。
*   目前，我们假设释放的内存返回到系统。
*   看在上帝的份上，别忘了释放之前动态分配的内存。 据说被遗忘的记忆已经泄露了*，这真的是一个很难捕捉到的错误！ 幸运的是，有一些工具可以帮助我们捕捉这些错误。 有关详细信息，请参阅[第 5 章](05.html)、*Linux 内存问题*、[第 6 章](06.html)、*内存问题调试工具*。*

 *# Calloc API

`calloc(3)`API 与`malloc(3)`几乎相同，主要有两个方面的不同：

*   它将其分配的内存块初始化为零值(即 ASCII 0 或 NULL，而不是数字`0`)
*   它接受两个参数，而不是一个

`calloc(3)`函数签名如下：

` void *calloc(size_t nmemb, size_t size);`

第一个参数`nmemb`是 n 个成员；第二个参数`size`是每个成员的大小。 实际上，`calloc(3)`分配了 1 个`(nmemb*size)`字节的内存块。 因此，如果您想要为一个由 1000 个整数组成的数组分配内存，可以这样做：

```sh
    int *ptr;
    ptr = calloc(1000, sizeof(int));
```

假设整数的大小是 4 字节，我们总共分配了(1000*4)=4000 字节。

无论何时需要内存用于项目数组(应用程序中的一个常见用例是结构数组)，`calloc`是一种既可以分配内存又可以同时初始化内存的便捷方法。

Demand paging (covered later in this chapter), is another reason programmers use `calloc` rather than `malloc(3)` (in practice, this is mostly useful for realtime applications). Read up on this in the up coming section.

# Realloc API

`realloc`API 用于*调整*现有内存块的大小-增大或缩小它。 此调整大小只能在先前使用`malloc(3)`系列 API 之一分配的内存上执行(通常为：`malloc(3)`、`calloc`或`realloc[array]`)。 以下是它的签名：

` void *realloc(void *ptr, size_t size);`

第一个参数`ptr`是指向先前使用`malloc(3)`系列 API 之一分配的内存块的指针；第二个参数`size`是内存块的新大小-它可以大于或小于原始大小，从而增大或缩小内存块。

一个快速示例代码片段将帮助我们理解`realloc`：

```sh
void *ptr, *newptr;
ptr = calloc(100, sizeof(char)); // error checking code not shown here
newptr = realloc(ptr, 150);
if (!newptr) {
    fprintf(stderr, "realloc failed!");
 free(ptr);
    exit(EXIT_FAILURE);
}
*< do your stuff >*
free(newptr);
```

`realloc`返回的指针是指向新调整大小的内存块的指针；它可能与原始的`ptr`地址相同，也可能不相同。 实际上，您现在应该完全忽略原始指针`ptr`，而将由 realloc 返回的`newptr`指针视为要使用的指针。 如果失败，则返回值为空(请检查！)。 并且原始内存块保持不变。

一个关键点：`realloc(3)`返回的指针，即`newptr`，是随后必须释放的指针，*而不是*，即指向(现在已调整大小的)内存块的原始指针(`ptr`)。 当然，不要试图释放两个指针，因为这是一个错误。

那么刚刚调整大小的内存块的内容呢？ 它们在`MIN(original_size, new_size)`之前保持不变。 因此，在前面的示例`MIN(100, 150) = 100`中，最大 100 字节的内存内容将保持不变。 剩余的(50 字节)怎么办？ 它被认为是随机内容(就像`malloc(3)`一样)。

# Realloc(3)-角案例

请考虑以下代码片段：

```sh
void *ptr, *newptr;
ptr = calloc(100, sizeof(char)); // error checking code not shown here
newptr = realloc(NULL, 150);
```

传递给`realloc`的指针是`NULL`吗？ 图书馆认为这相当于一个新的分配模式`malloc(150)`；而`malloc(3)`模式的所有含义就是这样。

现在，考虑以下代码片段：

```sh
void *ptr, *newptr;
ptr = calloc(100, sizeof(char)); // error checking code not shown here
newptr = realloc(ptr, 0);
```

传递给`realloc`的大小参数是`0`？ 库将其视为等同于`free(ptr)`*。* 就是这样。

# Reallocarray API

一个场景是：您使用`calloc(3)`为数组分配内存；稍后，您想要调整它的大小，比方说，大得多。 我们可以使用`realloc(3)`来执行此操作；例如：

```sh
struct sbar *ptr, *newptr;
ptr = calloc(1000, sizeof(struct sbar)); // array of 1000 struct sbar's
[...]
// now we want 500 more!
newptr = realloc(ptr, 500*sizeof(struct sbar));
```

很好。 不过，还有一种更简单的方法-使用`reallocarray(3)`API。 其签名如下：

` void *reallocarray(void *ptr, size_t nmemb, size_t size);`

有了它，代码就变得更简单了：

```sh
[...]
// now we want 500 more!
newptr = reallocarray(ptr, 500, sizeof(struct sbar));
```

`reallocarray`的返回值与`realloc`API 的返回值非常相同：成功时是指向调整大小的内存块的新指针(可能与原始的不同)，失败时是指向`NULL`的新指针。 如果失败，原始内存块将保持不变。

`reallocarray`与`realloc`相比有一个真正的优势-安全性。 从*realloc(3)的手册页*可以看到以下代码片段：

```sh
... However, unlike that realloc() call, reallocarray() fails safely in the case where the  multiplication  would  overflow.   If  such  an  overflow occurs, reallocarray() returns NULL, sets errno to ENOMEM, and leaves the original block of memory unchanged.
```

还要认识到`reallocarray`API 是 GNU 扩展；它可以在现代 Linux 上工作，但不应被视为可移植到其他操作系统。

最后，考虑一下这一点：一些项目对其数据对象有严格的对齐要求；使用`calloc`*和*(甚至通过`malloc(3)`分配所述对象)可能会导致细微的错误！ 在本章的后面部分，我们将使用`posix_memalign(3)`*和*API-它保证将内存分配给给定的字节对齐(指定字节数)！ 例如，要求内存分配与页边界完全对齐是相当常见的情况(回想一下，malloc 总是返回在 8 字节边界上对齐的内存区域)。

底线是：要小心。 阅读文档，思考并决定在特定情况下哪种 API 是合适的。 有关这方面的更多信息，请参阅关于 GitHub 存储库的*进一步阅读*部分。

# 超越基本要素

在本节中，我们将更深入地研究`malloc(3)`API 系列的动态内存管理。 了解这些方面，以及[第 5 章](05.html)、*Linux 内存问题*和[第 6 章](06.html)以及*内存问题调试工具*的内容，将对帮助开发人员有效调试常见的内存错误和问题大有裨益。

# 程序中断

当进程或线程需要内存时，它会调用一个动态内存例程-通常是`malloc(3)`或`calloc(3)`；该内存(通常)来自**堆段**。 如前所述，堆是一个动态段-它可以增长(朝向更高的虚拟地址)。 但显然，在任何给定的时间点，堆都有一个端点或顶部，超出这个端点或顶部，内存就不能被占用。 这个端点-堆上最后一个合法可引用的位置-称为**程序中断**。

# 使用 sbrk()API

那么，你怎么知道当前的节目中断在哪里呢？ 这很简单-当`sbrk(3)`API 与参数值零一起使用时，它返回当前的程序中断！ 让我们快速查找一下：

```sh
#include <unistd.h>
[...]
    printf("Current program break: %p\n", sbrk(0));
```

当前面的代码行运行时，您将看到一些示例输出，如下所示：

```sh
$ ./show_curbrk 
Current program break: 0x1bb4000
$ ./show_curbrk 
Current program break: 0x1e93000
$ ./show_curbrk 
Current program break: 0x1677000
$ 
```

它是有效的，但是为什么程序中断值一直在变化(似乎是随机的)？ 实际上，它*是*随机的：出于安全原因，Linux 将进程的虚拟地址空间的布局随机化(我们在[第 2 章](02.html)，*虚拟内存*中介绍了进程 VAS 布局)。 这种技术称为**地址空间布局随机化**(**ASLR**)。

让我们做更多的事情：我们将编写一个程序，如果在没有任何参数的情况下运行，它只显示当前的程序中断并退出(就像我们刚才看到的那样)；如果传递一个参数--要动态分配的内存字节数--它就会这样做(使用`malloc(3)`)，然后打印返回的堆地址以及原始和当前的程序中断。 在这里，您只能请求小于 128KB 的内容，原因稍后会清楚说明。

请参阅`ch4/show_curbrk.c`：

```sh
int main(int argc, char **argv)
{
    char *heap_ptr;
    size_t num = 2048;

    /* No params, just print the current break and exit */
    if (argc == 1) {
        printf("Current program break: %p\n", sbrk(0));
        exit(EXIT_SUCCESS);
    }

    /* If passed a param - the number of bytes of memory to
     * dynamically allocate - perform a dynamic alloc, then
     * print the heap address, the current break and exit.
     */
    num = strtoul(argv[1], 0, 10);
    if ((errno == ERANGE && num == ULONG_MAX)
         || (errno != 0 && num == 0))
         handle_err(EXIT_FAILURE, "strtoul(%s) failed!\n", argv[1]);
    if (num >= 128 * 1024)
         handle_err(EXIT_FAILURE, "%s: pl pass a value < 128 KB\n",
         argv[0]);

    printf("Original program break: %p ; ", sbrk(0));
    heap_ptr = malloc(num);
    if (!heap_ptr)
        handle_err(EXIT_FAILURE, "malloc failed!");
    printf("malloc(%lu) = %16p ; curr break = %16p\n",
            num, heap_ptr, sbrk(0));
    free(heap_ptr);

    exit(EXIT_SUCCESS);
}
```

让我们试试看：

```sh
$ make show_curbrk && ./show_curbrk [...]
Current program break: 0x1247000
$ ./show_curbrk 1024
Original program break: 0x1488000 ; malloc(1024) =        0x1488670 ; 
curr break =        0x14a9000
$ 
```

很有趣(见下图)！ 对于 1024 字节的分配，返回到该内存块开头的堆指针是`0x1488670`；也就是从原始中断开始的`0x1488670 - 0x1488000 = 0x670 = 1648`字节。

此外，新的中断值是`0x14a9000`，即`(0x14a9000 - 0x1488670 = 133520)`，距离新分配的块大约 130KB。 为什么只有 1KB 的分配，堆就会增长这么多？ 耐心；这一点以及更多内容将在下一节，即 malloc(3)的实际行为*中进行研究。* 同时，请参阅下图：

![](img/ecb44780-bcc1-448b-9677-5f2fa6f43f36.png)

Heap and the Program Break With respect to the preceding diagram:

```sh
Original program break = 0x1488000
heap_ptr               = 0x1488670
New program break      = 0x14a9000
```

请注意，`sbrk(2)`可用于递增或递减程序中断(通过向其传递整数参数)。 乍一看，这似乎是分配和释放动态内存的好方法；实际上，使用文档齐全且可移植的 glibc 实现(`malloc(3)`系列 API)总是更好的。

`sbrk` is a convenient library wrapper over the `brk(2)` system call.

# Malloc(3)的实际行为

普遍的共识是`malloc(3)`(以及`calloc(3)`和`realloc[array](3)`)从堆段获取其内存。 确实是这样，但深入挖掘会发现*并不总是*。 现代的 glibc`malloc(3)`引擎使用一些微妙的策略来最优化地利用可用内存区域和进程 VA--特别是在当今的 32 位系统上，它们正迅速成为一种相当稀缺的资源。

那么，它是如何工作的呢？ 该库使用预定义的*`MMAP_THRESHOLD`*变量*(默认情况下其值为 128 KB)来确定内存分配的位置。 假设我们使用 malloc(N)分配了第*n*字节的内存：

*   如果*n<MMAP_THRESHOLD，*使用堆段来分配请求的*n*字节

*   如果*n>=MMAP_THRESHOLD*，并且如果 n 字节在堆的空闲列表上不可用，则使用虚拟地址空间的任意空闲区域来满足请求的*n*字节分配

在第二种情况下，内存究竟是如何分配的？ 啊，`malloc(3)`内部调用`mmap(2)`-内存映射系统调用。 `mmap`系统调用非常通用。 在这种情况下，会使其保留调用进程的虚拟地址空间的 10n 字节的空闲区域！

Why use `mmap(2)`? The key reason is that mmap-ed memory can always be freed up (released back to the system) in an independent fashion whenever required; this is certainly not always the case with `free(3)`.

Of course, there are some downsides: `mmap` allocations can be expensive because, the memory is page-aligned (and could thus be wasteful), and the kernel zeroes out the memory region (this hurts performance).

The `mallopt(3)` man page (circa December 2016) also notes that nowadays, glibc uses a dynamic mmap threshold; initially, the value is the usual 128 KB, but if a large memory chunk between the current threshold and `DEFAULT_MMAP_THRESHOLD_MAX` is freed, the threshold is increased to become the size of the freed block.

# 代码示例 1-malloc(3)和程序中断

我们可以亲眼看到`malloc(3)`分配对堆和进程虚拟地址空间的影响，这很有趣，也很有教育意义。 查看以下代码示例的输出(源代码位于本书的 Git 存储库中)：

```sh
$ ./malloc_brk_test -h
Usage: ./malloc_brk_test [option | --help]
 option = 0 : show only mem pointers [default]
 option = 1 : opt 0 + show malloc stats as well
 option = 2 : opt 1 + perform larger alloc's (over MMAP_THRESHOLD)
 option = 3 : test segfault 1
 option = 4 : test segfault 2
-h | --help : show this help screen
$ 
```

这个应用程序中有几个场景在运行；现在让我们来看看其中的一些场景。

# 场景 1-默认选项

我们不带参数运行`malloc_brk_test`程序，即使用默认值：

```sh
$ ./malloc_brk_test
                              init_brk =        0x1c97000
 #: malloc(       n) =        heap_ptr           cur_brk   delta  
                                                      [cur_brk-init_brk]
 0: malloc(       8) =        0x1c97670         0x1cb8000 [135168]
 1: malloc(    4083) =        0x1c97690         0x1cb8000 [135168]
 2: malloc(       3) =        0x1c98690         0x1cb8000 [135168]
$ 
```

该进程打印出其初始程序中断值：`0x1c97000`。 然后它只分配 8 个字节(通过`malloc(3)`API)；在幕后，glibc 分配引擎调用*sbrk(2)*系统调用函数来增加堆；新的中断现在是`0x1cb8000`，比前一个中断增加了 135,168 字节=132KB(在前面代码的`delta`列中可以清楚地看到)！

为什么？ 优化：glibc 预计，在未来，该进程将需要更多的堆空间；它不会每次都调用系统调用(*`sbrk/brk`)*，而是执行一个较大的堆增长操作。 接下来的两个`malloc(3)`个 API(最左列中的数字 1 和 2)证明了这一点：我们分别分配了 4,083 和 3 个字节，您注意到了什么？ 程序中断不会*更改*-堆已经足够大，足以容纳请求。

# 场景 2-显示 malloc 统计数据的数据

这一次，我们传递了参数`1`，要求它也显示`malloc(3)`统计信息(使用`malloc_stats(3)`API 实现)：

```sh
$ ./malloc_brk_test 1
                              init_brk =   0x184e000
 #: malloc(       n) =        heap_ptr     cur_brk   delta   
                                            [cur_brk-init_brk]
 0: malloc(       8) =        0x184e670    0x186f000 [135168]
Arena 0:
system bytes     =     135168
in use bytes     =       1664
Total (incl. mmap):
system bytes     =     135168
in use bytes     =       1664
max mmap regions =          0
max mmap bytes   =          0

 1: malloc(    4083) =        0x184e690    0x186f000 [135168]
Arena 0:
system bytes     =     135168
in use bytes     =       5760
Total (incl. mmap):
system bytes     =     135168
in use bytes     =       5760
max mmap regions =          0
max mmap bytes   =          0

 2: malloc(       3) =        0x184f690    0x186f000 [135168]
Arena 0:
system bytes     =     135168
in use bytes     =       5792
Total (incl. mmap):
system bytes     =     135168
in use bytes     =       5792
max mmap regions =          0
max mmap bytes   =          0              
```

输出类似，除了程序调用有用的`malloc_stats(3)`API，该 API 查询`malloc(3)`状态信息并将其打印到`stderr`(顺便说一句，竞技场是由`malloc(3)`引擎在内部维护的分配区域)。 从此输出中，请注意：

*   可用的空闲内存(系统字节数)为 132 KB(执行微小的 8 字节`malloc(3)`后)
*   每次分配时使用的字节数都会增加，但系统字节数保持不变
*   `mmap`区域和`mmap`字节为零，因为没有发生基于 mmap 的分配。

# 情景 3-选择大笔拨款选项

这一次，我们传递参数`2`，要求程序执行更大的分配(大于`MMAP_THRESHOLD`)：

```sh
$ ./malloc_brk_test 2
                              init_brk =        0x2209000
 #: malloc(       n) =        heap_ptr           cur_brk   delta 
                                                      [cur_brk-init_brk]
[...]

 3: malloc(  136168) =   0x7f57288cd010         0x222a000 [135168]
Arena 0:
system bytes     =     135168
in use bytes     =       5792
Total (incl. mmap):
system bytes     =     274432
in use bytes     =     145056
max mmap regions =          1
max mmap bytes   =     139264

 4: malloc( 1048576) =   0x7f57287c7010         0x222a000 [135168]
Arena 0:
system bytes     =     135168
in use bytes     =       5792
Total (incl. mmap):
system bytes     =    1327104
in use bytes     =    1197728
max mmap regions =          2
max mmap bytes   =    1191936

$                     
```

(请注意，在前面的代码中，我们剪切了前两个小分配的输出，并且只显示了相关的大分配)。

现在，我们分配了 132KB(前面输出中的第 3 点)；需要注意的事项如下：

*   分配(#3 和#4)用于 132 KB 和 1 MB 内存-均高于`MMAP_THRESHOLD`(值 128 KB)
*   在这两个分配中，(Arena 0)堆*正在使用的字节*和(5792)的*没有*完全改变，这表明堆内存*没有*被使用
*   最大 mmap 区域和最大 mmap 字节数已更改为正值(从零)，表示使用了 mmap-ed 内存

剩下的几个场景将在稍后讨论。

# 释放的内存到哪里去了？

`free(3)`当然是一个库例程，因此当我们释放之前由一个动态分配例程分配的内存时，它不会被释放回系统，而是被释放到进程堆(当然，这是虚拟内存)，这是合情合理的。

但是，至少有两种情况可能不会发生：

*   如果通过*mmap*而不是通过堆段在内部满足分配，则会立即将其释放回系统
*   在现代的 glibc 上，如果释放的堆内存量非常大，则会触发至少部分内存块返回到操作系统。

# 高级特征

现在将介绍一些高级功能：

*   请求寻呼
*   锁定 RAM 中的内存
*   内存保护
*   使用*分配(3)*进行分配

# 早安

我们大多数人都知道，如果一个进程使用`malloc`动态分配内存(假设它不分配`ptr = malloc(8192) ;`)，那么假设成功，那么该进程现在将被分配 8KB 的物理 RAM。 这可能会让人大吃一惊，但在 Linux 等现代操作系统上，实际情况并非如此。

那么，到底是什么情况呢？ (在本书中，我们不深入研究内核级别的细节。 此外，您可能知道，操作系统分配器级别的内存粒度为*页*，通常为 4KB。)

It's not a good idea to assume anything when writing robust software. So, how can you correctly determine the page size on the OS? Use the `sysconf(3)` API; for example, `printf("page size = %ld\n", **sysconf(_SC_PAGESIZE)**);`, which outputs `page size = 4096`.

Alternatively, use the `getpagesize(2)` system call to retrieve the system page size. (Importantly, see [Chapter 19](19.html), *Troubleshooting and Best Practices*, covering similar points in the section *A Programmer’s Checklist: 7 Rules*).

实际上，nmalloc 所做的就是从进程 VAS 中保留虚拟内存页。

那么，该过程什么时候获得实际的物理页面呢？ 啊，当进程实际窥视或戳到页面中的任何字节时，实际上，当它对页面的任何字节进行任何类型的访问(试图读/写/执行它)时，进程都会陷入操作系统中-通过称为页面故障的硬件异常-在操作系统的故障处理程序中，如果一切正常，操作系统会为虚拟页面分配物理页帧。 这种高度优化的将物理内存分配给进程的方式称为**按需分页**-只有在实际需要时才按需物理分配分页！ 这与操作系统人员所说的内存或 VM 过量使用特性密切相关；是的，这是一个特性，而不是错误。

If you want to guarantee that physical page frames are allocated after a virtual allocation you can:

*   对所有页面中的所有字节执行`malloc(3)`，然后执行[T1
*   只需使用`calloc(3)`；它会将内存设置为零，从而使其在

On many implementations, the second method – using `calloc(3)` – is faster than the first.

实际上正是因为有了按需分页，我们才能编写一个应用程序来释放 malloc 的大量内存；只要进程不试图读取、写入或执行所分配区域的任何(虚拟)页面中的任何字节，它就会工作。 显然，有许多现实世界中的应用程序设计得相当糟糕，它们做的正是这种事情：通过`malloc(3)`分配大量内存，以防万一我们需要它。 请求分页是操作系统的一种对冲手段，可以避免浪费大量的物理内存，而这些内存在实践中几乎不会被使用。

当然，作为一个敏锐的读者，你会意识到每一个好处都可能有坏处。 在这种情况下，可以想象这可能会发生在多个进程同时执行大内存分配的情况下。 如果它们都分配了很大一部分虚拟内存，然后想要几乎同时物理地认领这些页面，这将给操作系统带来巨大的内存压力！ 你猜怎么着，操作系统绝对不能保证它会成功地为每个人提供服务。 事实上，在最坏的情况下，Linux 操作系统将运行物理 RAM 不足，以至于它必须调用一些有争议的组件-**内存不足**(**OOM**)Killer，它的工作是识别占用内存的进程并杀死它及其后代，从而回收内存并保持系统正常运行。 让你想起了黑手党，嗯。

同样，`malloc(3)`上的手册页清楚地记录了以下内容：

```sh
By  default, Linux follows an optimistic memory allocation strategy. This means that when malloc() returns non-NULL there is no guarantee that the memory really is available.  In case it turns out that the system is out of memory, one or more processes will be killed by the OOM  killer.
[...]
```

如果感兴趣，请参考 GitHub 存储库的*进一步阅读*部分中的参考资料进行更深入的挖掘。

# 是不是住院医生？

既然我们清楚地理解了*malloc*和 Friends 分配的页面是虚拟的，并且不能保证得到物理帧的支持(至少一开始是这样)，那么假设我们有一个指向(虚拟)内存区域的指针，并且我们知道它的长度。 我们现在想知道相应的页面是否在 RAM 中，也就是说，它们是否是常驻页面。

原来有一个可用的系统调用正好提供了这个信息：`mincore(2)`。

The `mincore(2)` system call is pronounced m-in-core, not min-core. Co*re *is an old word used to describe physical memory.

让我们来看一下以下代码：

```sh
#include <unistd.h>
#include <sys/mman.h>

int mincore(void *addr, size_t length, unsigned char *vec);
```

给定起始虚拟地址和长度后，`mincore(2)`填充第三个参数，即向量数组。 调用成功返回后，对于向量数组的每个字节，如果设置了 LSB(最低有效位)，则表示对应的页*驻留在*(在 RAM 中)，否则不驻留(可能未分配或在交换中)。

可通过`mincore(2)`手册页[https://linux.die.net/man/2/mincore](https://linux.die.net/man/2/mincore)获取用法详细信息。

当然，您应该意识到，在页面驻留上返回的信息仅仅是内存页面状态在该时间点的快照：在我们的控制下，它可能会改变，也就是说，它本质上是(或可能是)非常短暂的。

# 锁定内存

我们知道，在基于虚拟内存的操作系统(如 Linux)上，用户模式页面可以在任何时间点进行交换；Linux 内核内存管理代码做出这些决定。 对于常规应用程序进程，这应该无关紧要：每当它试图访问(读、写或执行)页面内容时，内核都会将其分页到 RAM 中，并允许它像什么都没有发生一样使用它。 这种处理通常称为*服务页面错误*(还有更多内容，但就本讨论而言，这就足够了)，并且对于用户模式应用程序进程是完全透明的。

但是，在某些情况下，不希望分页的内存页从 RAM 写到交换，反之亦然：

*   实时应用程序
*   密码学(安全)应用

在实时应用程序中，关键因素(至少在其关键代码路径内)是确定性*-*，即铁板一块地保证工作将花费一定的最坏情况下的时间，而不是更多，无论系统上的负载有多大。

假设实时进程正在执行关键代码路径，此时必须从交换分区调入数据页-引入的延迟(延迟)可能会破坏应用程序的特性，导致惨淡的故障(或更糟)。 在这些情况下，我们开发人员需要一种方法来保证所述内存页面可以保证驻留在 RAM 中，从而避免任何页面故障。

在某些类型的安全应用程序中，它们可能会在内存中存储一些秘密(密码、密钥)；如果包含这些秘密的内存页被写出到磁盘(交换)，那么在应用程序退出之后，它总是有可能留在磁盘上-导致所谓的信息泄漏，这是攻击者正等待着攻击的错误！ 这里，再次强调，现在需要的是保证这些页面不会被换出。

进入`mlock(2)`(和 Friends：*mlock2*和*mlockall*)系统调用；这些 API 的明确目的是锁定调用进程的虚拟地址空间内的内存页。 让我们弄清楚如何使用`mlock(2)`。 以下是它的签名：

`int mlock(const void *addr, size_t len);`

第一个参数`addr`是指向要锁定的(虚拟)内存区域的指针；第二个参数`len`是要锁定到 RAM 中的字节数。 作为一个简单的示例，请看下面的代码(在这里，为了保持代码的易读性，我们没有显示错误检查代码；在实际的应用程序中，请这样做！)

```sh
long pgsz = sysconf(_SC_PAGESIZE);
size_t len = 3*pgsz;

void *ptr = malloc(len);

[...]       // initialize the memory, etc

// Lock it!
if (mlock(ptr, len) != 0) {
     // mlock failed, handle it
     return ...;
}

[...]   /* use the memory, confident it is resident in RAM & will stay  
           there until unlocked */

munlock(ptr, len);   // it's now unlocked, can be swapped
```

# 限制和特权

特权进程，无论是以*root*身份运行，还是通过设置`CAP_IPC_LOCK`能力位以锁定内存(我们将在各自的章节中详细描述进程凭据和功能-[第 7 章](07.html)、*进程凭据*和[第 8 章](08.html)、*进程能力*)，都可以锁定无限大的内存。

从 Linux 2.6.9 开始，对于非特权进程，它受到最大`RLIMIT_MEMLOCK`个软资源限制(通常不会设置得很高)。 下面是一个关于 x86_64 Fedora 盒(以及 Ubuntu)的示例：

```sh
$ prlimit | grep MEMLOCK
MEMLOCK   max locked-in-memory address space   65536   65536 bytes
$ 
```

它只有 64KB(默认情况下，在嵌入式 ARM Linux 上也是如此)。

At the time of writing this book, on a recent *Fedora 28* distro running on x86_64, the resource limit for max locked memory seems to have been amped up to 16 MB! The following *prlimit(1)* outputshows just this:  

`$ prlimit | grep MEMLOCK`
`MEMLOCK     max locked-in-memory address space     16777216  16777216 bytes`
`$`

不过，请稍等片刻；在使用 mlock(2)时，POSIX 标准要求`addr`与页边界对齐(即，如果获取内存起始地址并将其除以系统页大小，则余数将为零，即`(addr % pgsz) == 0`。 您可以使用`posix_memalign(3)`API 来保证这一点；因此，我们可以稍微更改代码以适应此对齐要求：

请参阅以下内容(`ch4/mlock_try.c`)：

```sh
[...]
#define CMD_MAX  256
static void disp_locked_mem(void)
{
    char *cmd = malloc(CMD_MAX);
    if (!cmd)
        FATAL("malloc(%zu) failed\n", CMD_MAX);
    snprintf(cmd, CMD_MAX-1, "grep Lck /proc/%d/status", getpid());
    system(cmd);
    free(cmd);
}

static void try_mlock(const char *cpgs)
{
    size_t num_pg = atol(cpgs);
    const long pgsz = sysconf(_SC_PAGESIZE);
    void *ptr= NULL;
    size_t len;

    len = num_pg * pgsz;
    if (len >= LONG_MAX)
        FATAL("too many bytes to alloc (%zu), aborting now\n", len);

/* ptr = malloc(len); */
/* Don't use the malloc; POSIX wants page-aligned memory for mlock */
    posix_memalign(&ptr, pgsz, len);
    if (!ptr)
        FATAL("posix_memalign(for %zu bytes) failed\n", len);

    /* Lock the memory region! */
    if (mlock(ptr, len)) {
        free(ptr);
        FATAL("mlock failed\n");
    }
    printf("Locked %zu bytes from address %p\n", len, ptr);
    memset(ptr, 'L', len);
    disp_locked_mem();
    sleep(1);

    /* Now unlock it.. */
    if (munlock(ptr, len)) {
        free(ptr);
        FATAL("munlock failed\n");
    }
    printf("unlocked..\n");
    free(ptr);
}

int main(int argc, char **argv)
{
    if (argc < 2) {
        fprintf(stderr, "Usage: %s pages-to-alloc\n", argv[0]);
        exit(EXIT_FAILURE);
    }
    disp_locked_mem();
    try_mlock(argv[1]);
    exit (EXIT_SUCCESS);
}
```

让我们试一试吧：

```sh
$ ./mlock_try Usage: ./mlock_try pages-to-alloc $ ./mlock_try 1 VmLck:           0 kB
Locked 4096 bytes from address 0x1a6e000
VmLck:           4 kB
unlocked.. $ ./mlock_try 32 VmLck:           0 kB mlock_try.c:try_mlock:79: mlock failed
perror says: Cannot allocate memory
$ 
$ ./mlock_try 15 VmLck:           0 kB
Locked 61440 bytes from address 0x842000
VmLck:          60 kB
unlocked.. $ sudo ./mlock_try 32 [sudo] password for <user>: xxx 
VmLck:           0 kB
Locked 131072 bytes from address 0x7f6b478db000
VmLck:         128 kB
unlocked..
$ prlimit | grep MEMLOCK MEMLOCK    max locked-in-memory address space     65536     65536 bytes
$ 
```

Notice, in the successful cases, the address returned by `posix_memalign(3)`*;* it's on a page boundary. We can quickly tell by looking at the last three digits (from the right) of the address – if they are all zeroes, it's cleanly divisible by page size and thus on a page boundary. This is because the page size is usually 4,096 bytes, and 4096 decimal = 0x1000 hex!

我们请求 32 个页面；分配成功，但*mlock*失败，因为 32 个页面=32*4K=128KB；锁定内存的资源限制仅为 64KB。 但是，当我们*sudo*它(因此以 root 访问运行)时，它可以工作。

# 锁定所有页面

*mlock*基本上允许我们告诉操作系统将一定范围的内存锁定到 RAM 中。 然而，在某些实际情况下，我们无法准确预测需要预先驻留的内存页面(实时应用程序可能需要各种或所有内存页面始终驻留)。

为了解决这个棘手的问题，存在另一个系统调用--**mlockall(2)*；正如您可以猜到的那样，它允许您锁定所有进程内存页：

` int mlockall(int flags);`

如果成功(请记住，*mlockall*与*mlock*具有相同的权限限制)，则保证进程的所有内存页*-*(如文本、数据段、库页、堆栈和共享内存段)将一直驻留在 RAM 中，直到解锁。

*标志*参数为应用程序开发人员提供进一步的控制；它可以是以下各项的位或：

*   `MCL_CURRENT`
*   `MCL_FUTURE`
*   `MCL_ONFAULT (Linux 4.4 onward)`

使用`MCL_CURRENT`要求操作系统将调用进程的 VAS 中的所有当前页面锁定到内存中。

但是，如果您在初始化时发出*mlockall(2)和*系统调用，但是实时进程将在 5 分钟后执行一个*malloc*(比方说 200KB)，该怎么办呢？ 我们需要保证这 200KB 的内存(即 50 个页面，给定 4KB 的页面大小)始终驻留在 RAM 中(否则，实时应用程序将因未来可能出现的页面错误而遭受太大的延迟)。 这就是`MCL_FUTURE`标志的目的：它保证将来成为调用进程的 VAS 一部分的内存页将一直驻留在内存中，直到解锁。

我们在*请求分页*一节中了解到，执行*malloc*只不过是保留虚拟内存，而不是物理内存。 例如，如果(非实时)应用程序执行了相当大的兆字节(即 512 页)的分配，我们知道只保留了 512 个虚拟页，而物理页帧并没有实际分配-它们将在按需时出错。因此，典型的实时应用程序需要以某种方式保证，一旦出错，这 512 页将保持锁定(驻留)在 RAM 中。 使用`MCL_ONFAULT`标志来实现这一点。

此标志必须与`MCL_CURRENT`和/或`MCL_FUTURE`标志一起使用。 其想法是，物理内存消耗仍然非常高效(因为在执行*malloc*时没有进行物理分配)，但是，一旦应用程序开始接触虚拟页面(即，读取、写入或执行页面内的数据或代码)，物理页帧就会出错，然后它们将被锁定。 换句话说，我们不会预错记忆，因此我们两全其美。

问题的另一面是，完成后，应用程序可以通过发出对应的 API：**munlockall(2)*来解锁所有内存页。

# 内存保护

比方说，一个应用程序动态分配四页内存。 默认情况下，该内存是可读和可写的；我们将其称为页面上的*内存保护*。

如果应用程序开发人员可以按页动态修改内存保护，那不是很好吗？ 例如，使用默认保护保留第一页，将第二页*设为只读*，将第三页*设为读取+执行*，而在第四页上，不允许任何类型的访问(也许是保护页？)。

嗯，这个特性正是设计`mprotect(2)`系统调用的目的。 让我们深入研究一下如何利用它来完成所有这些工作。 以下是它的签名：

```sh
#include <sys/mman.h>
int mprotect(void *addr, size_t len, int prot);
```

实际上非常简单：从(虚拟)地址开始，对`len`字节使用`addr,`(即从`addr`到`addr+len-1`)，应用由*prot*位掩码指定的内存保护。 因为*mProtection*的粒度是一个页面，所以第一个参数：*addr*应该是页面对齐的(在页面边界上；回想一下，这也是`mlock[all](2)`所期望的)。

第三个参数-`prot`是指定实际保护的位置；它是位掩码，可以只是`PROT_NONE`位，也可以是余数的逐位 OR：

| **保护位** | **存储器保护的含义** |
| `PROT_NONE` | 不允许访问该页面 |
| `PROT_READ` | 页面上允许的读取 |
| `PROT_WRITE` | 页面上允许的写入 |
| `PROT_EXEC` | 执行页面上允许的访问权限 |

Within the man page on *mprotect(2),* there are several other rather arcane protection bits and useful information under the NOTES section. If required (or just curious), read about it here: [http://man7.org/linux/man-pages/man2/mprotect.2.html](http://man7.org/linux/man-pages/man2/mprotect.2.html)*.*

# 内存保护工具-提供代码示例

让我们考虑一个示例程序，其中进程动态分配四页内存，并希望对它们进行设置，以使每页的内存保护如下表所示：

| **页码** | **第 0 页** | **第 1 页** | **第 2 页** | **第 3 页** |
| 保护位 | `rw-` | `r--` | `rwx` | `---` |

代码的相关部分如下所示：

首先，*main*函数使用`posix_memalign(3)`API 动态分配页面对齐的内存(四页)，然后依次调用内存保护和内存测试函数：

```sh
[...]
    /* Don't use the malloc; POSIX wants page-aligned memory for mprotect(2) */
    posix_memalign(&ptr, gPgsz, 4*gPgsz);
    if (!ptr)
        FATAL("posix_memalign(for %zu bytes) failed\n", 4*gPgsz);
    protect_mem(ptr);
    test_mem(ptr, atoi(argv[1]));
[...]
```

内存保护功能如下：

```sh
int okornot[4];
static void protect_mem(void *ptr)
{
    int i;
    u64 start_off=0;
    char str_prots[][128] = {"PROT_READ|PROT_WRITE", "PROT_READ",
                             "PROT_WRITE|PROT_EXEC", "PROT_NONE"};
    int prots[4] = {PROT_READ|PROT_WRITE, PROT_READ,
 PROT_WRITE|PROT_EXEC, PROT_NONE};

    printf("%s():\n", __FUNCTION__);
    memset(okornot, 0, sizeof(okornot));

    /* Loop over each page, setting protections as required */
    for (i=0; i<4; i++) {
        start_off = (u64)ptr+(i*gPgsz);
        printf("page %d: protections: %30s: "
               "range [0x%llx:0x%llx]\n",
               i, str_prots[i], start_off, start_off+gPgsz-1);

        if (mprotect((void *)start_off, gPgsz, prots[i]) == -1)
            WARN("mprotect(%s) failed\n", str_prots[i]);
        else
            okornot[i] = 1;
    }
}
```

设置存储器保护后，我们让`main()`*和*函数调用存储器测试函数`test_mem`。 第二个参数确定我们是否将尝试在只读存储器上写入(我们需要对页面 1 执行此测试用例，因为它是只读保护的)：

```sh
static void test_mem(void *ptr, int write_on_ro_mem)
{
    int byte = random() % gPgsz;
    char *start_off;

    printf("\n----- %s() -----\n", __FUNCTION__);

    /* Page 0 : rw [default] mem protection */
    if (okornot[0] == 1) {
        start_off = (char *)ptr + 0*gPgsz + byte;
        TEST_WRITE(0, start_off, 'a');
        TEST_READ(0, start_off);
    } else
        printf("*** Page 0 : skipping tests as memprot failed...\n");

    /* Page 1 : ro mem protection */
    if (okornot[1] == 1) {
        start_off = (char *)ptr + 1*gPgsz + byte;
        TEST_READ(1, start_off);
        if (write_on_ro_mem == 1) {
            TEST_WRITE(1, start_off, 'b');
        }
    } else
        printf("*** Page 1 : skipping tests as memprot failed...\n");

    /* Page 2 : RWX mem protection */
    if (okornot[2] == 1) {
        start_off = (char *)ptr + 2*gPgsz + byte;
        TEST_READ(2, start_off);
        TEST_WRITE(2, start_off, 'c');
    } else
        printf("*** Page 2 : skipping tests as memprot failed...\n");

    /* Page 3 : 'NONE' mem protection */
    if (okornot[3] == 1) {
        start_off = (char *)ptr + 3*gPgsz + byte;
        TEST_READ(3, start_off);
        TEST_WRITE(3, start_off, 'd');
    } else
        printf("*** Page 3 : skipping tests as memprot failed...\n");
}
```

在尝试测试它之前，我们检查页面是否确实受到了`mprotect`*和*调用的保护(通过我们简单的`okornot[]`*和*数组)。 此外，为了提高可读性，我们构建了简单的`TEST_READ`和`TEST_WRITE`宏：

```sh
#define TEST_READ(pgnum, addr) do { \
    printf("page %d: reading: byte @ 0x%llx is ", \
    pgnum, (u64)addr); \
    fflush(stdout); \
    printf(" %x", *addr); \
    printf(" [OK]\n"); \
} while (0)

#define TEST_WRITE(pgnum, addr, byte) do { \
    printf("page %d: writing: byte '%c' to address 0x%llx now ...", \
            pgnum, byte, (u64)addr); \
    fflush(stdout); \
    *addr = byte; \
    printf(" [OK]\n"); \
} while (0)
```

如果该进程违反任何内存保护，操作系统将通过通常的*段故障*机制(在[第 12 章](12.html)*、*和*信令第 II 部分*中详细说明)立即终止该进程。

让我们在`memprot`程序上执行一些测试运行；首先(原因很快就会清楚)，我们将在通用的 Ubuntu Linux 机器上尝试它，然后在 Fedora 系统上，最后在(仿真的)ARM-32 平台上！

案例#1.1：标准 Ubuntu 18.04 LTS 上的`memprot`*和*程序，参数为 0**和**(输出经过重新格式化以提高可读性)：

```sh
$ cat /etc/issue Ubuntu 18.04 LTS \n \l $ uname -r 4.15.0-23-generic $ 

$ ./memprot
Usage: ./memprot test-write-to-ro-mem [0|1]
$ ./memprot 0
----- protect_mem() -----
page 0: protections: PROT_READ|PROT_WRITE: range [0x55796ccd5000:0x55796ccd5fff]
page 1: protections: PROT_READ: range [0x55796ccd6000:0x55796ccd6fff]
page 2: protections: PROT_READ|PROT_WRITE|PROT_EXEC: range [0x55796ccd7000:0x55796ccd7fff]
page 3: protections: PROT_NONE: range [0x55796ccd8000:0x55796ccd8fff]

----- test_mem() -----
page 0: writing: byte 'a' to address 0x55796ccd5567 now ... [OK]
page 0: reading: byte @ 0x55796ccd5567 is 61 [OK]
page 1: reading: byte @ 0x55796ccd6567 is 0 [OK]
page 2: reading: byte @ 0x55796ccd7567 is 0 [OK]
page 2: writing: byte 'c' to address 0x55796ccd7567 now ... [OK]
page 3: reading: byte @ 0x55796ccd8567 is Segmentation fault
$ 
```

好的，因此，`memprot`的参数是`0`或`1`；`0`表示我们不执行写到只读存储器测试，而`1`表示我们执行。 在这里，我们使用`0`参数运行它。

在前面的输出中需要注意的事项如下：

*   `protect_mem()`函数按页设置内存保护。 我们分配了 4 个页面，因此我们循环了 4 次，在每次循环迭代`i`时，在第 i 个内存页上执行`mprotect(2)`。
*   正如您在代码中清楚地看到的那样，它是在每次循环迭代中以这种方式完成的
    *   第`0 : rw-`页：将页面保护设置为`PROT_READ | PROT_WRITE`
    *   第`1 : r--`页：将页面保护设置为`PROT_READ`
    *   第`2 : rwx`页：将页面保护设置为`PROT_READ| PROT_WRITE | PROT_EXEC`
    *   页面`3 : ---`：将页面保护设置为`PROT_NONE`，即使页面不可访问

*   在前面的输出中，在*mProtection*之后显示的输出格式如下：

    `page <#>: protections: <PROT_xx|[...]> range [<start_addr>:<end_addr>]`
*   一切都很顺利；这四页根据需要得到了新的保护。
*   接下来，调用函数`test_mem()`，该函数测试每个页面的保护(页面的内存保护显示在通常的[`rwx`]格式的方括号中)：

    *   在第 0 页[Default：`rw-`]上：它在页面内写入和读取一个随机字节
    *   在页面 1[`r--`]上：它读取页面内的随机字节，如果用户将参数传递为`1`，它会尝试写入该页面内的随机字节(这里不是这种情况，但在下面的情况下会是这样)
    *   在第 2 页[`rwx`]上：不出所料，这里的随机字节读写成功
    *   在第 3 页[`---`]上：它尝试读写页内的随机字节。
        *   第一次访问失败-读取*段*段失败，出现*段错误*；这当然是意料之中的，因为该页没有任何权限(我们在这种情况下复制输出)：`**page 3: reading: byte @ 0x55796ccd8567 is Segmentation fault**`
*   总而言之，在参数为 0`0`的情况下，第 0、1 和 2 页上的测试用例成功；正如预期的那样，第 3 页上的任何访问都会导致操作系统终止进程(通过分段违规信号)。

案例#1.2：标准 Ubuntu 18.04 LTS 上的`memprot`脚本程序，参数为 1(输出经过重新格式化以提高可读性)。

现在让我们重新运行该程序，并将参数设置为`1`，从而尝试写入*只读*页`1`：

```sh
$ ./memprot 1 ----- protect_mem() -----
page 0: protections: PROT_READ|PROT_WRITE: range [0x564d74f2d000:0x564d74f2dfff]
page 1: protections: PROT_READ: range [0x564d74f2e000:0x564d74f2efff]
page 2: protections: PROT_READ|PROT_WRITE|PROT_EXEC: range [0x564d74f2f000:0x564d74f2ffff]
page 3: protections: PROT_NONE: range [0x564d74f30000:0x564d74f30fff]

----- test_mem() -----
page 0: writing: byte 'a' to address 0x564d74f2d567 now ... [OK]
page 0: reading: byte @ 0x564d74f2d567 is 61 [OK]
page 1: reading: byte @ 0x564d74f2e567 is 0 [OK]
page 1: writing: byte 'b' to address 0x564d74f2e567 now ...Segmentation fault
$ 
```

实际上，不出所料，当它违反只读页面权限时，它会*分段*。

案例 2：标准*Fedora 28*系统上的`memprot`*和*程序。

在撰写本书时，最新也是最好的*Fedora*工作站发行版是 28 版：

```sh
$ lsb_release -a
LSB Version: :core-4.1-amd64:core-4.1-noarch
Distributor ID: Fedora
Description: Fedora release 28 (Twenty Eight)
Release: 28
Codename: TwentyEight
$ uname -r
4.16.13-300.fc28.x86_64
$ 
```

我们在此标准*Fedora 28*工作站系统上构建并运行我们的`memprot`*和*程序(将`0`作为参数传递-意味着我们不会尝试写入只读存储器页)：

```sh
$ ./memprot 0
----- protect_mem() -----
page 0: protections: PROT_READ|PROT_WRITE: range [0x15d8000:0x15d8fff]
page 1: protections: PROT_READ: range [0x15d9000:0x15d9fff]
page 2: protections: PROT_READ|PROT_WRITE|PROT_EXEC: range [0x15da000:0x15dafff]
!WARNING! memprot.c:protect_mem:112:  
            mprotect(PROT_READ|PROT_WRITE|PROT_EXEC) failed
perror says: Permission denied
page 3: protections: PROT_NONE: range [0x15db000:0x15dbfff]

----- test_mem() -----
page 0: writing: byte 'a' to address 0x15d8567 now ... [OK]
page 0: reading: byte @ 0x15d8567 is 61 [OK]
page 1: reading: byte @ 0x15d9567 is 0 [OK]
*** Page 2 : skipping tests as memprot failed...
page 3: reading: byte @ 0x15db567 is Segmentation fault (core dumped)
$ 
```

我们如何解释前面的输出？ 以下是对此的解释：

*   页面 0、1 和 3 一切正常：*mProtect*API 成功地设置了页面的保护，完全如图所示

*   但是，当我们使用`PROT_READ | PROT_WRITE | PROT_EXEC`属性尝试第 2 页上的`mprotect(2)`系统调用时，我们会收到一个失败(以及一条错误的*警告*消息)。-*为什么？* 

    *   通常的操作系统安全是**自主访问控制**层(**DAC**)。许多现代 Linux 发行版，包括 Fedora，都有一个强大的安全功能--操作系统内部的额外一层安全--**强制访问控制**层(**MAC**)。所有这些都在 Linux 上实现为**Linux 安全模块**(**LSM[T11。 流行的 LSM 包括美国国家安全局的 SELinux(安全增强型 Linux)、AppArmor、Smack、Tomoyo 和 Yama。**
    *   Feddora 使用 SELinux，而 Ubuntu 变体倾向于使用 AppArmor。 无论哪种情况，通常都是这些 LSM 在违反安全策略时导致用户发出的系统调用失败。 这正是我们在第三个页面(当试图将页面保护设置为[`rwx`]时)上的 monmProtection(2)系统调用所发生的情况！
    *   作为一种快速概念验证，现在只是让它正常工作，我们暂时**禁用***SELinux*，然后重试：

        ```sh
        $ getenforce 
        Enforcing
        $ setenforce 
        usage: setenforce [ Enforcing | Permissive | 1 | 0 ]
        $ sudo setenforce 0
        [sudo] password for <username>: xxx
        $ getenforce 
        Permissive
        $ 
        ```

        *SELinux*现在处于允许模式；请重试该应用程序：

```sh
$ ./memprot 0
----- protect_mem() -----
page 0: protections: PROT_READ|PROT_WRITE: range [0x118e000:0x118efff]
page 1: protections: PROT_READ: range [0x118f000:0x118ffff]
page 2: protections: PROT_READ|PROT_WRITE|PROT_EXEC: range [0x1190000:0x1190fff]
page 3: protections: PROT_NONE: range [0x1191000:0x1191fff]

----- test_mem() -----
page 0: writing: byte 'a' to address 0x118e567 now ... [OK]
page 0: reading: byte @ 0x118e567 is 61 [OK]
page 1: reading: byte @ 0x118f567 is 0 [OK]
page 2: reading: byte @ 0x1190567 is 0 [OK]
page 2: writing: byte 'c' to address 0x1190567 now ... [OK]
page 3: reading: byte @ 0x1191567 is Segmentation fault (core dumped)
$ 
```

现在，它像预期的那样工作了！ 不要忘记重新启用 LSM：

```sh
$ sudo setenforce 1
$ getenforce 
Enforcing
$ 
```

# 一个备用的 LSM 日志，Ftrace

(如果您对此不感兴趣，请随意跳过这一节)。敏锐的读者可能会想：如何才能意识到最终导致系统调用失败的是操作系统安全层(LSM)？一般来说，有两种方法：检查给定的 LSM 日志，或者使用内核的`Ftrace`功能。第一种方法更简单，但第二种方法可以让我们深入了解操作系统级别的情况。

# LSM 日志

现代 Linux 系统使用功能强大的 csystemd 框架进行进程初始化、日志记录等。 该日志记录设施称为日志记录工具，可通过` journalctl(1)`日志实用程序进行访问。 我们使用它来验证是否确实是 SELinux LSM 导致了该问题：

```sh
$ journalctl --boot | grep memprot
[...]
<timestamp> <host> python3[31861]: SELinux is preventing memprot from using the execheap access on a process.
 If you do not think memprot should need to map heap memory that is both writable and executable.
 If you believe that memprot should be allowed execheap access on processes labeled unconfined_t by default.
 # ausearch -c 'memprot' --raw | audit2allow -M my-memprot
 # semodule -X 300 -i my-memprot.pp
```

它甚至向我们展示了我们如何才能允许访问。

# Ftrace

Linux 内核有一个非常强大的内置跟踪机制(好吧，它就是其中之一)：跟踪*Ftrace*。 使用`ftrace`，您可以验证确实是*LSM*代码在遵守其安全策略的同时，导致用户空间发出的系统调用返回失败。 我运行了一个跟踪(使用`ftrace`)：

![](img/82dbe338-ea14-4d08-b883-845de7a88777.png)

ftrace output snippet

函数`SyS_mprotect`是在内核*和*内的*mProtection(2)和*系统调用的结果；`security_file_mprotect`是指向实际 SELinux 函数的 LSM 钩子函数：`selinux_file_mprotect`；显然，它无法访问。

有趣的是，Ubuntu 18.04 LTS 还使用了一个新的 LSM 版本--AppArmor。 但是，它似乎没有配置为捕获这种*写+执行*(堆)页面保护情况。

当然，这些主题(LSM、ftrace)超出了本书的范围。 对于好奇的读者(我们喜欢的那类)，请在 GitHub 存储库的*进一步阅读*部分查看有关*LSM*和*Ftrace*的更多信息。

# 在 ARM-32 上运行 memprot 程序的实验

作为一个有趣的实验，我们将为**ARM 系统**交叉编译前面的*memprot*程序。 我使用了一种不需要实际硬件的便捷方法：使用功能强大的**自由开源软件**(**FOSS**)**Quick Emulator**(**QEMU**)项目，模拟 ARM 多功能 Express Cortex-A9 平台！

交叉编译代码确实很简单：请注意，现在在我们的`Makefile`*中有一个`CROSS_COMPILE`变量；*它是交叉编译器前缀-标识工具链的前缀字符串(所有工具都通用)。它实际上被添加到`CC`(对于`gcc`，或者`CL`对于 clang)变量的前缀，该变量是用于构建目标的编译器。 不幸的是，关于交叉编译和根文件系统构建的更多细节超出了本书的范围；有关帮助，请参阅本例输出后面的*提示*。 此外，为了简单起见，我们将使用直接方法-`Makefile`中 ARM 版本的单独目标。 让我们来看看这份报告的相关部分`Makefile`：

```sh
$ cat Makefile
[...]
CROSS_COMPILE=arm-linux-gnueabihf-
CC=gcc
CCARM=${CROSS_COMPILE}gcc
[...]
common_arm.o: ../common.c ../common.h
    ${CCARM} ${CFLAGS} -c ../common.c -o common_arm.o
memprot_arm: common_arm.o memprot_arm.o
    ${CCARM} ${CFLAGS} -o memprot_arm memprot_arm.c common_arm.o
[...]
```

因此，如下所示，我们交叉编译`memprot_arm`测试程序：

```sh
$ make clean [...] $ make memprot_arm
arm-linux-gnueabihf-gcc -Wall -c ../common.c -o common_arm.o gcc -Wall -c -o memprot_arm.o memprot_arm.c arm-linux-gnueabihf-gcc -Wall -o memprot_arm memprot_arm.c common_arm.o $ file ./memprot_arm ./memprot_arm: ELF 32-bit LSB executable, ARM, EABI5 version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux-armhf.so.3, for GNU/Linux 3.2.0, BuildID[sha1]=3c720<...>, with debug_info, not stripped $ 
```

啊哈，它生成了一个 ARM 可执行文件！ 我们将其复制到嵌入式根文件系统，引导(模拟的)ARM 板，并试用：

```sh
$ qemu-system-arm -m 512 -M vexpress-a9 \
   -kernel <..img/zImage \
   -drive file=<..img/rfs.img,if=sd,format=raw \
   -append \
    "console=ttyAMA0 rootfstype=ext4 root=/dev/mmcblk0 init=/sbin/init " \
   -nographic -dtb <..img/vexpress-v2p-ca9.dtb

[...]
Booting Linux on physical CPU 0x0
Linux version 4.9.1-crk (xxx@yyy) (gcc version 4.8.3 20140320 (prerelease) (Sourcery CodeBench Lite 2014.05-29) ) #16 SMP Wed Jan 24 10:09:17 IST 2018
CPU: ARMv7 Processor [410fc090] revision 0 (ARMv7), cr=10c5387d
CPU: PIPT / VIPT nonaliasing data cache, VIPT nonaliasing instruction cache

[...]

smsc911x 4e000000.ethernet eth0: SMSC911x/921x identified at 0xa1290000, IRQ: 31
/bin/sh: can't access tty; job control turned off
ARM / $ 
```

我们使用(模拟的)ARM-32 系统提示符；让我们尝试运行我们的程序：

```sh
ARM # ./memprot_arm Usage: ./memprot_arm test-write-to-ro-mem [0|1] ARM # ./memprot_arm 0 ----- protect_mem() -----
page 0: protections: PROT_READ|PROT_WRITE: range [0x24000, 0x24fff]
page 1: protections: PROT_READ: range [0x25000, 0x25fff]
page 2: protections: PROT_READ|PROT_WRITE|PROT_EXEC: range [0x26000, 0x26fff]
page 3: protections: PROT_NONE: range [0x27000, 0x27fff]

----- test_mem() -----
page 0: writing: byte 'a' to address 0x24567 now ... [OK]
page 0: reading: byte @ 0x24567 is 61 [OK]
page 1: reading: byte @ 0x25567 is 0 [OK]
page 2: reading: byte @ 0x26567 is 0 [OK]
page 2: writing: byte 'c' to address 0x26567 now ... [OK]
page 3: reading: byte @ 0x27567 is Segmentation fault (core dumped)
ARM # 
```

读者会注意到，与我们之前在 x86_64 系统上运行的*Fedora 28*发行版不同，我们尝试将第 2 页的内存保护设置为[`rwx`]的第 2 页测试用例(以粗体突出显示)确实成功了！当然，没有安装 LSM。

If you would like to try similar experiments, running code on an emulated ARM-32, consider using the **Simple Embedded ARM Linux System** (**SEALS**) project, again pure open source, to easily build a very simple, yet working, ARM/Linux-embedded system: [https://github.com/kaiwan/seals](https://github.com/kaiwan/seals).

通过强大的`mmap(2)`系统调用(我们在[第 18 章](18.html)，*高级文件 I/O*中介绍了关于文件 I/O 的`mmap(2)`)，可以实现类似的内存保护-在一系列内存设置保护属性(rwx 或 None)。

# 内存保护密钥-提供简短说明

最近的英特尔 64 位处理器提供了一种称为**内存保护密钥**(**MPK**)的功能。 简而言之，mpk(或在 Linux 上称为*pkeys*)也允许用户空间设置页面粒度的权限。 那么，如果它与*mProtection*或*mmap*做同样的事情，会带来什么好处呢？ 请参阅以下内容：

*   这是一项硬件功能，因此将大范围的页面(例如，GB 内存)设置为某些特定的内存权限将比`mprotect(2)`能够管理的速度快得多；这对某些类型的应用程序很重要
*   应用程序(也许是内存中的数据库)可以通过在绝对需要之前关闭对内存区域的写入来受益，从而减少虚假的写入错误

你是如何利用 MPK 的？ 首先，请注意，它目前只在最新的 Linux 内核和 x86_64 处理器体系结构上实现。 要使用它，请阅读*pkey 上的手册页(第 7 节)；*它有说明性注释和示例代码：[http://man7.org/linux/man-pages/man7/pkeys.7.html](http://man7.org/linux/man-pages/man7/pkeys.7.html)。

# 使用 Alloca 分配自动内存

Glibc 库通过 malloc(和 Friends)`alloca(3)`API 提供了动态内存分配的替代方案。

Alloca 可以被认为是一个方便的例程：**它在堆栈**(调用它的函数)上分配内存。 Showcase 的功能是不需要空闲内存，一旦函数返回，内存就会自动释放。 事实上，不能调用`free(3)`。 这是有道理的：堆栈上分配的内存称为自动内存-它将在该函数返回时释放。

像往常一样，使用有好处也有坏处-权衡取舍-`alloca(3)`：

以下是`alloca(3)`的优点：

*   不需要免费；这可以使编程、可读性和可维护性变得简单得多。 因此，我们可以避免危险的内存泄漏错误-这是一个巨大的收获！
*   它被认为非常快，内部碎片(损耗)为零。
*   使用它的主要原因：有时，程序员使用非本地出口，通常通过`longjmp(3)`和`siglongjmp(3)`API。 如果程序员使用`malloc(3)`分配内存区域，然后通过非本地出口突然离开函数，则会发生内存泄漏。 使用*Alloca*可以避免这种情况，并且代码易于实现和理解。

以下是 Alloca 的缺点：

*   Alloca 的主要缺点是，当传递一个大到足以导致堆栈溢出的值时，不能保证返回失败；因此，如果在运行时确实发生这种情况，则进程现在处于**未定义行为**(**UB**)状态，并且(最终)将崩溃。 换言之，像处理`malloc(3)`系列一样，检查空返回的 alloca 是没有用的！
*   可移植性并不是必然的。
*   通常，alloca 是作为内联函数实现的；这可以防止它通过第三方库被覆盖。

请看下面的代码(`ch4/alloca_try.c`)：

```sh
[...]
static void try_alloca(const char *csz, int do_the_memset)
{
    size_t sz = atol(csz);
    void *aptr;

    aptr = alloca(sz);
    if (!aptr)
        FATAL("alloca(%zu) failed\n", sz);
    if (1 == do_the_memset)
        memset(aptr, 'a', sz);

    /* Must _not_ call free(), just return;
     * the memory is auto-deallocated!
     */
}

int main(int argc, char **argv)
{
  [...]
    if (atoi(argv[2]) == 1)
        try_alloca(argv[1], 1);
    else if (atoi(argv[2]) == 0)
        try_alloca(argv[1], 0);
    else {
        fprintf(stderr, "Usage: %s size-to-alloca do_the_memset[1|0]\n", 
                     argv[0]);
        exit(EXIT_FAILURE);
    }
    exit (EXIT_SUCCESS);
}
```

让我们构建并试用它：

```sh
$ ./alloca_try
Usage: ./alloca_try size-to-alloca do_the_memset[1|0]
$ ./alloca_try 50000 1
$ ./alloca_try 50000 0
$ 
```

`alloca_try`的第一个参数是要分配的内存量(以字节为单位)，而第二个参数 iif`1`对该内存区域具有第一个`memset`进程调用；如果是`0`，则不是。

在前面的代码片段中，我们使用 50,000 字节的分配请求尝试了它-它在`memset`两种情况下都成功了。

现在，我们故意将`-1`作为第一个参数传递，它将被视为一个无符号的数量(从而成为 64 位操作系统上的`0xffffffffffffffff`)，这当然应该会导致`alloca(3)`失败，但令人惊讶的是，它没有报告失败；至少它认为没有问题：

```sh
$ ./alloca_try -1 0
$ echo $?
0
$ ./alloca_try -1 1
Segmentation fault (core dumped)
$
```

但是，执行`memset`*和*(将第二个参数作为`1`传递)会导致 bug 浮出水面；如果没有它，我们永远不会知道。

要进一步验证这一点，请尝试在库调用跟踪软件`ltrace`的控制下运行程序；我们将`1`作为第一个参数传递，强制进程在`alloca(3)`之后调用`memset`：

```sh
$ ltrace ./alloca_try -1 1
atoi(0x7ffcd6c3e0c9, 0x7ffcd6c3d868, 0x7ffcd6c3d888, 0)         =  1
atol(0x7ffcd6c3e0c6, 1, 0, 0x1999999999999999)                  = -1
memset(0x7ffcd6c3d730, 'a', -1 <no return ...>
--- SIGSEGV (Segmentation fault) ---
+++ killed by SIGSEGV +++
$ 
```

啊哈！ 我们可以看到，在 Memset 之后，该进程接收到致命信号并死亡。 但是为什么`alloca(3)`API 没有出现在`ltrace`中呢？ 因为它是一个内联函数，这是它的缺点之一。

但请注意；在这里，我们将`0`作为第一个参数传递，绕过对 Memset`alloca(3)`之后对 memset 的调用：

```sh
$ ltrace ./alloca_try -1 0
atoi(0x7fff9495b0c9, 0x7fff94959728, 0x7fff94959748, 0)     =  0
atoi(0x7fff9495b0c9, 0x7fff9495b0c9, 0, 0x1999999999999999) =  0
atol(0x7fff9495b0c6, 0, 0, 0x1999999999999999)              = -1
exit(0 <no return ...>
+++ exited (status 0) +++
$ 
```

它会正常退出，就好像没有 bug 一样！

此外，您还会记得，在[第 3 章](03.html)和*资源限制*中，我们看到进程的默认堆栈大小为 8 MB。 我们可以通过我们的`alloca_try`程序测试这一事实：

```sh
$ ./alloca_try 8000000 1
$ ./alloca_try 8400000 1
Segmentation fault (core dumped)
$ ulimit -s
8192
$ 
```

当我们超过 8MB 时，*`alloca(3)`会分配太多空间，但不会触发崩溃；相反，*`memset(3)`会导致 SEGFAULT 发生。 此外，ulimit 验证堆栈资源限制是否为 8,192KB，即 8MB。

To conclude, a really, really key point: you can often end up writing software that seems to be correct but is, in fact, not. The only way to gain confidence with the software is to take the trouble to perform 100% code coverage and run test cases against them! It's hard to do, but quality matters. Just do it.

# 简略的 / 概括的 / 简易判罪的 / 简易的

本章关注 Linux 操作系统上 C 应用程序开发人员的动态内存管理的简单和高级两个方面。 在最初的部分中，讨论了基本的 glibc 动态内存管理 API 及其在代码中的正确用法。

然后，我们转到更高级的主题，例如程序中断(和`sbrk(3)`API)、`malloc(3)`在分配不同大小的内存时在内部的行为，以及按需分页的关键概念。 然后，我们深入研究了执行内存锁定和内存区域保护的 API，以及使用它们的原因。 最后，我们介绍了替代 API`alloca(3)`。 我们使用了几个代码示例来巩固所学的概念。下一章将讨论一个非常重要的主题--由于内存 API 的糟糕编程实践而可能在 Linux 上出现的各种内存问题(缺陷*