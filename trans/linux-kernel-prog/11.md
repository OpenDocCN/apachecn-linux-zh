# 十一、CPU 调度器——第二部分

这是我们关于 Linux 内核 CPU 调度器的第二章，我们继续上一章的内容。在前一章中，我们讨论了几个关于 Linux 操作系统上 CPU 调度器的工作(和可视化)的关键领域。这包括关于 Linux 上的 KSE 到底是什么，Linux 实现的 POSIX 调度策略，使用`perf`查看调度程序流，以及现代调度程序的设计是如何基于模块化调度类的主题。我们还介绍了如何查询任何线程的调度策略和优先级(使用几个命令行实用程序)，并深入研究了操作系统调度器的内部工作方式。

有了这个背景，我们现在准备在 Linux 上探索更多的 CPU 调度程序；在本章中，我们将涉及以下领域:

*   用 LTTng 和`trace-cmd`可视化流程
*   理解、查询和设置 CPU 相似性掩码
*   查询和设置线程的调度策略和优先级
*   用计算机组控制中央处理器带宽
*   将主线 Linux 转换成 RTOS
*   潜伏期及其测量

我们希望你在阅读这一章之前，已经阅读了(或者有相当的知识)前一章。

# 技术要求

我假设您已经完成了[第 1 章](01.html)、*内核工作区设置*，并适当准备了一个运行 Ubuntu 18.04 LTS(或更高版本的稳定版本)的来宾**虚拟机** ( **VM** )并安装了所有需要的软件包。如果没有，我强烈建议你先做这个。

为了最大限度地利用这本书，我强烈建议您首先设置工作空间环境，包括克隆这本书的 GitHub 代码存储库，并以动手的方式进行处理。知识库可以在这里找到:[https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming)。

# 用 LTTng 和 trace-cmd 可视化流动

在上一章中，我们看到了如何使用`perf`(以及一些替代方案)可视化处理器上的线程流。现在，我们继续使用更强大、更直观的分析工具:LTTng(和 Trace Compass GUI)和`trace-cmd`(一个 Ftrace 前端和 KernelShark GUI)。

请注意，这里的目的只是向您介绍这些强大的跟踪技术；我们没有充分公正对待这些议题所需的范围和空间。

## 使用 LTTng 和跟踪罗盘进行可视化

**Linux 跟踪工具包下一代** ( **LTTng** )是一套开源工具，使您能够同时跟踪用户和内核空间。有点讽刺的是，跟踪内核很容易，而跟踪用户空间(应用、库甚至脚本)需要开发人员手动将工具(所谓的跟踪点)插入到应用中(内核的跟踪点工具由 LTTng 作为内核模块提供)。高质量的 LTTng 文档可在此在线获得:[https://lttng.org/docs/v2.12/](https://lttng.org/docs/v2.12/)(在撰写本文时涵盖 2.12 版本)。

我们这里不包括 LTTng 的安装；详情见[https://lttng.org/docs/v2.12/#doc-installing-lttng](https://lttng.org/docs/v2.12/#doc-installing-lttng)。一旦安装(它有点重——在我的原生 x86_64 Ubuntu 系统上，有超过 40 个内核模块加载到 LTTng 中！)，使用 LTTng -对于系统范围的内核会话，就像我们在这里做的那样-很容易，并且分两个不同的阶段执行:记录，然后是数据分析；下面是这些步骤。(由于本书侧重于内核开发，因此我们不涉及使用 LTTng 来跟踪用户空间应用。)

### 用 LTTng 记录内核跟踪会话

您可以记录一个系统范围的内核跟踪会话，如下所示(在这里，我们有意让讨论尽可能简单):

1.  新建一个会话，将输出目录设置为`<dir>`保存追踪元数据:

```sh
sudo lttng create <session-name> --output=<dir>
```

2.  只需启用所有内核事件(可能会导致生成大量跟踪元数据):

```sh
sudo lttng enable-event --kernel --all

```

3.  开始录制“内核会话”:

```sh
sudo lttng start
```

留出一些时间(跟踪的时间越长，跟踪元数据使用的磁盘空间就越多)。在此期间，所有内核活动都由 LTTng 记录。

4.  停止录制:

```sh
sudo lttng stop
```

5.  销毁会话；别担心，这不会删除跟踪元数据:

```sh
sudo lttng destroy
```

所有上述命令都应该以管理员权限(或同等权限)运行。

I have a few wrapper scripts to perform tracing with (LTTng, Ftrace, `trace-cmd`) at [https://github.com/kaiwan/L5_debug_trg/tree/master/kernel_debug/tracing](https://github.com/kaiwan/L5_debug_trg/tree/master/kernel_debug/tracing); do check them out.

跟踪元数据文件(采用**通用跟踪格式** ( **CTF** )文件格式)保存到前面指定的输出目录。

### 使用图形用户界面报告-跟踪罗盘

数据分析可以通过两种广泛的方式进行——使用基于命令行界面的系统，通常与称为`babeltrace`的 LTTng 一起打包，或者通过称为**跟踪罗盘**的复杂图形用户界面。图形用户界面更有吸引力；我们在这里只展示它的基本用法。

Trace Compass 是一个强大的跨平台 GUI 应用，与 Eclipse 集成良好。事实上，我们直接引用了日食轨迹指南针网站([https://projects.eclipse.org/projects/tools.tracecompass](https://projects.eclipse.org/projects/tools.tracecompass)):

” *Eclipse Trace Compass 是一个开源应用，通过读取和分析系统的日志或踪迹来解决性能和可靠性问题。它的目标是提供视图、图形、度量等，以帮助从跟踪中提取有用的信息，这种方式比巨大的文本转储更加用户友好和信息丰富。*”

可以从这里下载(安装):[https://www.eclipse.org/tracecompass/](https://www.eclipse.org/tracecompass/)。

Trace Compass minimally requires a **Java Runtime Environment** (**JRE**) to be installed as well. I installed one on my Ubuntu 20.04 LTS system with `sudo apt install openjdk-14-jre`.

安装后，启动跟踪罗盘，单击文件|打开跟踪菜单，并导航到在前面步骤中为跟踪会话保存跟踪元数据的输出目录。Trace Compass 将读取元数据并可视化显示，同时提供各种视角和工具视图。这里显示了我们简短的全系统内核跟踪会话的部分截图(*图 11.1*)；您可以看到从`gnome-shell`进程到`swapper/1`内核线程(运行在 CPU #1 上的空闲线程)的上下文切换(显示为`sched_switch`事件–参见事件类型列):

![](Images/8d1707c7-7cbf-4892-9f55-7909a40e44e7.png)

Figure 11.1 – Trace Compass GUI showing a sample kernel tracing session obtained via LTTng

仔细看前面的截图(图 11.1)；在下方的水平窗格中，您不仅可以看到执行了哪个内核函数，还可以*和*获得(在标记为 Contents 的列下)参数列表以及每个参数当时的值！这确实非常有用。

## 使用 trace-cmd 可视化

现代 Linux 内核(从 2.6.27 开始)嵌入了一个非常强大的跟踪引擎，叫做 **Ftrace** 。Ftrace 是用户空间`strace(1)`实用程序的粗略内核等价物，但这将是卖空它！Ftrace 允许 sysad(或开发人员、测试人员或任何真正拥有 root 权限的人)真正地在幕后查看，查看内核空间中正在执行的每一个函数、谁(哪个线程)执行了它、它运行了多长时间、它调用了什么 API、中断(硬中断和软中断)的发生情况、各种类型的延迟测量等等。您可以使用 Ftrace 了解系统实用程序、应用和内核的实际工作方式，以及在操作系统级别执行深度跟踪。

在这本书里，我们避免深究原始 Ftrace 用法的深度(因为它偏离了手头的主题)；相反，在 Ftrace 上使用一个用户空间包装器会更快更容易，这是一个更方便的界面，叫做`trace-cmd(1)`(同样，我们只是触及表面，展示了一个如何使用`trace-cmd`的例子)。

For Ftrace details and usage, the interested reader will find this kernel document useful: [https://www.kernel.org/doc/Documentation/trace/ftrace.rst](https://www.kernel.org/doc/Documentation/trace/ftrace.rst).

大多数现代 Linux 发行版将允许通过它们的包管理系统安装`trace-cmd`；例如，在 Ubuntu 上，`sudo apt install trace-cmd`就足以安装它(如果在比如说 ARM 上的定制 Linux 需要，你总是可以从它的 GitHub 存储库的源代码中进行交叉编译:[https://git . kernel . org/pub/SCM/Linux/kernel/git/rostedt/trace-cmd . git/tree/](https://git.kernel.org/pub/scm/linux/kernel/git/rostedt/trace-cmd.git/tree/))。

让我们执行一个简单的`trace-cmd`会话；首先，我们将在`ps(1)`实用程序运行时记录数据样本；然后，我们将通过`trace-cmd report` **命令行界面** ( **CLI** )以及名为 KernelShark 的图形用户界面前端(它实际上是`trace-cmd`包的一部分)来检查捕获的数据。

### 使用 trace-cmd 记录记录样本会话

在本节中，我们用`trace-cmd(1)`记录一个会话；我们使用几个(许多可能的)选项开关到`trace-cmd record`；像往常一样，`trace-cmd-foo(1)`(用`check-events`、`hist`、`record`、`report`、`reset`等代替`foo`、上的手册页对于查找各种选项开关和使用细节非常有用。一些特别适用于`trace-cmd record`的选项开关如下:

*   `-o`:指定输出文件名(如未指定，默认为`trace.dat`)。
*   `-p`:要使用的插件，`function`、`function_graph`、`preemptirqsoff`、`irqsoff`、`preemptoff`、`wakeup`中的一个；这里，在我们的小演示中，我们使用了`function-graph`插件(在内核中也可以配置其他几个插件)。
*   `-F`:要追踪的命令(或 app)；这非常有用，允许您精确地指定专门跟踪哪个进程(或线程)(否则，跟踪所有线程可能会在尝试解密输出时导致大量噪音)；同样，可以使用`-P`选项开关指定要跟踪的 PID。
*   `-r priority`:以指定的实时优先级运行`trace-cmd`线程(典型范围为 1 到 99；我们将很快介绍查询和设置线程的调度策略和优先级)；这更有可能让`trace-cmd`能够按照要求采集样本。

在这里，我们运行一个快速演示:我们运行`ps -LA`；当它运行时，它产生的所有内核流量都(专门)通过它的`record`功能(我们使用`function-graph`插件)被`trace-cmd`捕获:

```sh
$ sudo trace-cmd record -o trace_ps.dat -r 99 -p function_graph -F ps -LA
plugin 'function_graph'
PID     LWP TTY         TIME CMD
 1        1   ?     00:01:42 systemd
 2        2   ?     00:00:00 kthreadd
[ ... ]
32701   734 tty2   00:00:00 ThreadPoolForeg
CPU 2: 48176 events lost
CPU0 data recorded at offset=0x761000
[ ... ]
CPU3 data recorded at offset=0xf180000
114688 bytes in size
$ ls -lh trace_ps.dat
-rw-r--r-- 1 root root 242M Jun 25 11:23 trace_ps.dat
$
```

结果是一个相当大的数据文件(因为我们捕获了所有事件并执行了一个`ps -LA`来显示所有活动的线程，这需要一段时间，因此捕获的数据样本比较大。还要认识到，默认情况下，内核跟踪是跨系统上的所有 CPU 执行的；您可以通过`-M cpumask`选项进行更改。)

In the preceding example, we captured all events. The `-e` option switch to `trace-cmd(1)` allows you to specify a class of events to trace; for example, to trace the `ping(1)` utility and capture only events related to networking and kernel memory, run the following command:
`sudo trace-cmd record -e kmem -e net -p function_graph -F ping -c1 packtpub.com`.

### 使用跟踪-cmd 报告进行报告和解释

继续前面的部分，在命令行上，我们可以得到一个(非常！)详细报告当`ps`进程运行时内核中发生了什么；使用`trace-cmd report`命令查看。我们还传递了`-l`选项开关:它以被称为 Ftrace 的**延迟格式**显示报告，揭示了许多有用的细节；`-i`开关当然指定要使用的输入文件:

```sh
trace-cmd report -i ./trace_ps.dat -l > report_tc_ps.txt 
```

现在变得非常有趣！我们展示了一些用`vim(1)`打开的(巨大的)输出文件的部分截图；首先我们有以下内容:

![](Images/5091064d-03e6-4383-abde-e6efd3f2fab5.png)

Figure 11.2 – A partial screenshot showing the output of the trace-cmd report

请看图 11.2；对内核 API`schedule()`的调用被故意突出显示并以粗体显示(*图 11.2* ，在线`785303`！).为了解释这一行的所有内容，我们必须理解每一个(空格分隔的)列；他们有八个人:

*   第 1 列:这里，它只是 vim 显示的文件中的行号(让我们忽略它)。
*   第 2 列:这是调用这个函数的流程上下文(函数本身在第 8 列)；显然，这里的流程是`ps-PID`(其 PID 附加在一个`-`字符之后)。

*   第三栏:有用！一系列五个字符，以**延迟格式**显示(我们用`-l`选项切换到`trace-cmd record`，记住！);这个(在我们前面的例子中，是`2.N..`)非常有用，可以解释如下:
    *   第一个字符是运行它的中央处理器核心(这里是核心#2)(注意，一般来说，除了第一个字符之外，如果该字符是一个周期`.`，则表示它为零或不适用)。
    *   第二个字符表示硬件中断状态:
        *   `.`表示默认硬件中断已启用。
        *   `d`表示硬件中断当前被禁用。
    *   第三个字符代表`need_resched`位(我们在上一章*中对此进行了解释，调度程序什么时候运行？*节):
        *   `.`表示已清除。
        *   `N`表示设置好了(表示内核要求尽快进行重调度！).
    *   第四个字符只有在中断正在进行时才有意义，否则，它只是一个`.`，暗示我们处于一个过程上下文中；如果中断正在进行中——这意味着我们处于中断上下文中——其值为以下值之一:
        *   `h`表示我们正在 hardirq(或上半部分)中断上下文中执行。
        *   `H`表示我们正在一个软 irq 内发生的 hardirq 中执行。
        *   `s`表示我们在一个软中断上下文中执行。
    *   第五个字符表示抢占计数或深度；如果是`.`，则为零，表示内核运行在可抢占状态；如果非零，则会显示一个整数，这意味着许多内核级锁已经被占用，迫使内核进入不可抢占的状态。
    *   顺便说一下，输出与 Ftrace 的原始输出非常相似，除了在原始 Ftrace 的情况下，我们将只看到四个字符–第一个字符(CPU 内核号)*没有*出现在这里；相反，它显示为最左边的列；以下是原始 Ftrace(不是`trace-cmd`)延迟格式的部分截图:

![](Images/e1683af7-417c-4bb1-8dab-2210dc29c3fb.png)

Figure 11.3 – A partial screenshot focused on raw Ftrace's four-character latency format (fourth field)

前面的截图是直接从原始 Ftrace 输出中挑选出来的。

*   (现在回到图 11.2 中的剩余列)
    第 4 列:时间戳，单位为*秒:微秒*格式。
*   第 5 列:发生的事件的名称(这里，因为我们使用了`function_graph`插件，它将是`funcgraph_entry`或`fungraph_exit`，分别意味着功能进入或退出)。
*   第 6 列[可选]:前面函数调用的持续时间，所用时间及其单位(us =微秒)一起显示；前缀字符用于表示函数执行是否花费了很长时间(我们简单地将其视为此列的一部分)；从内核 Ftrace 文档(这里:[https://www.kernel.org/doc/Documentation/trace/ftrace.rst](https://www.kernel.org/doc/Documentation/trace/ftrace.rst)，我们有这个:

    *   `+`，这意味着一个函数超过了 10 微秒
    *   `!`，这意味着一个函数超过了 100 微秒
    *   `#`，这意味着一个函数超过了 1000 微秒
    *   `*`，这意味着一个函数超过了 10 毫秒
    *   `@`，这意味着一个函数超过了 100 毫秒
    *   `$`，这意味着一个函数超过了 1 秒
*   第 7 列:只是分隔符`|`。
*   第 8 列:最右列是正在执行的内核函数的名称；右边一个左大括号，`{`，表示刚才调用了函数；只有一个右大括号`}`的列意味着前面函数的结束(匹配左大括号)。

这种详细程度对于解决内核(甚至用户空间)问题，以及深入了解内核流程都非常有价值。

When `trace-cmd record` is used without the `-p function-graph` option switch, we do lose the nicely indented function call graph-like output, but we do gain something as well: you will now see all function parameters along with their runtime values to the right of every single function call! A truly valuable aid at times.

我忍不住展示了同一份报告的另一个片段——另一个有趣的例子，关于我们在现代 Linux 上学习的调度类的工作原理(在前一章中已经介绍过)；这实际上显示在`trace-cmd`输出中:

![](Images/21976669-95f0-4c59-8ab6-901b9a2fe248.png)

Figure 11.4 – A partial screenshot of trace-cmd report output

仔细解读前面的截图(*图 11.4* ):第二行(最右边的函数名一栏用粗体显示，紧随其后的两个函数也是如此)显示`pick_next_task_stop()`函数被调用；这意味着发生了一个调度，内核中的核心调度代码完成了它的例程——它按照优先级顺序遍历调度类的链表，询问每个调度类是否有线程要调度；如果有，核心调度器上下文会切换到它(如前一章*模块化调度类*一节中详细解释的那样)。

在图 11.4 中，您确实看到了这种情况:核心调度代码通过依次调用`pick_next_task_stop()`、`pick_next_task_dl()`和`pick_next_task_rt()`函数，询问**停止调度** ( **SS** )、**截止时间** ( **DL** )和**实时** ( **RT** )类是否有任何线程想要运行。显然，对于他们所有人来说，答案都是否定的，因为下一个要运行的函数是 fair (CFS)类的函数(那么为什么前面的截图中没有出现`pick_next_task_fair()`函数呢？啊，同样，这是对您的代码优化:内核开发人员理解这是可能的情况，他们检查它并在大多数时间直接调用公平类代码)。

我们在这里介绍的强大的 Ftrace 框架和`trace-cmd`实用程序只是基础；我敦促你查阅`trace-cmd-<foo>`(其中`<foo>`被`record`、`report`等代替)上的手册页，那里有典型的好例子。此外，还有几篇写得非常好的关于 Ftrace(和`trace-cmd`)的文章——请参考*进一步阅读*部分。

### 使用图形用户界面前端进行报告和解释

更好的消息是:`trace-cmd`工具集包括一个图形用户界面前端，用于更人性化的解释和分析，称为 KernelShark(尽管在我看来，它不如 Trace Compass 功能全面)。在 Ubuntu/Debian 上安装就像做`sudo apt install kernelshark`一样简单。

下面，我们运行`kernelshark`，将前面`trace-cmd`记录会话输出的跟踪数据文件作为参数传递给它(将参数调整为 KernelShark，以引用保存跟踪元数据的位置):

```sh
$ kernelshark ./trace_ps.dat
```

这里显示了使用前面的跟踪数据运行的 KernelShark 的屏幕截图:

![](Images/5febff2e-dcdf-4a82-bc85-10f1bc629287.png)

Figure 11.5 – A screenshot of the kernelshark GUI displaying the earlier-captured data via trace-cmd

有趣；`ps`进程在 CPU #2 上运行(正如我们之前在 CLI 版本中看到的)。在这里，我们还可以看到在下方平铺的水平窗格中执行的功能；例如，我们突出显示了`pick_next_task_fair()`的条目。这些列非常明显，`Latency`列格式(四个字符，而不是五个)解释为我们之前解释的(原始)Ftrace。

**快速测验**:图 11.5 中的延迟格式字段`dN..`意味着什么？

答:这意味着，目前，我们有以下情况:

*   第一列`d`:硬件中断被禁用。
*   第二列`N`:设置`need_resched`位(意味着需要在下一个可用的调度机会点调用调度程序)。
*   第三列`.`:内核`pick_next_task_fair()`函数的代码在进程上下文中运行(任务是`ps`，PID 为`22545`；记住，Linux 是一个单片内核！).
*   第四列`.`:抢占深度(计数)为零，表示内核处于可抢占状态。

现在，我们已经介绍了如何使用这些强大的工具来帮助生成和可视化与内核执行和调度相关的数据，接下来让我们进入另一个领域:在下一节中，我们将关注另一个重要的方面——线程的 CPU 相似性掩码到底是什么，以及您如何以编程方式(或其他方式)获取/设置它。

# 理解、查询和设置 CPU 相似性掩码

任务结构，包含几十个线程属性的根数据结构，有几个与调度直接相关的属性:优先级(好的*以及 RT 优先级值)、调度类结构指针、线程所在的运行队列(如果有的话)等等。*

 *其中有一个重要成员， **CPU 亲和位掩码**(实际结构成员为`cpumask_t cpus_allowed`)。这也告诉您，CPU 亲缘性位掩码是每个线程的数量；这是有道理的——毕竟，Linux 上的 KSE 是一个线程。它本质上是一个位数组，每个位代表一个 CPU 内核(变量中有足够的可用位)；如果设置了对应于某个内核的位(`1`)，则允许在该内核上调度和执行线程；如果清除了(`0`)，那就不是了。

默认情况下，设置所有的 CPU 相似性掩码位；因此，线程可以在任何内核上运行。例如，在具有(操作系统看到的)四个中央处理器内核的盒子上，每个线程的默认中央处理器相似性位掩码将是二进制的`1111` ( `0xf`)。(从概念上看，浏览图 11.6 可以看到 CPU 相似性位掩码的外观。)

在运行时，调度器决定线程将实际运行在哪个核心上。其实仔细想想，真的很含蓄:默认情况下，每个 CPU 内核都有一个与之关联的运行队列；每个可运行的线程将在一个 CPU 运行队列上；因此，它有资格运行，并且默认情况下在它的运行队列所代表的 CPU 上运行。当然，调度器有一个负载均衡器组件，可以根据需要将线程迁移到其他 CPU 内核(实际上是运行队列)(内核线程称为`migration/n`，其中`n`是协助执行该任务的内核号)。

内核确实向用户空间公开了应用接口(系统调用，当然是`sched_{s,g}etaffinity(2)`和它们的`pthread`包装器库应用接口)，这允许应用在它认为合适的时候将一个线程(或多个线程)仿射或关联到特定的 CPU 内核(通过同样的逻辑，我们也可以在内核中为任何给定的内核线程这样做)。例如，将中央处理器关联掩码设置为`1010`二进制，十六进制等于`0xa`，意味着线程只能在中央处理器内核 1 和 3 上执行*(从零开始计数)。*

 *一个关键点:虽然你可以操纵 CPU 亲和掩码，但建议避免这样做；内核调度器详细了解 CPU 拓扑结构，可以最好地对系统进行负载平衡。

话虽如此，显式设置线程的 CPU 亲缘关系掩码可能是有益的，原因如下:

*   通过确保一个线程始终运行在同一个中央处理器内核上，可以大大减少缓存无效(以及令人不快的缓存“反弹”)。
*   有效消除了内核之间的线程迁移成本。
*   中央处理器保留——一种策略，通过保证所有其他线程不被明确允许在该核心上执行，将核心专门授予一个线程。

前两个在某些角落情况下很有用；第三个，CPU 预留，倾向于在一些时间关键的实时系统中使用的技术，这样做的成本是合理的。然而，在实践中执行 CPU 保留是相当困难的，需要操作系统级的干预。)线程创建；费用可能高得令人望而却步。为此，这实际上是通过指定某个(或多个)CPU 被*与所有任务隔离*来实现的；Linux 内核为这项工作提供了一个内核参数`isolcpus`。

在这方面，我们直接引用`sched_{s,g}etaffinity(2)`系统调用上的手册页:

The isolcpus boot option can be used to isolate one or more CPUs at boot time, so that no processes are scheduled onto those CPUs. Following the use of this boot option, the only way to schedule processes onto the isolated CPUs is via sched_setaffinity() or the cpuset(7) mechanism. For further information, see the kernel source file Documentation/admin-guide/kernel-parameters.txt. As noted in that file, isolcpus is the preferred mechanism of isolating CPUs (versus the alternative of manually setting the CPU affinity of all processes on the system). Note, though, the previously mentioned `isolcpus` kernel parameter is now considered deprecated; it's preferable to use the cgroups `cpusets` controller instead (`cpusets` is a cgroup feature or controller; we do have some coverage on cgroups later in this chapter, in the *CPU bandwidth control with cgroups* section).

We refer you to more details in the kernel parameter documentation (here: [https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt](https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt)), specifically under the parameter labeled `isolcpus=`.

现在您已经理解了它背后的理论，让我们实际编写一个用户空间 C 程序来查询和/或设置任何给定线程的 CPU 亲缘关系掩码。

## 查询和设置线程的 CPU 关联掩码

作为演示，我们提供了一个小的用户空间 C 程序来查询和设置用户空间进程(或线程)的 CPU 亲缘关系掩码。通过`sched_getaffinity(2)`系统调用并通过设置其对应项来查询中央处理器关联掩码:

```sh
#define _GNU_SOURCE
#include <sched.h>

int sched_getaffinity(pid_t pid, size_t cpusetsize,
                        cpu_set_t *mask);
int sched_setaffinity(pid_t pid, size_t cpusetsize,
                        const cpu_set_t *mask);
```

一种称为`cpu_set_t`的专用数据类型被用来表示 CPU 相似性位掩码；它相当复杂:它的大小是根据系统上看到的 CPU 内核数量动态分配的。该 CPU 掩码(类型为`cpu_set_t`)必须首先初始化为零；`CPU_ZERO()`宏实现了这一点(存在几个类似的辅助宏；一定要参考`CPU_SET(3)`上的手册页。前面两个系统调用中的第二个参数是 CPU 集的大小(我们简单地使用`sizeof`运算符来获取它)。

为了更好地理解这一点，查看我们的代码的示例运行(`ch11/cpu_affinity/userspc_cpuaffinity.c`)很有启发性；我们在具有 12 个 CPU 内核的本机 Linux 系统上运行它:

![](Images/6d66a20c-580f-4cc1-b08a-987717b30c76.png)

Figure 11.6 – Our demo user space app showing the CPU affinity mask

这里，我们已经运行了没有参数的应用。在这种模式下，它会查询自身的 CPU 亲缘关系掩码(即`userspc_cpuaffinity`调用进程的)。我们打印出位掩码的位:正如您在前面的截图中可以清楚地看到的，它是二进制的`1111 1111 1111`(相当于`0xfff`)，这意味着默认情况下，该进程可以在系统上可用的 12 个 CPU 内核中的任何一个上运行。

该应用通过有用的`popen(3)`库应用编程接口运行`nproc(1)`实用程序来检测可用的中央处理器内核数量。请注意，`nproc`返回的值是调用进程可用的 CPU 内核数量；它可能比实际的 CPU 内核数量少(通常是一样的)；可用内核的数量可以通过几种方式改变，正确的方式是通过 cgroup `cpuset`资源控制器(我们将在本章后面介绍一些关于 cgroup 的信息)。

查询代码如下:

```sh
// ch11/cpu_affinity/userspc_cpuaffinity.c

static int query_cpu_affinity(pid_t pid)
{
    cpu_set_t cpumask;

    CPU_ZERO(&cpumask);
    if (sched_getaffinity(pid, sizeof(cpu_set_t), &cpumask) < 0) {
        perror("sched_getaffinity() failed");
        return -1;
    }
    disp_cpumask(pid, &cpumask, numcores);
    return 0;
}
```

我们的`disp_cpumask()`函数绘制位掩码(我们让您来检查)。

如果传递了额外的参数——进程(或线程)的 PID 作为第一个参数，以及一个中央处理器位掩码作为第二个参数——我们将尝试*将该进程(或线程)的中央处理器关联掩码设置为传递的值。当然，更改 CPU 关联性位掩码需要您拥有进程或拥有根权限(更正确的说法是，拥有`CAP_SYS_NICE`功能)。*

一个快速演示:在图 11.7 中，`nproc(1)`向我们展示了 CPU 内核的数量；然后，我们运行我们的应用来查询和设置我们的 shell 进程的 CPU 亲缘关系掩码。在笔记本电脑上，假设`bash`的亲和面具是`0xfff`(二进制`1111 1111 1111`)开始，果然如此；我们将其更改为`0xdae`(二进制`1101 1010 1110`)并再次查询以验证更改:

![](Images/6da7a9bc-571a-4678-8989-c7d321731e55.png)

Figure 11.7 – Our demo app queries then sets the CPU affinity mask of bash to 0xdae

好吧，这很有趣:首先，应用正确地检测到它可用的 CPU 内核数量为 12 个；然后，它查询 bash 进程的(默认)CPU 亲缘关系掩码(因为我们将它的 PID 作为第一个参数传递)；它出现了，如`0xfff`所料。然后，我们还传递了第二个参数——现在设置的位掩码(`0xdae`),它这样做了，将 bash 的 CPU 关联掩码设置为`0xdae`。现在，由于我们所在的终端窗口也是同样的 bash 进程，运行`nproc`再次显示值为 8，而不是 12！这确实是正确的:bash 进程现在只有八个 CPU 核心可用。(这是因为我们不会在退出时将 CPU 关联掩码恢复到其原始值。)

下面是设置 CPU 相似性掩码的相关代码:

```sh
// ch11/cpu_affinity/userspc_cpuaffinity.c
static int set_cpu_affinity(pid_t pid, unsigned long bitmask)
{
    cpu_set_t cpumask;
    int i;

    printf("\nSetting CPU affinity mask for PID %d now...\n", pid);
    CPU_ZERO(&cpumask);

    /* Iterate over the given bitmask, setting CPU bits as required */
    for (i=0; i<sizeof(unsigned long)*8; i++) {
        /* printf("bit %d: %d\n", i, (bitmask >> i) & 1); */
        if ((bitmask >> i) & 1)
            CPU_SET(i, &cpumask);
    }

    if (sched_setaffinity(pid, sizeof(cpu_set_t), &cpumask) < 0) {
        perror("sched_setaffinity() failed");
        return -1;
    }
    disp_cpumask(pid, &cpumask, numcores);
    return 0;
}
```

在前面的代码片段中，您可以看到我们首先适当地设置`cpu_set_t`位掩码(通过循环每个位)，然后使用`sched_setaffinity(2)`系统调用在给定的`pid`上设置新的 CPU 关联性掩码。

### 使用任务集(1)来执行 CPU 关联性

类似于(在前一章中)我们如何使用方便的用户空间实用程序`chrt(1)`来获取(或设置)进程(或线程)的调度策略和/或优先级，您可以使用用户空间`taskset(1)`实用程序来获取和/或设置给定进程(或线程)的 CPU 相似性掩码。下面是几个简单的例子:请注意，这些示例是在具有 4 个 CPU 内核的 x86_64 Linux 系统上运行的:

*   使用`taskset`查询 systemd (PID 1)的 CPU 亲缘关系掩码:

```sh
$ taskset -p 1
pid 1's current affinity mask: f 
$
```

*   使用`taskset`确保编译器——及其后代(汇编器和链接器)——只在前两个 CPU 内核上运行；taskset 的第一个参数是 CPU 相似性位掩码(`03`是二进制的`0011`):

```sh
$ taskset 03 gcc userspc_cpuaffinity.c -o userspc_cpuaffinity -Wall 
```

有关完整的使用细节，请查阅`taskset(1)`手册页。

### 在内核线程上设置 CPU 相似性掩码

举个例子，如果我们想演示一种叫做每 CPU 变量的同步技术，我们需要创建两个内核线程，并保证它们都在单独的 CPU 内核上运行。为此，我们必须设置每个内核线程的 CPU 关联掩码(第一个设置为`0`，第二个设置为`1`，以便让它们分别只在 CPU`0`和`1`上执行。问题是，这不是一份干净的工作——老实说，这是一份相当“T4”的工作，绝对不是推荐的工作。该代码的以下注释说明了原因:

```sh
  /* ch17/6_percpuvar/6_percpuvar.c */
  /* WARNING! This is considered a hack.
   * As sched_setaffinity() isn't exported, we don't have access to it
   * within this kernel module. So, here we resort to a hack: we use
   * kallsyms_lookup_name() (which works when CONFIG_KALLSYMS is defined)
   * to retrieve the function pointer, subsequently calling the function
   * via it's pointer (with 'C' what you do is only limited by your
   * imagination :).
   */
  ptr_sched_setaffinity = (void *)kallsyms_lookup_name("sched_setaffinity");
```

稍后，我们调用函数指针，实际上是调用`sched_setaffinity`代码，如下所示:

```sh
    cpumask_clear(&mask);
    cpumask_set_cpu(cpu, &mask); // 1st param is the CPU number, not bitmask
    /* !HACK! sched_setaffinity() is NOT exported, we can't call it
     *   sched_setaffinity(0, &mask); // 0 => on self 
     * so we invoke it via it's function pointer */
    ret = (*ptr_sched_setaffinity)(0, &mask);   // 0 => on self
```

非常规且有争议；它确实有效，但是在生产中请避免这样的黑客攻击。

现在您已经知道了如何获取/设置线程的 CPU 亲缘关系掩码，让我们进入下一个逻辑步骤:如何获取/设置线程的调度策略和优先级！下一节将深入探讨细节。

# 查询和设置线程的调度策略和优先级

在[第 10 章](10.html)、*CPU 调度程序-第 1 部分*中，在*线程-哪个调度策略和优先级*部分，您学习了如何通过`chrt(1)`查询任何给定线程的调度策略和优先级(我们还演示了一个简单的 bash 脚本)。在这里，我们提到了这样一个事实，即`chrt(1)`在内部调用`sched_getattr(2)`系统调用来查询这些属性。

非常类似地，设置调度策略和优先级可以通过使用`chrt(1)`实用程序(例如，使在脚本中这样做变得简单)来执行，或者通过`sched_setattr(2)`系统调用在(用户空间)C 应用中以编程方式执行。此外，内核还公开了其他 API:`sched_{g,s}etscheduler(2)`及其`pthread`库包装器 API，`pthread_{g,s}etschedparam(3)`(由于这些都是用户空间 API，我们让您浏览它们的手册页以获取详细信息，并亲自试用)。

## 内核内部——在内核线程上

正如你现在知道的，内核肯定不是一个进程，也不是一个线程。话虽如此，内核确实包含内核线程；像它们的用户空间对应物一样，可以根据需要创建内核线程(从内核内核、设备驱动程序、内核模块中)。它们*是*可调度实体(KSEs！)当然，它们每个都有一个任务结构；因此，可以根据需要查询或设置它们的调度策略和优先级..

所以，说到这里:为了设置内核线程的调度策略和/或优先级，内核通常使用`kernel/sched/core.c:sched_setscheduler_nocheck()` (GFP 导出)内核 API 在这里，我们展示了它的签名和一个典型用法的例子；下面的评论使它变得不言自明:

```sh
// kernel/sched/core.c
/**
 * sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace.
 * @p: the task in question.
 * @policy: new policy.
 * @param: structure containing the new RT priority.
 *
 * Just like sched_setscheduler, only don't bother checking if the
 * current context has permission. For example, this is needed in
 * stop_machine(): we create temporary high priority worker threads,
 * but our caller might not have that capability.
 *
 * Return: 0 on success. An error code otherwise.
 */
int sched_setscheduler_nocheck(struct task_struct *p, int policy,
                   const struct sched_param *param)
{
    return _sched_setscheduler(p, policy, param, false);
}
EXPORT_SYMBOL_GPL(sched_setscheduler_nocheck);
```

内核使用内核线程的一个很好的例子是内核使用线程中断。这里，内核必须使用`SCHED_FIFO`(软)实时调度策略和`50`(中间)实时优先级值创建一个专用的内核线程，用于中断处理。这里显示的(相关)代码是在内核线程上设置调度策略和优先级的示例:

```sh
// kernel/irq/manage.c
static int
setup_irq_thread(struct irqaction *new, unsigned int irq, bool secondary)
{ 
    struct task_struct *t;
    struct sched_param param = {
        .sched_priority = MAX_USER_RT_PRIO/2,
    };
    [ ... ]
    sched_setscheduler_nocheck(t, SCHED_FIFO, &param);
    [ ... ]
```

(这里，我们不展示通过`kthread_create()` API 创建内核线程的代码。另外，仅供参考，`MAX_USER_RT_PRIO`是值`100`。)

现在，您已经很好地理解了 CPU 调度在操作系统级别上是如何工作的，我们将继续另一个非常引人注目的讨论——cggroups；继续读！

# 用计算机组控制中央处理器带宽

在朦胧的过去，内核社区与一个相当令人烦恼的问题进行了激烈的斗争:尽管调度算法及其实现——早期的 2.6.0 O(1)调度器，以及稍晚一些的(具有 2.6.23)完全公平的调度器(**CFS**)——承诺了完全公平的调度，但事实并非如此。想一想:假设你和其他九个人一起登录了一个 Linux 服务器。在其他条件相同的情况下，处理器时间可能(或多或少)在所有十个人之间公平分配；当然，你会明白，真正运行的不是人，而是代表他们运行的进程和线程。

至少现在，让我们假设它大部分是公平共享的。但是，如果你编写一个用户空间程序，在一个循环中，不加区别地产生几个新线程，每个线程执行大量的 CPU 密集型工作(也许作为额外的奖励，还分配了大量的内存；每个循环迭代中可能有一个文件压缩器应用！？CPU 带宽分配不再是任何真正意义上的公平，您的帐户将有效地占用 CPU(也许还有其他系统资源，如内存)！

需要一种精确有效地分配和管理中央处理器(和其他资源)带宽的解决方案；最终，谷歌工程师不得不使用补丁将现代 cgroups 解决方案植入 Linux 内核(版本 2.6.24)。简而言之，cgroups 是一个内核特性，它允许系统管理员(或任何具有根访问权限的人)对系统上的各种资源(或称之为 cgroup 词典中的*控制器*)执行带宽分配和细粒度资源管理。一定要注意:使用 cgroups，不仅是处理器(CPU 带宽)，还有内存、网络、块 I/O(以及更多)带宽，可以根据您的项目或产品的需要仔细分配和监控。

所以，嘿，你现在有兴趣了！如何启用此 cgroups 功能？简单–这是一个内核特性，您可以用通常的方式以相当精细的粒度启用(或禁用):通过配置内核！相关菜单(通过方便的`make menuconfig`界面)为`General setup / Control Group support`。试试这个:`grep`你的`CGROUP`内核配置文件；如果需要，调整你的内核配置，重建，用新内核重启，并测试。(我们在[第 2 章](02.html)、*中详细介绍了内核配置，从源代码构建 5.x Linux 内核–第 1 部分*，在[第 3 章](03.html)、*中详细介绍了内核构建和安装，从源代码构建 5.x Linux 内核–第 2 部分。*)

Good news: cgroups is enabled by default on any (recent enough) Linux system that runs the systemd init framework. As mentioned just now, you can query the cgroup controllers enabled by `grep`-ping your kernel config file, and modify the config as desired.

从 2.6.24 开始，cgroups 就像所有其他内核特性一样，不断发展。最近，达到了一个点，充分改进的 cgroup 特性变得与旧的不兼容，导致了一个新的 cgroup 版本，命名为 cgroups v2(或简称 cgroup S2)；这在 4.5 内核系列中被宣布为生产就绪(旧版本现在被称为 cgroups v1 或旧版 cggroups 实现)。请注意，在撰写本文时，两者都可以并且确实一起存在(有一些限制；许多应用和框架仍然使用较旧的 cgroups v1，并且尚未迁移到 v2)。

A detailed rationale of why to use cgroups v2 as opposed to cgroups v1 can be found within the kernel documentation here: [https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#issues-with-v1-and-rationales-for-v2](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#issues-with-v1-and-rationales-for-v2)

`cgroups(7)`上的手册页较为详细地描述了接口和各种可用(资源)控制器(或*子系统*，因为它们有时被称为)；对于组 v1，它们是`cpu`、`cpuacct`、`cpuset`、`memory`、`devices`、`freezer`、`net_cls`、`blkio`、`perf_event`、`net_prio`、`hugetlb`、`pids`和`rdma`。我们请感兴趣的读者参考上述手册了解详情；作为一个例子，PIDS 控制器在防止叉形炸弹方面非常有用(通常，这是一种愚蠢但致命的 DoS 攻击，其中`fork(2)`系统调用是在无限循环中发出的！)，允许您限制可以从该组(或其后代)分叉的进程数量。在运行 cgroups v1 的 Linux 盒子上，查看`/proc/cgroups`的内容:它显示了可用的 v1 控制器及其当前使用情况。

控制组通过专门构建的合成(伪)文件系统公开，通常安装在`/sys/fs/cgroup`下。在 cgroups v2 中，所有控制器都安装在一个层次结构(或树)中。这与 cgroups v1 不同，在 cgroups v1 中，多个控制器可以安装在多个层次或组下。现代 init 框架*system d*是 v1 和 v2 用户组的用户。`cgroups(7)`手册页确实提到了这样一个事实，`systemd(1)`在启动期间(在`/sys/fs/cgroup/unified`自动安装了一个 cgroups v2 文件系统。

在 cgroups v2 中，这些是受支持的控制器(或资源限制器或子系统，如果您愿意):`cpu`、`cpuset`、`io`、`memory`、`pids`、`perf_event`和`rdma`(前五个通常被部署)。

本章的重点是 CPU 调度；因此，我们不再深入研究其他控制器，而是将我们的讨论限制在使用 cggroups v2`cpu`控制器来限制 CPU 带宽分配的示例上。关于使用其他控制器的更多信息，我们请你参考前面提到的资源(以及在本章的*进一步阅读*部分找到的更多信息)。

## 在 Linux 系统上查找 cgroups v2

首先，让我们查找可用的 v2 控制器；为此，请找到 cgroups v2 装载点；它通常在这里:

```sh
$ mount | grep cgroup2 
cgroup2 on /sys/fs/cgroup/unified type cgroup2 
   (rw,nosuid,nodev,noexec,relatime,nsdelegate) 
$ sudo cat /sys/fs/cgroup/unified/cgroup.controllers 
$ 
```

嘿，`cgroup2`没有控制器了！？实际上，在*混合*组的情况下会是这样，v1 和 v2，这是默认的(截至编写时)。要专门使用更高版本，从而使所有已配置的控制器可见，您必须首先通过在引导时传递以下内核命令行参数来禁用 cggroups v1:`cgroup_no_v1=all`(回想一下，所有可用的内核参数都可以在此处方便地看到:[https://www . kernel . org/doc/Documentation/admin-guide/kernel-parameters . txt](https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt))。

使用前面的选项重新启动系统后，您可以检查您指定的内核参数(通过 x86 上的 GRUB，或者通过嵌入式系统上的 U-Boot)是否确实已经被内核解析:

```sh
$ cat /proc/cmdline
 BOOT_IMAGE=/boot/vmlinuz-4.15.0-118-generic root=UUID=<...> ro console=ttyS0,115200n8 console=tty0 ignore_loglevel quiet splash cgroup_no_v1=all 3
$
```

好吧。现在让我们重试查找`cgroup2`控制器；您应该会发现它通常安装在`/sys/fs/cgroup/`下-`unified`文件夹不再存在(现在我们已经使用`cgroup_no_v1=all`参数启动了):

```sh
$ cat /sys/fs/cgroup/cgroup.controllers
cpu io memory pids 
```

啊，现在我们看到了它们(您看到的确切控制器取决于内核是如何配置的)。

管理 cgroups2 工作的规则超出了本书的范围；如果你愿意，我建议你在这里通读一下:[https://www . kernel . org/doc/html/latest/admin-guide/cgroup-v2 . html # control-group-v2](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#control-group-v2)。另外，一个 cgroup 下的所有`cgroup.<foo>`伪文件在*核心接口文件*部分详细描述(https://www . kernel . org/doc/html/latest/admin-guide/cgroup-v2 . html #核心接口文件)。类似的信息以更简单的方式呈现在`cgroups(7)`的优秀手册页中(在 Ubuntu 上通过`man 7 cgroups`查找)。

## 试用–cggroups v2 中央处理器控制器

让我们尝试一些有趣的事情:我们将在系统上的 cgroups v2 层次结构下创建一个新的子组。然后，我们将为它设置一个 CPU 控制器，运行几个测试进程(这些进程会不断敲打系统的 CPU 内核)，并为这些进程实际可以使用的 CPU 带宽设置一个用户指定的上限！

这里，我们概述了您通常会采取的步骤(所有这些步骤都要求您以 root 访问权限运行):

1.  确保您的内核支持 cgroups v2:
    *   您应该在 4.5 或更高版本的内核上运行。
    *   如果存在混合的 cgroups(旧版 v1 和较新的 v2，在编写本文时，这是默认的)，请检查您的内核命令行是否包含`cgroup_no_v1=all`字符串。这里，我们将假设 cgroup v2 层次结构在`/sys/fs/cgroup`处得到支持和安装。
2.  向 cgroups v2 层次结构添加一个`cpu`控制器；这是通过以 root 身份执行以下操作来实现的:

```sh
echo "+cpu" > /sys/fs/cgroup/cgroup.subtree_control
```

The kernel documentation on cgroups v2 ([https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu)) does mention this point: *WARNING: cgroup2 doesn’t yet support control of realtime processes and the cpu controller can only be enabled when all RT processes are in the root cgroup. Be aware that system management software may already have placed RT processes into nonroot cgroups during the system boot process, and these processes may need to be moved to the root cgroup before the cpu controller can be enabled.*

3.  创建子组:只需在 cgroup v2 层次结构下创建一个具有所需子组名称的目录即可；例如，要创建名为`test_group`的子组，请使用以下命令:

```sh
mkdir /sys/fs/cgroup/test_group
```

4.  有趣的地方在这里:为属于这个子组的进程设置最大允许的 CPU 带宽；这是通过写入`<cgroups-v2-mount-point>/<sub-group>/cpu.max`(伪)文件实现的。为清楚起见，根据内核文档([https://www . kernel . org/doc/html/latest/admin-guide/cgroup-v2 . html # CPU-interface-files](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html#cpu-interface-files))对此文件的解释转载于此:

```sh
cpu.max
A read-write two value file which exists on non-root cgroups. The default is “max 100000”. The maximum bandwidth limit. It’s in the following format: 
$MAX $PERIOD
which indicates that the group may consume upto $MAX in each $PERIOD duration. “max” for $MAX indicates no limit. If only one number is written, $MAX is updated.
```

实际上，子控制组中的所有进程将被共同允许在`$PERIOD`微秒的时间段之外运行`$MAX`；因此，例如，使用`MAX = 300,000`和`PERIOD = 1,000,000`，我们有效地允许子控制组中的所有进程在 1 秒的时间内运行 0.3 秒！

5.  将新的子控制组插入一些流程；这是通过将他们的个人识别码写入`<cgroups-v2-mount-point>/<sub-group>/cgroup.procs`伪文件来实现的:
    *   您可以通过查找每个进程的`/proc/<PID>/cgroup`伪文件的内容，进一步验证它们是否真的属于这个子组；如果它包含一行形式`0::/<sub-group>`，那么它确实属于子组！
6.  就是这样；*新分组下的进程现在将在*施加的 CPU 带宽限制下执行它们的工作；完成后，他们会像往常一样死去...你可以用一个简单的`rmdir <cgroups-v2-mount-point>/<sub-group>`删除(或删除)子组。

这里有一个实际执行上述步骤的 bash 脚本:`ch11/cgroups_v2_cpu_eg/cgv2_cpu_ctrl.sh`。一定要看看！有趣的是，它允许您通过最大允许 CPU 带宽–在*步骤 4* 中讨论的`$MAX`值！不仅如此；我们特意编写了一个测试脚本(`simp.sh`)敲打 CPU——它们生成整数值，我们将其重定向到文件。因此，它们在其生命周期内生成的整数数量表明了有多少 CPU 带宽可供它们使用...这样，我们就可以测试脚本并实际看到 cggroups(v2)在运行！

这里的几次测试运行将帮助您理解这一点:

```sh
$ sudo ./cgv2_cpu_ctrl.sh
[sudo] password for <username>: 
Usage: cgv2_cpu_ctrl.sh max-to-utilize(us)
 This value (microseconds) is the max amount of time the processes in the sub-control
 group we create will be allowed to utilize the CPU; it's relative to the period,
 which is the value 1000000;
 So, f.e., passing the value 300,000 (out of 1,000,000) implies a max CPU utilization
 of 0.3 seconds out of 1 second (i.e., 30% utilization).
 The valid range for the $MAX value is [1000-1000000].
$ 
```

您需要以 root 用户身份运行它，并将`$MAX`值作为参数传递(之前看到的使用屏幕非常清楚地解释了这一点，包括显示有效范围(微秒值))。

在下面的截图中，我们运行参数为`800000`的 bash 脚本，这意味着在 100 万周期中有 80 万的 CPU 带宽；实际上，CPU 利用率相当高，每 1 秒钟有 0.8 秒(80%):

![](Images/37abcab9-5863-4a96-8edc-99fd6a186813.png)

Figure 11.8 – Screenshot of running our cgroups v2 CPU controller demo bash script with an effective max CPU bandwidth of 80%

在*图 11.8* 中研究我们脚本的输出；可以看到它各司其职:在验证了 cgroup v2 支持后，它添加了一个`cpu`控制器，并创建了一个子组(称为`test_group`)。然后，它开始启动两个名为`j1`和`j2`的测试过程(实际上，它们只是我们的`simp.sh`脚本的象征性链接)。一旦启动，它们当然会运行。然后，脚本查询并将其 PID 添加到子控制组中(如*步骤 5* 所示)。我们给这两个进程 5 秒钟的运行时间；然后，脚本显示他们写入的文件的内容。设计为作业`j1`从`1`开始写整数，作业`j2`从`900`开始写整数。在前面的截图中，您可以清楚地看到，在它们的生命周期中，在有效的 80%的 CPU 带宽下，作业`j1`发出从 1 到 68 的数字；类似地(在相同的约束条件下)，作业`j2`发出从`900`到`965`的数字(实际上是类似的工作量)。然后，脚本进行清理，删除作业并删除子组。

然而，为了真正欣赏效果，我们再次运行我们的脚本(研究以下输出)，但这一次的最大 CPU 带宽仅为 1000(即`$MAX`值)–实际上，最大 CPU 利用率仅为 0.1%！：

```sh
$ sudo ./cgv2_cpu_ctrl.sh 1000 [+] Checking for cgroup v2 kernel support
[+] Adding a 'cpu' controller to the cgroups v2 hierarchy
[+] Create a sub-group under it (here: /sys/fs/cgroup/test_group)

***
Now allowing 1000 out of a period of 1000000 by all processes (j1,j2) in this
sub-control group, i.e., .100% !
***

[+] Launch processes j1 and j2 (slinks to /home/llkd/Learn-Linux-Kernel-Development/ch11/cgroups_v2_cpu_eg/simp.sh) now ...
[+] Insert processes j1 and j2 into our new CPU ctrl sub-group
Verifying their presence...
0::/test_group
Job j1 is in our new cgroup v2 test_group
0::/test_group
Job j2 is in our new cgroup v2 test_group

............... sleep for 5 s ................

[+] killing processes j1, j2 ...
./cgv2_cpu_ctrl.sh: line 185: 10322 Killed ./j1 1 > ${OUT1}
cat 1stjob.txt
1 2 3 
cat 2ndjob.txt
900 901 
[+] Removing our cpu sub-group controller
rmdir: failed to remove '/sys/fs/cgroup/test_group': Device or resource busy
./cgv2_cpu_ctrl.sh: line 27: 10343 Killed ./j2 900 > ${OUT2}
$  
```

真不一样！这一次，我们的作业`j1`和`j2`实际上可以发出两到三个整数(作业 j1 的值`1 2 3`和作业 j2 的值`900 901` ，如前面的输出所示)，清楚地证明了 cgroups v2 CPU 控制器的有效性。

Containers, essentially lightweight VMs (to some extent), are currently a hot commodity. The majority of container technologies in use today (Docker, LXC, Kubernetes, and others) are, at heart, a marriage of two built-in Linux kernel technologies, namespaces, and cgroups.

至此，我们完成了对一个非常强大和有用的内核特性:cgroups 的简短介绍。让我们进入本章的最后一节:学习如何将普通的 Linux 变成实时操作系统！

# 将主线 Linux 转换成 RTOS

主线或者普通的 Linux(你从[https://kernel.org](https://kernel.org)下载的内核)绝对是*而不是*一个**实时操作系统**(**RTOS**)；是**通用操作系统**(**GPOS**；Windows、macOS、Unix 也是如此)。在 RTOS，硬实时特性开始发挥作用，不仅软件必须获得正确的结果，还有相关的截止日期；它必须保证每一次都能满足这些期限。主线 Linux 操作系统，虽然不是 RTOS，但却做了一件了不起的事情:它很容易被认为是一个软实时操作系统(一个大多数时间都能满足最后期限的操作系统)。然而，真正的硬实时领域(例如，军事行动、许多类型的运输、机器人、电信、工厂自动化、股票交易所、医疗电子等)需要 RTOS。

这方面的另一个关键点是**确定性**:关于实时性，一个经常被忽略的点是软件响应时间不需要总是非常快(例如，在几微秒内响应)；它可能会慢很多(例如，在几十毫秒的范围内)；在 RTOS，这本身并不重要。重要的是该系统是可靠的，以同样一致的方式工作，并始终保证最后期限得到满足。

例如，响应一个调度请求所花费的时间应该是一致的，而不是到处跳跃。与所需时间(或基线)的偏差通常被称为**抖动**；RTOS 使抖动保持很小，甚至可以忽略不计。在 GPOS，这通常是不可能的，抖动会有很大的变化——一点低，一点高。总的来说，即使面对极端的工作负载压力，也能以最小的抖动保持稳定均匀的响应，这种能力被称为决定论，是 RTOS 的标志。为了提供这种确定性的响应，算法必须尽可能地设计成对应于 *O(1)* 时间复杂度。

托马斯·格莱克斯纳，连同社区的支持，已经朝着这个目标努力了很长时间；事实上，自 2.6.18 内核以来，多年来一直有离线补丁将 Linux 内核转换成 RTOS。这些补丁可以在很多版本的内核中找到，这里是:[https://mirrors . edge . kernel . org/pub/Linux/kernel/project/rt/](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/)。这个项目的旧称是`PREEMPT_RT`；后来(2015 年 10 月以后)， **Linux 基金会** ( **LF** )接管了这个项目的管理权——这是非常积极的一步！–并将其重新命名为**实时 Linux** ( **RTL** )协作项目([https://wiki . linuxfoundation . org/Real/RTL/start # the _ RTL _ Collaborative _ Project](https://wiki.linuxfoundation.org/realtime/rtl/start#the_rtl_collaborative_project))或 RTL(不要将该项目与诸如 Xenomai 或 RTAI 之类的共同内核方法或称为 RTLinux 的较老且现已停止的尝试相混淆)。

当然，一个常见问题是“为什么这些补丁不在主线中？”事实证明:

*   RTL 的大部分工作确实已经合并到主线内核中；这包括重要的领域，如调度子系统、互斥体、lockdep、线程中断、PI、跟踪等等。事实上，RTL 正在进行的一个主要目标是在可行的情况下尽可能将其合并(我们在*主线和 RTL–技术差异总结*部分显示了一个总结该目标的表格)。
*   Linus Torvalds 认为，Linux 主要是作为一个 GPOS 来设计和构建的，不应该具有只有 RTOS 真正需要的高度侵入性的特性；因此，尽管补丁确实会被合并进来，但这是一个缓慢的深思熟虑的过程。

我们在本章的*进一步阅读*部分包含了几篇有趣的文章和对 RTL(和硬实时)的参考；一定要看看。

接下来你要做的事情确实很有趣:你将学习如何用 RTL 补丁修补主线 5.4 LTS 内核，配置它，构建和引导它；因此，你最终将运行一个 RTOS–*实时 Linux 或 RTL* ！我们将在我们的 x86_64 Linux 虚拟机(或本机系统)上这样做。

我们不会止步于此；然后，您将了解更多–普通 Linux 和 RTL 之间的技术差异，什么是系统延迟，以及如何实际测量它。为此，我们将首先在树莓 Pi 设备的内核源上应用 RTL 补丁，配置和构建它，并使用 *cyclictest* 应用将其用作系统延迟测量的测试平台(您还将学习使用现代 BPF 工具来测量调度程序延迟)。让我们继续，首先在 x86_64 上为我们的 5.4 内核构建一个 RTL 内核！

## 为主线 5.x 内核构建 RTL(在 x86_64 上)

在本节中，您将逐步学习如何将 Linux 作为 RTOS 系统进行修补、配置和构建。如前一节所述，这些实时补丁已经存在很长时间了；是时候利用它们了。

### 获得 RTL 补丁

导航到[https://mirrors . edge . kernel . org/pub/Linux/kernel/projects/rt/5.4/](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/)(或者，如果您在备用内核上，请转到该目录之上的一个目录级别并选择所需的内核版本):

![](Images/a740d54f-0b82-4b91-bf66-1d18bb63d658.png)

Figure 11.9 – Screenshot of the RTL patches for the 5.4 LTS Linux kernels

您会很快注意到，RTL 补丁仅适用于某些版本的内核(这里是 5.4 . y)；关于这一点的更多内容如下。在前面的截图中，您可以发现两种广泛的补丁文件类型，解释如下:

*   `patch-<kver>rt[nn].patch.[gz|xz]`:前缀为`patch-`；这是在一个统一的(和压缩)文件中修补主线内核(版本`<kver>` ) **所需的完整补丁集合。**
*   `patches-<kver>-rt[nn].patch.[gz|xz]`:前缀为`patches-`；这个压缩文件包含了组成这个版本的 RTL 补丁系列的每个单独的补丁(作为一个单独的文件)。

(还有，你应该知道，`<fname>.patch.gz`和`<fname>.patch.xz`是同一个档案；只是压缩器不同而已-`.sign`文件是 PGP 签名文件。)

我们将使用第一种类型；通过点击链接(或通过`wget(1)`)将`patch-<kver>rt[nn].patch.xz`文件下载到您的目标系统。

请注意，对于 5.4.x 内核(在撰写本文时)，RTL 补丁似乎只针对 5.4.54 和 5.4.69 版本(而不是我们一直在使用的 5.4.0 内核)。

In fact, the particular kernel version that the RTL patches apply against can certainly vary from what I've mentioned here at the time of this writing. That's expected - just follow the steps substituting the release number you're using with what's mentioned here.

别担心，我们一会儿会给你看一个变通办法。情况确实会是这样；社区不可能针对每一个内核版本构建补丁——补丁太多了。这确实有一个重要的含义:要么我们将我们的 5.4.0 内核修补到，比如说，5.4.69，要么，我们简单地下载 5.4.69 内核，并对其应用 RTL 补丁。

第一种方法是可行的，但需要做更多的工作(尤其是在没有修补工具(如 git/番茄酱/棉被或类似工具)的情况下；在这里，我们选择不使用 git 来应用补丁，而是只处理稳定的内核树)。由于 Linux 内核补丁是增量的，从 5.4.0 到 5.4.69，我们必须下载每一个补丁(总共 69 个补丁！)，然后依次按顺序应用:先 5.4.1，再 5.4.2，再 5.4.3，以此类推，直到最后一个！这里，为了帮助保持简单，因为我们知道要修补的内核是 5.4.69，所以下载和提取它会更容易。所以，去 https://www.kernel.org/吧。因此，在这里，我们最终下载了两个文件:

*   主线 5.4.69 的压缩内核源码:[https://mirrors . edge . kernel . org/pub/Linux/kernel/V5 . x/Linux-5 . 4 . 69 . tar . xz](https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.4.69.tar.xz)
*   5.4.69 的 RTL 补丁

(正如在[第 3 章](03.html)、*从源代码构建 5.x Linux 内核–第 2 部分*中详细解释的那样，如果您打算为另一个目标交叉编译内核，通常的步骤是将其构建在功能适当强大的工作站上，因此请在那里下载。)

接下来，提取 RTL 补丁文件和内核代码库`tar.xz`文件，得到内核源码树(这里是 5.4.69 版本；当然，这些细节已经在[第 2 章](02.html)、*从源代码构建 5.x Linux 内核–第 1 部分*中得到了很好的阐述。到目前为止，您的工作目录内容应该如下所示:

```sh
$ ls -lh
total 106M
drwxrwxr-x 24 kaiwan kaiwan 4.0K Oct  1 16:49 linux-5.4.69/
-rw-rw-r--  1 kaiwan kaiwan 105M Oct 13 16:35 linux-5.4.69.tar.xz
-rw-rw-r--  1 kaiwan kaiwan 836K Oct 13 16:33 patch-5.4.69-rt39.patch
$ 
```

(仅供参考，`unxz(1)`实用程序可用于提取`.xz`-压缩补丁文件。)对于好奇的读者:看一眼补丁(文件`patch-5.4.69-rt39.patch`，看看所有代码级的改变带来了一个硬实时内核；这当然不是小事！在即将到来的*主线和 RTL–技术差异总结*部分，将看到技术变化的概述。现在我们已经准备好了，让我们从将补丁应用到稳定的 5.4.69 内核树开始；下一节将介绍这一点。

### 应用 RTL 补丁

确保将提取的补丁文件`patch-5.4.69-rt39.patch`保存在 5.4.69 内核源代码树正上方的目录中(如前所述)。现在，让我们应用补丁。小心–(显然)不要试图将压缩文件作为补丁应用；提取并使用未压缩的补丁文件。为了确保补丁正确应用，我们首先对`patch(1)`使用`--dry-run`(虚拟运行)选项:

```sh
$ cd linux-5.4.69
$ patch -p1 --dry-run < ../patch-5.4.69-rt39.patch 
checking file Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.html
checking file Documentation/RCU/Design/Requirements/Requirements.html
[ ... ]
checking file virt/kvm/arm/arm.c
$ echo $?
0
```

没关系，现在让我们实际应用它:

```sh
$ patch -p1 < ../patch-5.4.69-rt39.patch patching file Documentation/RCU/Design/Expedited-Grace-Periods/Expedited-Grace-Periods.html
patching file Documentation/RCU/Design/Requirements/Requirements.html
[ ... ] 
```

太好了——我们已经为 RTL 准备好了补丁内核！

当然可以采用多种方式和各种捷径；例如，您也可以通过`xzcat ../patch-5.4.69-rt39.patch.xz | patch -p1`命令(或类似命令)实现上述功能。

### 配置和构建 RTL 内核

我们已经在[第 2 章](02.html)、*从源代码构建 5.x Linux 内核–第 1 部分*、[第 3 章](03.html)、*从源代码构建 5.x Linux 内核–第 2 部分*中详细介绍了内核配置和构建步骤，因此在此不再赘述。几乎一切都保持不变；唯一显著的区别是我们必须配置这个内核来利用 RTL(这在新的 RTL 维基网站上有解释，这里:[https://wiki . linuxfoundation . org/real time/documents/how to/applications/prempert _ setup](https://wiki.linuxfoundation.org/realtime/documentation/howto/applications/preemptrt_setup))。

为了减少要构建的内核特性以大致匹配当前的系统配置，我们首先在内核源代码树目录(`linux-5.4.69`)中执行以下操作(我们也在[第 2 章](02.html)、*通过 localmodconfig 方法*在*优化内核配置下从源代码构建 5.x Linux 内核-第 1 部分*)中讨论了这一点:

```sh
$ lsmod > /tmp/mylsmod 
$ make LSMOD=/tmp/mylsmod localmodconfig
```

接下来，用`make menuconfig`启动内核配置:

1.  导航至`General setup`子菜单:

![](Images/7ebb5f95-e893-4724-ab4a-25fcf66f39f5.png)

Figure 11.10 – make menuconfig / General setup: configuring the RTL-patched kernel

2.  到达后，向下滚动至`Preemption Model`子菜单；我们在前面的截图中看到它被突出显示，同时还有一个事实，即当前(默认)选择的抢占模式是`Voluntary Kernel Preemption (Desktop)`。
3.  点击*进入*进入`Preemption Model`子菜单:

![](Images/0fe6cbde-87fe-42e9-b46a-b18c4f27ce9d.png)

Figure 11.11 – make menuconfig / General setup / Preemption Model: configuring the RTL-patched kernel

就在那里！回想上一章，在*可抢占内核*部分，我们描述了这个内核配置菜单有三个项目(前三个见图 11.11)。现在它有四个。第四项——第`Fully Preemptible Kernel (Real-Time)`选项——由于我们刚刚应用的 RTL 补丁而增加了！

4.  因此，要为 RTL 配置内核，向下滚动并选择`Fully Preemptible Kernel (Real-Time)`菜单选项(参见图 11.1)。这对应于内核`CONFIG_PREEMPT_RT`配置宏，它的`< Help >`是相当描述性的(看一看)；事实上，它的结论是这样的:*如果您正在为需要实时保证的系统构建内核，请选择此项*。

`Preemption Model`

5.  一旦您选择了第四个选项并保存并退出了`menuconfig`用户界面，(重新)检查是否选择了完全可抢占的内核——实际上是 RTL:

```sh
$ grep PREEMPT_RT .config
CONFIG_PREEMPT_RT=y
```

好吧，看起来不错！(当然，在构建之前，您可以根据产品的需要调整其他内核配置选项。)

6.  现在让我们构建 RTL 内核:

```sh
make -j4 && sudo make modules_install install 
```

7.  一旦它成功构建和安装，重新启动系统；开机时，按键显示 GRUB 引导加载程序菜单(按住其中一个 *Shift* 键可以帮助确保开机时显示 GRUB 菜单)；在 GRUB 菜单内，选择新构建的`5.4.69-rtl` RTL 内核(实际上，刚安装的内核通常是引导时选择的默认内核)。它现在应该启动了；登录到 shell 后，让我们验证内核版本:

```sh
$ uname -r
5.4.69-rt39-rtl-llkd1
```

注意`CONFIG_LOCALVERSION`设置为数值`-rtl-llkd1`。(还有，有了`uname -a`，就能看到`PREEMPT RT`弦了。)我们现在——正如承诺的那样——运行 Linux，RTL，作为一个硬实时操作系统，一个 RTOS！

然而，非常重要的一点是要明白，对于真正的硬实时来说，仅仅拥有一个硬实时内核是不够的*；您还必须非常仔细地设计和编写您的用户空间(应用、库和工具)以及内核模块/驱动程序，以符合实时性。例如，频繁的页面错误会将确定性抛之脑后，导致高延迟(和高抖动)。(回想一下您在第 9 章、*模块作者的内核内存分配–第 2 部分*中，在*内存分配和按需分页*一节中所学的内容。页面错误是生活中的一个事实，可能而且确实经常发生；小的页面错误通常没什么好担心的。但是在硬 RT 场景下呢？无论如何，“重大故障”会影响性能。)很可能需要一些技术，比如使用`mlockall(2)`来锁定实时应用进程的所有页面。这里提供了这个以及其他几个编写实时代码的技巧和提示:[https://rt.wiki.kernel.org/index.php/HOWTO:_Build_an_ RT-application](https://rt.wiki.kernel.org/index.php/HOWTO:_Build_an_RT-application)。(同样，关于 CPU 亲和力和屏蔽、`cpuset`管理、IRQ 优先级排序等话题，可以在前面提到的老 RT wiki 网站上找到；[https://rt.wiki.kernel.org/index.php/Main_Page](https://rt.wiki.kernel.org/index.php/Main_Page)。)*

太好了，你现在知道如何配置和构建 Linux 作为 RTOS 了！我鼓励你自己尝试一下。接下来，我们将总结标准内核和 RTL 内核之间的主要区别。

## 主线和 RTL–总结了技术差异

为了让您更深入地理解这个有趣的主题领域，在这一节中，我们将深入研究它:我们总结了标准(或主线)和 RTL 内核之间的主要区别。

在下表中，我们总结了标准内核(或主线内核)和 RTL 内核之间的一些关键区别。RTL 项目的主要目标是最终完全集成到常规主线内核树中。由于这一过程是进化的，从 RTL 到主线的斑块融合缓慢但稳定；有趣的是，从下表中最右边的一列可以看出，RTL 的大部分工作(在撰写本文时约为 80%)实际上已经合并到主线内核中，并且还在继续:

| **组件/功能** | **标准或主线(普通)Linux** | **RTL(完全可抢占/硬实时 Linux)** | **RT 工作并入主线？** |
| 自旋锁 | 自旋锁关键部分是不可抢占的内核代码 | 尽可能的被人类抢占；叫做“沉睡的刺锁”！实际上，自旋锁已经被转换成了互斥锁。 | 不 |
| 中断处理 | 传统上通过上半部分和下半部分(hardirq/tasklet/softirq)机制完成 | 线程中断:大部分中断处理是在内核线程中完成的(2009 年 6 月 2.6.30)。 | 是 |
| 高分辨率计时器 | 由于与 RTL 合并，此处可用 | 纳秒分辨率计时器(2.6.16，2006 年 3 月)。 | 是 |
| RW 锁 | 无界；作家可能会挨饿 | 写入器延迟受限的公平读写锁。 | 不 |
| 锁定的 | 由于与 RTL 合并，此处可用 | 非常强大的(内核空间)工具，用于检测和证明锁定正确性或其缺乏。 | 是 |
| 追踪 | 由于来自 RTL 的合并，这里提供了一些跟踪技术 | Ftrace 的起源(在某种程度上也是 perf 的起源)是 RT 开发人员试图发现延迟问题。 | 是 |
| 调度程序 | 由于从 RTL 合并，这里有许多调度程序功能 | 实时调度和截止时间调度类(`SCHED_DEADLINE`)的工作首先在这里完成(2014 年 3 月 3.14 日)；还有，全备忘录操作(2013 年 6 月 3.10 日)。 | 是 |

(不要担心——我们肯定会在本书的后续章节中涵盖前面的许多细节。)

当然，一个众所周知(至少应该如此)的经验法则简单来说就是这样:*没有银弹*。当然，这意味着没有一个解决方案能满足所有需求。

*The Mythical Man-Month: Essays on Software Engineering* 

如[第 10 章](10.html)*CPU 调度器-第 1 部分*所述，在*可抢占内核*部分，Linux 内核可以配置`CONFIG_PREEMPT`选项；这通常被称为**低延迟**(或 **LowLat** )内核，提供接近实时的性能。在许多领域(虚拟化、电信等)，使用低延迟内核可能比使用硬实时 RTL 内核更好，这主要是由于 RTL 的开销。您经常会发现，在高实时性的情况下，用户空间应用会受到吞吐量、CPU 可用性降低以及延迟增加的影响。(参见*进一步阅读*部分，了解 Ubuntu 的白皮书，该白皮书对普通发行版内核、低延迟可抢占内核和完全可抢占内核(实际上是 RTL 内核)进行了比较。)

考虑到延迟，下一节将帮助您理解系统延迟的确切含义；然后，您将学习一些在实时系统上测量它的方法。上，上！

# 潜伏期及其测量

我们经常遇到潜伏期这个术语；它在内核的上下文中到底意味着什么？延迟的同义词是延迟，这是一个很好的提示。*延迟(或延迟)是作出反应所花费的时间*-在我们这里的上下文中，内核调度程序唤醒用户空间线程(或进程)，从而使其可运行，与它在处理器上实际运行的时间之间的时间是**调度延迟**。(但是，请注意，术语调度延迟也用于另一个上下文中，表示每个可运行任务保证至少运行一次的时间间隔；可调参数在这里:`/proc/sys/kernel/sched_latency_ns`，至少在最近的 x86_64 Linux 上，默认为 24 ms)。类似地，从硬件中断发生(比如网络中断)到它的处理程序例程实际提供服务所经过的时间，就是中断延迟。

**cycle test**用户空间程序由托马斯·格雷克斯纳编写；它的目的:测量内核延迟。它的输出值以微秒为单位。平均和最大延迟值通常是感兴趣的值——如果它们在系统可接受的范围内，那么一切都好；如果不是，它可能指向特定于产品的重新设计和/或内核配置调整、检查其他时间关键的代码路径(包括用户空间)等等。

让我们以 cyclictest 流程本身为例，清楚地了解调度延迟。循环测试过程正在运行；在内部，它发出`nanosleep(2)`(或者，如果通过了`-n`选项开关，`clock_nanosleep(2)`系统调用)，在指定的时间间隔内将其自身置于睡眠状态。由于这些`*sleep()`系统调用显然是阻塞的，内核在内部将 cyclictest(为简单起见，我们在下图中将它称为`ct`进程)排入等待队列，这只是一个保存休眠任务的内核数据结构。

等待队列与事件相关联；当该事件发生时，内核唤醒所有在该事件上休眠的任务。这里，所讨论的事件是计时器的到期；这由定时器硬件通过发出硬件中断(或 IRQ)来传达；这将启动一系列事件，这些事件必须发生才能使 cyclictest 进程唤醒并在处理器上运行。当然，这里的关键点是说起来容易做起来难:许多潜在的延迟可能发生在处理器内核上实际运行的进程的路径上！下图试图传达的是潜在的延迟来源:

![](Images/625a2837-3502-4447-9da4-99ceffacab58.png)

(前面的一些输入来自于出色的演示*使用和理解实时周期测试基准，罗文，2013 年 10 月*。)仔细研究图 11.12；它显示了由于定时器到期(在时间`t0`硬件中断断言的时间线，因为周期测试进程通过`nanosleep()`应用编程接口发出的睡眠是在时间`t1`完成的，通过 IRQ 处理(`t1`到`t3`)和 ct 进程的唤醒，结果它被排队到它最终将运行的内核的运行队列中(在`t3`和`t4`之间)。

从那里，它将最终成为它所属的调度类的最高优先级，或最好或最值得的任务(在时间`t6`；我们在前面的章节中介绍了这些细节)，因此，它将抢占当前正在运行的线程(`t6`)。然后执行`schedule()`代码(时间`t7` 到`t8`，上下文切换发生在`schedule()`的尾端，最后(！)，循环测试过程实际上将在处理器内核上执行(时间`t9`)。虽然一开始可能看起来很复杂，但实际情况是，这是一个简化的图表，因为省略了其他几个潜在的延迟源(例如，由于 IPI、SMI、缓存迁移、前面事件的多次发生、在不合适的时刻触发导致更多延迟的额外中断等造成的延迟)。

确定以实时优先级运行的用户空间任务的最大延迟值的经验法则如下:

```sh
max_latency = CLK_WAVELENGTH x 105 s
```

举个例子，树莓 Pi Model 3 的 CPU 时钟运行频率为 1 GHz 它的波长(一个时钟周期到下一个时钟周期之间的时间)是频率的倒数，即 10 <sup>-9</sup> 或 1 纳秒。因此，根据前面的等式，理论上的最大延迟应该是(在)10 <sup>-7</sup> 秒内，约为 10 纳秒(纳秒)。你很快就会发现，这只是理论上的。

## 使用 cyclictest 测量调度延迟

为了使这更有趣(以及在受约束的系统上运行延迟测试)，我们将使用众所周知的 cyclictest 应用在同样著名的树莓 Pi 设备上执行延迟测量，同时系统处于一定的负载下(通过`stress(1)`实用程序)。本节分为四个逻辑部分:

1.  首先，在树莓皮装置上设置工作环境。
2.  其次，在内核源代码上下载并应用 RT 补丁，配置并构建它。

3.  第三，在设备上安装 cyclictest app，以及其他几个需要的包(包括`stress`)。
4.  第四，运行测试用例并分析结果(甚至绘制图表来帮助这样做)。

第一步和第二步的大部分内容已经在[第 3 章](03.html)、*从源代码构建 5.x Linux 内核–第 2 部分*、在*树莓皮的内核构建*部分详细介绍过了。这包括下载树莓 Pi 特定的内核源代码树，配置内核，并安装适当的工具链；我们在此不再重复这些信息。这里唯一显著的区别是，我们首先必须将实时补丁应用到内核源代码树，并为硬实时进行配置；我们将在下一节讨论这个问题。

我们走吧！

### 获取和应用 RTL 补丁程序集

检查您的树莓 Pi 设备上运行的主线或发行版内核版本(用您可能运行 Linux 的任何其他设备替换树莓 Pi)；例如，在我使用的树莓 Pi 3B+上，它运行的是 5.4.51-v7+内核的股票 Raspbian(或树莓 Pi OS) GNU/Linux 10(巴斯特)。

我们想为树莓 Pi 构建一个 RTL 内核，它与当前运行的标准内核最接近；对于我们这里的案例，在运行 5.4.51[-v7+]的情况下，可用的最接近的 RTL 补丁是内核版本 5.4 . y-rt[nn]([https://mirrors . edge . kernel . org/pub/Linux/kernel/projects/rt/5.4/](https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/))；我们将很快回到这个问题上来...

我们一步一步来:

1.  将树莓 Pi 特定内核源代码树下载到您的主机系统磁盘的步骤已经在[第 3 章](03.html)、*从源代码构建 5.x Linux 内核–第 2 部分*、在*树莓 Pi 内核构建*部分*中介绍过；*一定要参考它，获取源树。
2.  一旦这个步骤完成，你应该会看到一个名为`linux`的目录；它保存了 5.4.y 版本内核的树莓 Pi 内核源码(截至撰写本文时)`y`有什么价值？那很容易；只需执行以下操作:

```sh
$ head -n4 linux/Makefile 
# SPDX-License-Identifier: GPL-2.0
VERSION = 5
PATCHLEVEL = 4
SUBLEVEL = 70
```

这里的`SUBLEVEL`变量是`y`的值；很明显，它是 70，使得内核版本为 5.4.70。

3.  接下来，让我们下载适当的实时(RTL)补丁:最好的是一个精确匹配，也就是说，补丁应该命名为类似`patch-5.4.70-rt[nn].tar.xz`的东西。幸运的是，它确实存在于服务器上；我们来获取一下(注意我们下载的是`patch-<kver>-rt[nn]`文件；它更容易使用，因为它是统一的补丁):
    `wget https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/patch-5.4.70-rt40.patch.xz`。

*not* 

别忘了解压缩补丁文件！

4.  现在应用补丁(如前所示，在*应用 RTL 补丁*部分):

```sh
cd linux
patch -p1 < ../patch-5.4.70-rt40.patch
```

5.  配置打补丁的内核，打开`CONFIG_PREEMPT_RT`内核配置选项(如前所述):
    1.  首先，正如我们在[第 3 章](03.html)、*中从源代码构建 5.x Linux 内核–第 2 部分*中了解到的，关键是*要为目标适当地设置初始内核配置；这里，由于目标设备是树莓 Pi 3[B+]，请执行以下操作:*

```sh
make ARCH=arm bcm2709_defconfig
```

6.  我还假设您已经为安装了树莓 Pi 的 x86_64 到 ARM32 安装了合适的工具链:

```sh
make -j4 ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- zImage modules dtbs
```

`sudo apt install ​crossbuild-essential-armhf`

*Configuring and building the RTL kernel*

7.  安装刚刚构建的内核模块；确保您使用`INSTALL_MOD_PATH`环境变量将该位置指定为 SD 卡的根文件系统(否则它可能会覆盖您主机的模块，这将是灾难性的！).假设 microSD 卡的第二个分区(包含根文件系统)安装在`/media/${USER}/rootfs`下，然后执行以下操作(一行):

```sh
sudo env PATH=$PATH make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- INSTALL_MOD_PATH=/media/${USER}/rootfs modules_install
```

8.  将映像文件(引导加载程序文件、内核`zImage`文件、**设备树 Blobs** ( **DTBs** )、内核模块)复制到树莓 Pi SD 卡上(这些详细信息包含在官方树莓 Pi 文档中:这里:[https://www . raspberripi . org/documentation/Linux/kernel/building . MD](https://www.raspberrypi.org/documentation/linux/kernel/building.md)；我们在[第 3 章](03.html)、*中也(略微)介绍了从源代码构建 5.x Linux 内核–第 2 部分*。
9.  测试:用 SD 卡中的新内核映像引导树莓 Pi。你应该可以登录一个 shell(通常是通过`ssh`)。验证内核版本和配置:

```sh
rpi ~ $ uname -a 
Linux raspberrypi 5.4.70-rt40-v7-llkd-rtl+ #1 SMP PREEMPT_RT Thu Oct 15 07:58:13 IST 2020 armv7l GNU/Linux
rpi ~ $ zcat /proc/config.gz |grep PREEMPT_RT
CONFIG_PREEMPT_RT=y
```

我们确实在设备上运行一个硬实时内核！所以，很好——这解决了“准备”部分；您现在可以进行下一步了。

### 在设备上安装 cyclictest(和其他必需的软件包)

我们打算通过 cyclictest 应用针对标准和新开发的 RTL 内核运行测试用例。当然，这意味着我们必须首先获得循环测试源，并将其构建在设备上(注意，这里的工作是在树莓 Pi 上进行的)。

*Latency of Raspberry Pi 3 on Standard and Real-Time Linux 4.9*

*Kernel*

[https://metebalci.com/blog/latency-of-raspberry-pi-3-on-standard-and-real-time-linux-4.9-kernel/](https://metebalci.com/blog/latency-of-raspberry-pi-3-on-standard-and-real-time-linux-4.9-kernel/)

`dwc_otg.fiq_enable=0`

`dwc_otg.fiq_fsm_enable=0`

`/boot/cmdline.txt`

首先，确保所有必需的软件包都安装到您的树莓 Pi 上:

```sh
sudo apt install coreutils build-essential stress gnuplot libnuma-dev
```

`libnuma-dev`包是可选的，可能在树莓 Pi 操作系统上不可用(即使没有它，您也可以继续)。

现在让我们获取 cyclictest 的源代码:

```sh
git clone git://git.kernel.org/pub/scm/utils/rt-tests/rt-tests.git
```

有点奇怪的是，最初，只有一个文件，即`README`。读一读(惊喜，惊喜)。它通知您如何获取和构建稳定的版本；很简单，只需执行以下操作:

```sh
git checkout -b stable/v1.0 origin/stable/v1.0
make
```

对我们来说令人高兴的是，**开源自动化开发实验室** ( **OSADL** )在 cyclictest 上有一个非常有用的 bash 脚本包装器；它运行 cyclictest，甚至绘制延迟图。从这里抓取脚本:[https://www.osadl.org/uploads/media/mklatencyplot.bash](https://www.osadl.org/uploads/media/mklatencyplot.bash)(上面的注释:[https://www . osadl . org/Create-a-latency-plot-from-cyclictest-hi . bash-script-for-latency-plot . 0 . html？&no _ cache = 1&sword _ list[0]= cyclictest](https://www.osadl.org/Create-a-latency-plot-from-cyclictest-hi.bash-script-for-latency-plot.0.html?&no_cache=1&sword_list%5B0%5D=cyclictest)。为了我们的目的，我稍微修改了一下；它就在这本书的 GitHub 存储库中:`ch11/latency_test/latency_test.sh`。

### 运行测试用例

为了更好地了解系统(调度)延迟，我们将运行三个测试用例；在这三种情况下，cyclictest 应用将在`stress(1)`实用程序加载系统时对系统延迟进行采样:

1.  运行 5.4 32 位 RTL 补丁内核的树莓 Pi 3 b+(4 个 CPU 内核)
2.  运行标准 5.4 32 位树莓 Pi OS 内核的树莓 Pi 3 b+(4 个 CPU 内核)
3.  运行标准 5.4(主线)64 位内核的 x86_64 (4 个 CPU 内核)Ubuntu 20.04 LTS

为了方便起见，我们在`latency_test.sh`脚本上使用了一个名为`runtest`的小包装脚本。它运行`latency_test.sh`脚本来测量系统延迟，同时运行`stress(1)`实用程序；它使用以下参数调用`stress`，对系统施加中央处理器、输入/输出和内存负载:

```sh
stress --cpu 6 --io 2 --hdd 4 --hdd-bytes 1MB --vm 2 --vm-bytes 128M --timeout 1h
```

(仅供参考，也有一个更高版本的`stress`叫做`stress-ng`。)当`stress`应用执行时，加载系统，`cyclictest(8)`应用采样系统延迟，将其`stdout`写入文件:

```sh
sudo cyclictest --duration=1h -m -Sp90 -i200 -h400 -q >output
```

(请务必参考`stress(1)`和`cyclictest(8)`上的手册页来了解参数。)它将运行一个小时(为了获得更准确的结果，我建议您运行更长时间的测试——可能是 12 个小时)。我们的`runtest`脚本(和底层脚本)使用适当的参数在内部运行`cyclictest`；它捕获并显示所用的最小、平均和最大延迟挂钟时间(通过`time(1)`)，并生成直方图。请注意，在这里，我们运行`cyclictest`的持续时间(最大)为一小时。

`runtest`

`latency_tests`

`LAT=~/booksrc/ch11/latency_tests`

`latency_tests`

在运行 RTL 内核的树莓皮 3B+上运行我们的测试用例#1 的脚本截图如下:

![](Images/ac4326f5-6f00-4d31-94df-e6246e1ecc94.png)

研究前面的截图；你可以清楚地看到系统细节，内核版本(注意是 RTL 补丁的`PREEMPT_RT`内核！)，以及 cyclictest 的最小、平均和最大(调度)延迟的延迟测量结果。

### 查看结果

我们对剩下的两个测试用例执行类似的过程，并在图 11.14 中总结了所有三个测试用例的结果:

![](Images/067dec65-4fbd-4e6d-b976-392df18799b5.png)

有趣；尽管 RTL 内核的最大延迟比其他标准内核低得多，但最小延迟和更重要的平均延迟都优于标准内核。这最终为标准内核带来了卓越的整体吞吐量(这一点在前面也强调过)。

`latency_test.sh` bash 脚本调用`gnuplot(1)`实用程序来生成图形，以这种方式，标题行显示最小/平均/最大延迟值(以微秒计)和运行测试的内核。回想一下，测试用例#1 和#2 运行在树莓 Pi 3B+设备上，而测试用例#3 运行在通用(且功能更强大)x86_64 系统上)。请参见`gnuplot`编辑的图表(针对所有三个测试用例):

![](Images/b5132864-dcad-46a8-9af5-62455c48604c.png)

图 11.15 显示了由测试用例#1 的`gnuplot(1)`(从我们的`ch11/latency_test/latency_test.sh`脚本中调用)绘制的图表。正在测试的**设备** ( **DUT** )、覆盆子皮 3B+，有四个中央处理器核心(如操作系统所见)。请注意图表是如何向我们展示这个故事的——绝大多数样本都在左上角，这意味着在大多数情况下，延迟非常小(100，000 到 100 万个延迟样本(y 轴)之间的延迟在几微秒到 50 微秒(x 轴)之间)！).那真是太好了！当然，在另一个极端也会有异常值——尽管样本数量要少得多，但所有 CPU 内核上的样本都有更高的延迟(在 100 到 256 微秒之间)。cyclictest 应用为我们提供了最小、平均和最大系统延迟值。使用 RTL 补丁内核，虽然最大延迟实际上非常好(相当低)，但平均延迟可能相当高:

![](Images/28466cce-002f-408f-90a3-2f8dcee6ef11.png)

图 11.16 显示了测试用例#2 的图。同样，与前面的测试案例一样——事实上，这里更明显——绝大多数系统延迟样本都表现出非常低的延迟！标准内核因此做了大量的工作；甚至平均延迟也是一个“体面”的值。然而，最差情况(最大)延迟值可能非常大–*向我们展示了为什么它不是 RTOS* 。对于大多数工作负载来说，延迟往往“通常”非常好，但少数极端情况会出现。换句话说，这是*而不是*——RTOS 的关键特征:

![](Images/b12edf18-e475-4855-ab51-fa371e218f6b.png)

图 11.17 显示了测试用例#3 的图。这里的方差——或**抖动**——甚至更明显(同样，不确定！)，尽管最小和平均系统延迟值确实非常好。当然，它运行在比前两个测试用例更强大的系统上——桌面级 x86_64。最大延迟值(少数几个角落的情况，尽管这里有更多)往往相当高。同样，它不是 RTOS——它不是决定性的。

你注意到这些图如何清楚地表现出*抖动*:测试用例#1 具有最少的量(图倾向于非常快地下降到 x 轴——意味着非常少的延迟样本，如果不是零，则表现出高(er)延迟)并且测试用例#3 具有最大的抖动(图的大部分保持在 *x* 轴之上！).

我们再次强调这一点:结果非常清楚地表明，RTOS 是确定性的(非常小的抖动量)，GPOS 是高度不确定性的！(根据经验，标准 Linux 会导致中断处理产生大约+/- 10 微秒的抖动，而在运行 RTOS 的微控制器上，抖动会小得多，大约+/- 10 纳秒！)

做这个实验，你会意识到对标是一件棘手的事情；你不应该在几次测试运行中读得太多(长时间运行测试，有一个大的样本集很重要)。用您期望在系统上体验到的实际工作负载进行测试，将是一种更好的方式来了解哪种内核配置产生了卓越的性能；它确实随工作量而变化！

(Canonical 的一个有趣的案例研究显示了某些工作负载的常规、低延迟和实时内核的统计数据；在本章的*进一步阅读*部分查找。)如前所述，RTL 内核优越的 *max* 延迟特性通常会导致整体吞吐量下降(由于 RTL 相当无情的优先级划分，用户空间可能会受到 CPU 减少的影响)。

## 通过现代 BPF 工具测量调度器延迟

在不涉及太多细节的情况下，我们可能会遗漏掉最近的强大的 BPF Linux 内核特性及其相关的前端；有一些方法可以专门测量调度程序和运行队列相关的系统延迟。(我们在第 1 章、*内核工作区设置*的*现代跟踪和性能分析【e】BPF*一节中介绍了[e]BPF 工具的安装)。

下表总结了其中一些工具(BPF 前端)；所有这些工具都需要以 root 身份运行(与任何 BPF 工具一样)；它们将输出显示为直方图(默认时间以微秒计):

| **BPF 工具** | **它测量的是什么** |
| `runqlat-bpfcc` | 任务在运行队列上等待轮到它在处理器上运行的时间 |
| `runqslower-bpfcc` | (作为 runqueue 读取速度较慢)；任务在运行队列上等待轮到它在处理器上运行的时间，只显示那些超过给定阈值的线程，该阈值默认为 10 ms(可以通过将时间阈值作为参数传递来调整，单位为微秒)；实际上，您可以看到哪些任务面临(相对)较长的计划延迟 |
| `runqlen-bpfcc` | 显示调度程序运行队列长度+占用率(当前排队等待运行的线程数) |

这些工具还可以在每个任务的基础上为系统中的每个进程提供这些度量，甚至可以通过 PID 名称空间(用于容器分析；当然，这些选项取决于所讨论的工具)。一定要查找更多的细节(甚至例子用法！)从这些工具的手册页(第 8 节)中。

`cpudist- cpudist-bpfcc`

`cpuunclaimed-bpfcc`

`offcputime-bpfcc`

`wakeuptime-bpfcc`

*Further reading*

现在，你不仅能够理解，甚至能够测量系统延迟(通过`cyclictest`应用和一些现代 BPF 工具)。

我们用一些杂七杂八但有用的小(内核空间)例程来结束这一章:

*   `rt_prio()`:给定优先级作为参数，返回一个布尔值，表示是否是实时任务。
*   `rt_task()`:基于任务的优先级值，给定任务结构指针作为参数，返回一个布尔值，表示是否为实时任务(对`rt_prio()`的包装)。
*   `task_is_realtime()`:类似，但基于任务的调度策略。给定任务结构指针作为参数，返回一个布尔值来指示它是否是实时任务。

# 摘要

在这，我们关于 Linux 操作系统上的中央处理器调度的第二章，你已经学到了几个关键的东西。其中，您学习了如何使用强大的工具(如 LTTng 和 Trace Compass GUI)以及`trace-cmd(1)`实用程序(内核强大的 Ftrace 框架的便捷前端)来可视化内核流。然后，您看到了如何编程查询和设置任何线程的 CPU 关联掩码。这自然引发了关于如何以编程方式查询和设置任何线程的调度策略和优先级的讨论。“完全公平”的整个概念(通过 CFS 实现)受到了质疑，一些被称为 cgroups 的优雅解决方案被揭示出来。您甚至还学习了如何利用 cgroups v2 CPU 控制器根据需要将 CPU 带宽分配给子组中的进程。然后我们了解到，虽然 Linux 是一个 GPOS，但是 RTL 补丁非常多，一旦应用了它，并且配置和构建了内核，你就可以运行 Linux 作为一个真正的硬实时系统，一个 RTOS。

最后，您学习了如何通过 cyclictest 应用和一些现代 BPF 工具来测量系统延迟。我们甚至在树莓 Pi 3 设备上用 cyclictest 进行测试，在 RTL 和标准内核上对它们进行测量和对比。

真有点！一定要花时间去正确理解材料，并以动手的方式去做。

# 问题

作为我们的总结，这里有一个问题列表，供您测试您对本章材料的知识:[https://github . com/packt publishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions)。你会在这本书的 GitHub repo 中找到一些问题的答案:[https://GitHub . com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions _ to _ assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn)。

# 进一步阅读

为了帮助您用有用的材料更深入地研究这个主题，我们在本书的 GitHub 存储库中的进一步阅读文档中提供了一个相当详细的在线参考资料和链接列表(有时甚至是书籍)。*进一步阅读*文档可在此处获得:[https://github . com/packt publishing/Linux-Kernel-Programming/blob/master/进一步阅读. md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md) 。**