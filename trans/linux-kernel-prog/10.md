The CPU Scheduler - Part 1

在本章和下一章中，您将深入了解一个关键操作系统主题的细节，即 Linux 操作系统上的 CPU 调度。我将尝试通过提出(和回答)典型问题以及执行与日程安排相关的常见任务来保持学习的实践性。从内核(和驱动程序)开发人员的角度来看，了解调度在操作系统级别的工作方式不仅很重要，而且它还会自动使您成为更好的系统架构师(即使对于用户空间应用程序)。

我们将从介绍基本的背景材料开始；这将包括 Linux 上的**内核可调度实体** ( **KSE** )以及 Linux 实现的 POSIX 调度策略。然后，我们将继续使用工具–`perf`和其他工具来可视化操作系统在 CPU 上运行任务并在它们之间切换时的控制流。这对于了解分析应用程序也很有用！之后，我们将更深入地探讨 CPU 调度在 Linux 上到底是如何工作的细节，涵盖模块化调度类、**完全公平调度** ( **CFS** )、核心调度功能的运行等等。在此过程中，我们还将介绍如何以编程方式(和动态方式)查询和设置系统上任何线程的调度策略和优先级。

在本章中，我们将涵盖以下领域:

*   了解中央处理器调度内部——第 1 部分——基本背景
*   可视化流程
*   了解中央处理器调度内部——第 2 部分
*   线程–哪个调度策略和优先级
*   了解中央处理器调度内部——第 3 部分

现在，让我们从这个有趣的话题开始吧！

# 技术要求

我假设您已经完成了[第 1 章](01.html)、*内核工作区设置*，并适当准备了一个运行 Ubuntu 18.04 LTS(或更高版本的稳定版本)的来宾**虚拟机** ( **VM** )并安装了所有需要的软件包。如果没有，我强烈建议你先做这个。

为了充分利用这本书，我强烈建议您首先设置工作区环境，包括克隆这本书的 GitHub 代码存储库，并以动手的方式进行处理。知识库可以在这里找到:[https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming)。

# 了解中央处理器调度内部——第 1 部分——基本背景

让我们快速了解一下理解 Linux 上的 CPU 调度所需的基本背景信息。

Note that in this book, we do not intend to cover material that competent system programmers on Linux should already be well aware of; this includes basics such as process (or thread) states, the state machine and transitions on it, and more information on what real time is, the POSIX scheduling policies, and so on. This (and more) has been covered in some detail in my earlier book: *Hands-On System Programming with Linux*, published by Packt in October 2018.

## Linux 上的 KSE 是什么？

正如您在[第 6 章](06.html)、*内核内部要素—进程和线程*中所学的那样，在*组织进程、线程及其堆栈—用户和内核空间*部分中，每个进程—实际上，系统中的每个活动线程—都被赋予了任务结构(`struct task_struct`)以及用户模式和内核模式堆栈。

这里要问的关键问题是:执行调度时，*作用于*什么对象，换句话说，**内核可调度实体**、 **KSE** 是什么？在 Linux 上，**KSE 是一个线程**，而不是一个进程(当然，每个进程至少包含一个线程)。因此，线程是执行调度的粒度级别。

一个例子将有助于解释这一点:如果我们有一个假设的情况，其中我们有一个中央处理器内核和 10 个用户空间进程，每个由三个线程组成，加上五个内核线程，那么我们总共有(10 x 3) + 5，这等于 35 个线程。除了五个内核线程之外，每个线程都有一个用户和内核堆栈以及一个任务结构(内核线程只有内核堆栈和任务结构；所有这些在[第 6 章](06.html)、*内核内部要素-进程和线程*中，在*组织进程、线程及其堆栈-用户和内核空间*一节中有详细的解释。现在，如果所有这 35 个线程都是可运行的，那么它们将争夺单个处理器(虽然它们不太可能同时运行，但为了讨论起见，让我们考虑一下)，那么我们现在有 35 个*线程*在争夺 CPU 资源，而不是 10 个进程和 5 个内核线程。

既然我们理解了 KSE 是一个线程，我们将(几乎)总是在调度的上下文中引用该线程。既然理解了这一点，让我们继续讨论 Linux 实现的调度策略。

## POSIX 调度策略

重要的是要认识到，Linux 内核并不只有一个算法来实现 CPU 调度；事实是，POSIX 标准规定了符合 POSIX 的 OS 必须遵守的最少三种调度策略(算法，实际上)。Linux 超越了这一点，用一种叫做调度类的强大设计实现了这三个以及更多(在本章后面的*理解模块化调度类*一节中有更多关于这一点的内容)。

Again, information on the POSIX scheduling policies on Linux (and more) is covered in more detail in my earlier book, *Hands-On System Programming with Linux*, published by Packt in October 2018\.

现在，让我们在下表中简单总结一下 POSIX 调度策略及其效果:

| **调度策略** | **要点** | **优先等级** |
| `SCHED_OTHER`或`SCHED_NORMAL` | 总是默认值；使用此策略的线程是非实时的；内部实现为一个**完全公平调度** ( **CFS** )类(见后面*关于 CFS 的一句话和 vruntime 值*部分)。这种调度策略背后的动机是公平性和总吞吐量。 | 实时优先级为`0`；非实时优先级被称为 nice 值:它的范围从-20 到+19(较低的数字意味着较高的优先级)，基数为 0 |
| `SCHED_RR` | 这个调度策略背后的动机是一个适度激进的(软)实时策略。
具有有限的时间片(通常默认为 100 毫秒)。
一个`SCHED_RR`线程将产生处理器 IFF(当且仅当):
-它阻塞输入/输出(进入睡眠状态)。
-它停止或死亡。
-一个更高优先级的实时线程变得可运行(这将抢占这个线程)。
-其时间片过期。 | (软)实时:
1 到 99(更高的数字
表示更高的优先级) |
| `SCHED_FIFO` | 这个调度策略背后的动机是一个(软)实时策略，它(相比之下)非常激进。
一个`SCHED_FIFO`线程将产生处理器 IFF:
-它阻塞输入/输出(进入睡眠状态)。
-停止或死亡。
-一个更高优先级的实时线程变得可运行(这将抢占这个线程)。
实际上，它有无限的时间片。 | (与`SCHED_RR`相同) |
| `SCHED_BATCH` | 这个调度策略背后的动机是一个适合于非交互式批处理作业的调度策略，较少抢占。 | 不错的数值范围(-20 到+19) |
| `SCHED_IDLE` | 特例:典型的 PID `0`内核线程
(传统上称为`swapper`；实际上，是每 CPU 空闲线程)使用了这个策略。它总是保证是系统中优先级最低的线程，并且只在没有其他线程需要 CPU 时运行。 | 所有优先级中最低的一个(认为它低于尼斯值+19) |

It's important to note that when we say real-time in the preceding table, we really mean *soft* (or at best, *firm*) real time and *not* hard real time as in an **Real-Time Operating System** (**RTOS**). Linux is a **GPOS**, a **general-purpose OS**, not an RTOS. Having said that, you can convert vanilla Linux into a true hard real-time RTOS by applying an external patch series (called the RTL, supported by the Linux Foundation); you'll learn how to do precisely this in the following chapter in the *Converting mainline Linux into an RTOS* section.

请注意，一个`SCHED_FIFO`线程实际上有无限的时间片，并且一直运行，直到它希望或者前面提到的条件之一实现。在这一点上，重要的是要理解我们只关心线程(KSE)调度；在 Linux 这样的操作系统上，现实是硬件(和软件)*中断*总是占优，甚至会一直抢占(内核或用户空间)`SCHED_FIFO`线程！请务必参考图 6.1 来了解这一点。此外，我们将在[第 14 章](10.html)、*处理硬件中断*中详细介绍硬件中断。对于我们在这里的讨论，我们将暂时忽略中断。

优先级缩放很简单:

*   非实时线程(`SCHED_OTHER`)的实时优先级为`0`；这确保了它们甚至不能与实时线程竞争。他们使用一个(旧的 UNIX 风格)优先级值，称为**好值**，范围从-20 到+19 (-20 是最高优先级，+19 是最差优先级)。

The way it's implemented on modern Linux, each nice level corresponds to an approximate 10% change (or delta, plus or minus) in CPU bandwidth, which is a significant amount.

*   实时线程(`SCHED_FIFO / SCHED_RR`)的实时优先级从 1 到 99，1 是最低优先级，99 是最高优先级。这样想:在一个只有一个 CPU 的不可抢占的 Linux 系统上，一个`SCHED_FIFO`优先级为 99 的线程在牢不可破的无限循环中旋转，会有效地挂起机器！(当然，即使这样也会被中断抢占——包括硬中断和软中断；参见图 6.1。

当然，调度策略和优先级(静态 nice 值和实时优先级)都是任务结构的成员。一个线程所属的调度类是排他的:一个线程在给定的时间点只能属于一个调度策略(不用担心，我们稍后会在 *CPU 调度内部部分–第 2 部分*部分中详细介绍调度类)。

此外，您应该意识到，在现代 Linux 内核中，还有其他调度类(停止调度和截止日期)，它们实际上(在优先级上)优于我们前面提到的 FIFO/RR 类。现在你已经有了基本的概念，让我们继续进行一些非常有趣的事情:我们如何实际上*可视化*控制流。继续读！

# 可视化流程

多核系统已经导致进程和线程在不同的处理器上并发执行。这有助于获得更高的吞吐量和性能，但也会导致共享可写数据的同步问题。因此，例如，在具有四个处理器内核的硬件平台上，我们可以期望进程(和线程)在其上并行执行。这不是什么新鲜事；然而，有没有一种方法可以真正看到哪些进程或线程正在哪个 CPU 内核上执行——也就是说，一种可视化处理器时间线的方法？事实证明，确实有几种方法可以做到这一点。在接下来的部分中，我们将用`perf`看一个有趣的方法，随后是其他方法(用 LTTng、Trace Compass 和 Ftrace)。

## 使用 perf 可视化流程

拥有大量开发人员和**质量保证** ( **QA** )工具的 Linux 在`perf(1)`中有一个非常强大的工具。简而言之，`perf`工具集是在 Linux 盒子上执行 CPU 分析的现代方式。(除了几个小技巧，本书没有详细介绍`perf`。)

类似于古老的`top(1)`实用程序，要想了解吞噬 CPU 的因素(比`top(1)`详细得多)，这套 **`perf(1)`** 实用程序非常出色。不过，请注意，对于一款应用来说，`perf`与它运行的内核紧密相连，这是非常不寻常的。首先安装`linux-tools-$(uname -r)`软件包很重要。此外，发行包将不能用于我们已经构建的定制 5.4 内核；所以，在使用`perf`的时候，我建议你用标准(或者发行版)内核之一引导你的客户 VM，安装`linux-tools-$(uname -r)`包，然后尝试使用`perf`。(当然，您总是可以从内核源代码树中的`tools/perf/`文件夹下手动构建 perf。)

安装并运行`perf`后，请尝试以下`perf`命令:

```sh
sudo perf top
sudo perf top --sort comm,dso
sudo perf top -r 90 --sort pid,comm,dso,symbol
```

(顺便说一下，`comm`暗含命令/进程的名称，`**dso**`是**动态共享对象**的缩写。使用`alias`使它更容易；尝试这一个(一行)获得更详细的细节(调用栈也可以扩展！):

```sh
alias ptopv='sudo perf top -r 80 -f 99 --sort pid,comm,dso,symbol --demangle-kernel -v --call-graph dwarf,fractal'
```

`perf(1)`上的`man`页面提供了详细信息；使用`man perf-<foo>`符号，例如`man perf-top`，获得`perf top`的帮助。

使用`perf`的一种方法是获得什么任务在什么 CPU 上运行的想法；这是通过`perf`中的`timechart`子命令完成的。您可以使用`perf`记录事件，既可以是系统范围的，也可以是特定过程的。要在系统范围内记录事件，请运行以下命令:

```sh
sudo perf timechart record
```

用信号(`^C`)终止录制会话。这将默认生成名为`perf.data`的二进制数据文件。现在可以用以下内容对其进行检查:

```sh
sudo perf timechart 
```

该命令生成一个**可伸缩矢量图形** ( **SVG** )文件！可以使用矢量绘图工具(如 Inkscape，或通过 ImageMagick 中的`display`命令)或简单地在网络浏览器中查看。研究时序图会很有意思；我劝你试试。但是请注意，矢量图像可能非常大，因此需要一段时间才能打开。

在运行 Ubuntu 18.10 的本机 Linux x86_64 笔记本电脑上运行的系统范围采样如下所示:

```sh
$ sudo perf timechart record
[sudo] password for <user>:
^C[ perf record: Woken up 18 times to write data ] 
[ perf record: Captured and wrote 6.899 MB perf.data (196166 samples) ] 
$ ls -lh perf.data 
-rw------- 1 root root 7.0M Jun 18 12:57 perf.data 
$ sudo perf timechart
Written 7.1 seconds of trace to output.svg.
```

It is possible to configure `perf` to work with non-root access. Here, we don't; we just run `perf` as root via `sudo(8)`.

`perf`生成的 SVG 文件截图如下图所示。要查看 SVG 文件，您只需将其拖放到网络浏览器中:

![](Images/47ce58a7-0513-45cc-a9c6-8ff3721ecb60.png)

Figure 10.1 – (Partial) screenshot showing the SVG file generated by sudo perf timechart

在前面的截图中，作为一个例子，你可以看到`EMT-0`线程正忙，占用了最大的 CPU 周期(短语 CPU 3 不幸不清楚；仔细看 CPU 2 下方的紫色条)。这是有道理的；是代表 VirtualBox 的**虚拟 CPU** ( **VCPU** )的线程，我们在这里运行 Fedora 29 ( **EMT** 代表**仿真器线程**)！

你可以放大和缩小这个 SVG 文件，研究`perf`默认记录的调度和 CPU 事件。下图是放大 400%到上一张截图的 CPU 1 区域时的部分截图，显示了在 CPU #1 上运行的`htop`(紫色带显示了执行时的切片):

![](Images/2c6587c5-2a8f-45d6-94be-178d62028f31.png)

Figure 10.2 – Partial screenshot of perf timechart's SVG file, when zoomed in 400% to the CPU 1 region

还有什么？通过使用`-I`选项切换到`perf timechart record`，您可以仅请求记录系统范围的磁盘输入/输出(显然还有网络)事件。这可能特别有用，因为真正的性能瓶颈通常是由输入/输出活动(而不是中央处理器；输入输出通常是罪魁祸首！).`perf-timechart(1)`上的`man`页面详细介绍了更多有用的选项；例如，`--callchain`执行堆栈回溯记录。又如，`--highlight <name>`选项开关将突出显示所有名称为`<name>`的任务。

You can convert `perf`'s binary `perf.data` record file into the popular **Common Trace Format** (**CTF**) file format, using `perf data convert -- all --to-ctf`, where the last argument is the directory where the CTF file(s) get stored. Why is this useful? CTF is the native data format used by powerful GUI visualizers and analyzer tools such as Trace Compass (seen later in [Chapter 11](11.html), *The CPU Scheduler – Part 2*, under the *Visualization with LTTng and Trace Compass* section).

However, there is a catch, as mentioned in the Trace Compass Perf Profiling user guide ([https://archive.eclipse.org/tracecompass.incubator/doc/org.eclipse.tracecompass.incubator.perf.profiling.doc.user/User-Guide.html](https://archive.eclipse.org/tracecompass.incubator/doc/org.eclipse.tracecompass.incubator.perf.profiling.doc.user/User-Guide.html)): "*Not all Linux distributions have the ctf conversion builtin. One needs to compile perf (thus linux) with environment variables LIBBABELTRACE=1 and LIBBABELTRACE_DIR=/path/to/libbabeltrace to enable that support*."

Unfortunately, as of the time of writing, this is the case with Ubuntu.

## 通过替代方法可视化流程

当然，还有其他方法来可视化每个处理器上运行的内容；我们在这里提到了一对夫妇，并为第 11 章、*的中央处理器调度程序–第 2 部分*保存了另一个有趣的(LTTng)在*LTTNG 可视化和跟踪罗盘*部分:

*   用`perf(1)`，再次运行`sudo perf sched record`命令；这记录了活动。用`^C`信号终止它，然后用`sudo perf sched map`查看处理器上的执行图。
*   一些简单的 Bash 脚本可以显示给定内核上正在执行什么(对`ps(1)`的简单包装)。在下面的代码片段中，我们展示了示例 Bash 函数；例如，下面的`c0()`功能显示了当前在 CPU 核心`#0`上执行的内容，而`c1()`对核心`#1`也是如此:

```sh
# Show thread(s) running on cpu core 'n' - func c'n'
function c0() 
{ 
    ps -eLF | awk '{ if($5==0) print $0}' 
} 
function c1() 
{ 
    ps -eLF | awk '{ if($5==1) print $0}' 
} 
```

While on the broad topic of `perf`, Brendan Gregg has a very useful series of scripts that perform a lot of the hard work required when monitoring production Linux systems using `perf`; do take a look at them here: [https://github.com/brendangregg/perf-tools](https://github.com/brendangregg/perf-tools) (some distributions include them as a package called `perf-tools[-unstable]`).

一定要试试这些选择(包括`perf-tools[-unstable] package`)！

# 了解中央处理器调度内部——第 2 部分

这一部分深入研究了内核中央处理器调度的内部，重点是现代设计的核心方面，模块化调度器类。

## 理解模块化调度类

重要的内核开发者 Ingo Molnar(和其他人一起)重新设计了内核调度器的内部结构，引入了一种称为**调度类**的新方法(这要追溯到 2007 年 10 月 2.6.23 内核的发布)。

As a side note, the word *class* here isn't a coincidence; many Linux kernel features are intrinsically, and quite naturally, designed with an **object-oriented** nature. The C language, of course, does not allow us to express this directly in code (hence the preponderance of structures with both data and function pointer members, emulating a class). Nevertheless, the design is very often object-oriented (as we shall again see with the driver model in a later chapter). Please see the *Further reading* section of this chapter for more details on this.

在核心调度代码`schedule()`函数下引入了一层抽象。`schedule()`下的这一层一般称为调度类，在设计上是模块化的。注意*模块化*这个词在这里意味着调度程序类可以从内嵌内核代码中添加或删除；与**可加载内核模块** ( **LKM** )框架无关。

基本思想是这样的:当核心调度器代码(由`schedule()`函数封装)被调用时，理解它下面有各种可用的调度类，它以预定义的优先级顺序迭代每个类，询问每个类是否有线程(或进程)需要调度到处理器上(具体如何，我们将很快看到)。

从 5.4 Linux 内核开始，这些是内核中的调度程序类，按优先级顺序列出，优先级最高的优先:

```sh
// kernel/sched/sched.h
[ ... ] 
extern const struct sched_class stop_sched_class; 
extern const struct sched_class dl_sched_class; 
extern const struct sched_class rt_sched_class; 
extern const struct sched_class fair_sched_class; 
extern const struct sched_class idle_sched_class;
```

在这里，我们有五个调度类——停止调度、截止日期、(软)实时、公平和空闲——按优先级从高到低排序。抽象这些调度类的数据结构`struct sched_class`被串在一个单链表上，核心调度代码对其进行迭代。(稍后你会看到`sched_class`结构是什么；暂时忽略它)。

每个线程都与其自己唯一的任务结构相关联(`task_struct`)；在任务结构中，`policy`成员指定线程遵循的调度策略(通常是`SCHED_FIFO`、`SCHED_RR`或`SCHED_OTHER`之一)。它是排他的——一个线程在任何给定的时间点只能遵守一个调度策略(尽管它可以被改变)。类似地，任务结构的另一个成员`struct sched_class`持有线程所属的模块化调度类(也是排他的)。调度策略和优先级都是动态的，可以通过编程方式(或通过实用程序；你很快就会看到这一点)。

所以知道了这些，你现在会意识到，所有遵循`SCHED_FIFO`或`SCHED_RR`调度策略的线程都映射到`rt_sched_class`(对于它们在任务结构中的`sched_class`)，所有属于`SCHED_OTHER`(或`SCHED_NORMAL`)的线程都映射到`fair_sched_class`，空闲线程(`swapper/n`，其中`n`是从`0`开始的 CPU 号)总是映射到`idle_sched_class`调度类。

当内核需要调度时，这是基本的调用序列:

```sh
schedule() --> __schedule() --> pick_next_task() 
```

前面调度类的实际迭代发生在这里；参见`pick_next_task()`的(部分)代码，如下:

```sh
// kernel/sched/core.c
 /* 
  * Pick up the highest-prio task: 
  */ 
static inline struct task_struct * 
pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) 
{ 
    const struct sched_class *class; 
    struct task_struct *p; 

    /* Optimization: [...] */
    [...]

   for_each_class(class) { 
        p = class->pick_next_task(rq, NULL, NULL);
        if (p)
            return p;
    }

    /* The idle class should always have a runnable task: */
    BUG();
}
```

前面的`for_each_class()`宏建立了一个`for`循环来迭代所有的调度类。其实施如下:

```sh
// kernel/sched/sched.h
[...]
#ifdef CONFIG_SMP
#define sched_class_highest (&stop_sched_class)
#else
#define sched_class_highest (&dl_sched_class)
#endif

#define for_class_range(class, _from, _to) \
    for (class = (_from); class != (_to); class = class->next)

#define for_each_class(class) \
    for_class_range(class, sched_class_highest, NULL)
```

从前面的实现中可以看到，从`sched_class_highest`到`NULL`(意味着它们所在的链表的末尾)，每个类中的代码都被通过`pick_next_task()`“方法”询问下一个调度对象。现在，调度类代码确定它是否有任何要执行的候选。怎么做？其实很简单；它只是查找它的**运行队列**数据结构。

现在，这是一个关键点:*内核为每个处理器内核和每个调度类维护一个运行队列*！所以，如果我们有一个系统，比如说，有八个中央处理器核心，那么我们将有 *8 个核心* 5 个调度类= 40 个运行队列*！运行队列实际上是按 CPU 变量实现的，这是一种有趣的无锁技术(例外:在**单处理器** ( **UP** )系统上，`stop-sched`类不存在):

![](Images/81a46dfa-80bb-40c0-b2df-5a9e2178fb63.png)

Figure 10.3 – There is a runqueue per CPU core per scheduling class

请注意，在前面的图中，我显示运行队列的方式可能会使它们看起来像数组。这根本不是目的，这只是一个概念图。实际使用的 runqueue 数据结构取决于调度类(类代码毕竟实现了 runqueue)。它可以是一系列链表(如实时类)，一棵树-一棵红-黑(rb)树-**T3(如公平类)，等等。**

为了帮助更好地理解调度器类模型，我们将设计一个例子:假设在一个**对称多处理器** ( **SMP** )或多核)系统上，我们有 100 个活动线程(在用户和内核空间中)。其中，我们有几个竞争的中央处理器；也就是说，它们处于准备运行(运行)状态，这意味着它们是可运行的，因此在运行队列数据结构上排队:

*   线程 S1:调度器类，`stop-sched` ( **SS** )
*   线程 D1 和 D2:调度程序类，**截止时间** ( **DL** )
*   线程 RT1 和 RT2:调度器类，**实时** ( **RT** )
*   线程 F1、F2 和 F3:调度程序类，CFS(或公平)
*   线程 I1:调度程序类，空闲。

想象一下，首先，线程 F2 在处理器内核上，愉快地执行代码。在某个时候，内核希望上下文切换到该 CPU 上的其他任务(是什么触发了这一点？你很快就会看到的)。在调度代码路径上，内核代码最终在`kernel/sched/core.c:void schedule(void)`内核例程中结束(同样，代码级细节在后面)。现在需要理解的是，`schedule()`调用的`pick_next_task()`例程遍历调度器类的链表，询问每个调度器类是否有候选运行。它的代码路径(当然是概念上的)看起来像这样:

1.  核心调度器代码(`schedule()`):*H**ey，SS，有没有想运行的线程？*”
2.  SS 类代码:迭代它的运行队列，并找到一个可运行的线程；它这样回答:“*是的，我知道，是线程 S* *1。*”
3.  核心调度器代码(`schedule()`):*好的，让我们上下文切换到 S1。*”

任务完成了。但是，如果该处理器的 SS 运行队列中没有可运行的线程 S1(或者它已经进入睡眠状态，或者被停止，或者它在另一个 CPU 的运行队列中)，该怎么办呢？然后，SS 会说“*不*”，接下来最重要的调度班 DL 会被问到。如果它有想要运行的潜在候选线程(在我们的例子中是 D1 和 D2)，它的类代码将识别应该运行 D1 还是 D2，并且内核调度器将忠实地上下文切换到它。对于实时和公平调度类，这个过程将继续。(一图胜千言，对吧:见图 10.4)。

很可能(在您典型的中等负载的 Linux 系统上)，没有 SS、DL 或 RT 候选线程想要在有问题的 CPU 上运行，并且通常至少有一个公平(CFS)线程想要运行；因此，它将被挑选并被上下文切换到。如果没有想要运行的线程(没有 SS/DL/RT/CFS 类线程想要运行)，这意味着系统当前是空闲的(懒惰的一章)。现在，空闲类被问及是否要运行:它总是说是！这很有道理:毕竟，在没有其他人需要的时候，在处理器上运行是 CPU 空闲线程的工作。因此，在这种情况下，内核将上下文切换到空闲线程(通常标记为`swapper/n`，其中`n`是它正在执行的中央处理器号(从`0`开始)。

此外，请注意`swapper/n` (CPU 空闲)内核线程没有出现在`ps(1)`列表中，尽管它一直存在(回想一下我们在[第 6 章](06.html)、*内核内部本质–进程和线程*中演示的代码，这里:`ch6/foreach/thrd_showall/thrd_showall.c`。在那里，我们编写了一个`disp_idle_thread()`例程来显示中央处理器空闲线程的一些细节，因为即使是我们在那里使用的内核`do_each_thread() { ... } while_each_thread()`循环也没有显示空闲线程。

下图简洁地总结了核心调度代码以优先级顺序调用调度类的方式，上下文切换到最终选择的下一个线程:

![](Images/28ae1a7d-27e9-43a1-afe4-7eda1414f686.png)

Figure 10.4 – Iterating over every scheduling class to pick the task that will run next

在下一章中，你将学习如何通过一些强大的工具来可视化内核流。在那里，这种迭代模块化调度器类的工作实际上被看到了。

### 询问排班

核心调度器代码(`pick_next_task()`)到底是怎么问调度类有没有想运行的线程？我们已经看到了这一点，但是我觉得为了清楚起见，值得重复下面的代码片段(主要从`__schedule()`调用，也从线程迁移代码路径调用):

```sh
// kernel/sched/core.c
[ ... ] 
static inline struct task_struct * 
pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) 
{ 
    const struct sched_class *class;
    struct task_struct *p;
    [ ... ] 
for_each_class(class){
        p = class->pick_next_task(rq, NULL, NULL);
        if (p)
            return p;
    }
    [ ... ]

```

请注意动作中的对象方向:`class->pick_next_task()`代码实际上是在调用调度类`class`的方法`pick_next_task()`！方便地说，返回值是指向所选任务的任务结构的指针，代码现在上下文切换到该任务结构。

当然，上一段暗示有一个`class`结构，体现了我们所说的调度类的真正含义。事实上，情况就是这样:它包含了所有可能的操作，以及在调度类中可能需要的有用的钩子。它(令人惊讶地)被称为`sched_class`结构:

```sh
// location: kernel/sched/sched.h
[ ... ] 
struct sched_class {
    const struct sched_class *next;
    [...]
    void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags); 
    void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
    [ ... ]
    struct task_struct * (*pick_next_task)(struct rq *rq,
                           struct task_struct *prev,
                           struct rq_flags *rf);
    [ ... ] 
    void (*task_tick)(struct rq *rq, struct task_struct *p, int queued); 
    void (*task_fork)(struct task_struct *p); 
    [ ... ]
};
```

(这个结构的成员比我们在这里展示的要多得多；一定要在代码中查找)。现在应该很明显，每个调度类都实例化了这个结构，适当地用方法(当然是函数指针)填充它。核心调度代码迭代调度类的链表(以及内核中的其他地方)，根据需要调用方法和钩子函数，只要不是`NULL`。

例如，让我们考虑公平调度类(CFS)如何实现它的调度类:

```sh
// kernel/sched/fair.c
const struct sched_class fair_sched_class = {
    .next = &idle_sched_class,
    .enqueue_task = enqueue_task_fair,
    .dequeue_task = dequeue_task_fair,
    [ ... ]
    .pick_next_task = pick_next_task_fair,
    [ ... ]
    .task_tick = task_tick_fair,
    .task_fork = task_fork_fair,
    .prio_changed = prio_changed_fair,
    [ ... ]
};
```

所以现在你看到了:fair sched 类用来选择下一个要运行的任务的代码(当核心调度程序询问时)，就是函数`pick_next_task_fair()`。仅供参考，`task_tick`和`task_fork`成员是调度类钩子的好例子；这些函数将由调度器内核在每个定时器滴答(即每个定时器中断，理论上每秒至少触发–`CONFIG_HZ`次)和属于该调度类的线程分叉时分别调用。

An interesting in-depth Linux kernel project, perhaps: create your own scheduling class with its particular methods and hooks, implementing its internal scheduling algorithm(s). Link all the bits and pieces as required (into the scheduling classes-linked list, inserted at the desired priority, and so on) and test! Now you can see why they're called modular scheduling classes.

太好了——现在你已经理解了现代模块化 CPU 调度器背后的架构，让我们简单地看一下 CFS 背后的算法，CFS 可能是通用 Linux 上最常用的调度类。

### 关于 cfs 和运行时值的一个词

自 2.6.23 版本以来，CFS 一直是常规线程事实上的内核 CPU 调度代码；大部分线程为`SCHED_OTHER`，由 CFS 驱动。CFS 背后的驱动因素*是公平性和整体吞吐量*。简而言之，在其实现中，内核跟踪每个可运行的 CFS ( `SCHED_OTHER`)线程的实际 CPU 运行时间(以纳秒粒度)；运行时间最小的线程是最值得运行的线程，将在下一次调度切换时被授予处理器。相反，不断敲打处理器的线程会积累大量的运行时间，因此会受到惩罚(这真的很有因果报应)！

没有深入研究太多关于 CFS 实现内部的细节，任务结构中嵌入了另一个数据结构`struct sched_entity`，它包含一个名为`vruntime`的无符号 64 位值。简单来说，这是一个单调计数器，用于跟踪线程在处理器上累积(运行)的时间(以纳秒为单位)。

实际上，这里需要大量的代码级调整、检查和平衡。例如，通常，内核会将`vruntime`值重置为`0`，从而触发另一个调度时期。此外，`/proc/sys/kernel/sched_*`下还有各种可调参数，有助于更好地微调中央处理器调度器的行为。

CFS 如何选择下一个要运行的任务封装在`kernel/sched/fair.c:pick_next_task_fair()`函数中。理论上，CFS 的工作方式本身就是简单:将所有可运行的任务(对于该 CPU)排队到运行队列中，运行队列是一个 rb 树(一种自平衡二叉查找树)，以这种方式，在处理器上花费时间最少的任务是树中最左边的叶节点，右边的后续节点代表下一个要运行的任务，然后是后面的任务。

实际上，从左到右扫描树给出了未来任务执行的时间表。这怎么保证？通过使用上述`vruntime`值作为任务入队到 rb-tree 的关键字！

当内核需要调度时，它会询问 CFS，CFS 类代码——我们已经提到过了，`pick_next_task_fair()`函数——*只需在树*上挑选最左边的叶子节点，将指针返回到嵌入在那里的任务结构；根据定义，它是`vruntime`值最低的任务，实际上是运行最少的任务！(遍历一棵树是一种 *O(log n)* 时间复杂度算法，但由于一些代码优化和最左边叶节点的巧妙缓存，实际上将它渲染成一种非常理想的 *O(1)* 算法！)当然，实际的代码要比这里的代码复杂得多；这需要多方面的制衡。我们不在这里深究血淋淋的细节。

We refer those of you that are interested in learning more on CFS to the kernel documentation on the topic, at [https://www.kernel.org/doc/Documentation/scheduler/sched-design-CFS.txt](https://www.kernel.org/doc/%20Documentation/scheduler/sched-design-CFS.txt).

Also, the kernel contains several tunables under `/proc/sys/kernel/sched_*` that have a direct impact on scheduling. Notes on these and how to use them can be found on the *Tuning the Task Scheduler* page ([https://documentation.suse.com/sles/12-SP4/html/SLES-all/cha-tuning-taskscheduler.html](https://documentation.suse.com/sles/12-SP4/html/SLES-all/cha-tuning-taskscheduler.html)), and an excellent real-world use case can be found in the article at [https://www.scylladb.com/2016/06/10/read-latency-and-scylla-jmx-process/](https://www.scylladb.com/2016/06/10/read-latency-and-scylla-jmx-process/). Now let's move onto learning how to query the scheduling policy and priority of any given thread.

# 线程–哪个调度策略和优先级

在本节中，您将学习如何查询系统上任何给定线程的调度策略和优先级。(但是以编程方式查询和设置相同的内容呢？我们将讨论推迟到下一章*查询和设置线程的调度策略和优先级*部分。)

我们了解到，在 Linux 上，线程是 KSE；它实际上是在处理器上调度和运行的。此外，Linux 有几种调度策略(或算法)可供选择。该策略以及分配给给定任务(进程或线程)的优先级是基于每个线程分配的，默认始终是具有实时优先级的`SCHED_OTHER`策略`0`。

在给定的 Linux 系统上，我们总是可以看到所有进程都是活动的(通过一个简单的`ps -A`)，或者，对于 GNU `ps`，甚至每个线程都是活动的(`ps -LA`)。但是，这并没有告诉我们这些任务运行的调度策略和优先级；我们如何查询？

这很简单:在 shell 中，`chrt(1)`实用程序非常适合查询和设置给定进程的调度策略和/或优先级。使用`-p`选项开关发出`chrt`并提供 PID 作为参数，使其显示调度策略以及所讨论任务的实时优先级；例如，让我们查询`init`流程(或系统)PID `1`:

```sh
$ chrt -p 1 
pid 1's current scheduling policy: SCHED_OTHER 
pid 1's current scheduling priority: 0 
$ 
```

像往常一样，`chrt(1)`上的`man`页面提供了所有选项开关及其用法；一定要看一眼。

在下面的(部分)截图中，我们展示了一个简单的 Bash 脚本(`ch10/query_task_sched.sh`，本质上是`chrt`的包装器)的运行，它查询并显示所有活动线程的调度策略和实时优先级(在它们运行的时候):

![](Images/49aca83c-a29e-44ac-81b6-5a78ddd709c3.png)

Figure 10.5 – (Partial) screenshot of our ch10/query_task_sched.sh Bash script in action

需要注意的几件事:

*   在我们的脚本中，通过使用 GNU `ps(1)`，配合`ps -LA`，我们能够捕获系统上所有活跃的线程；显示它们的 PID 和 TID。正如您在[第 6 章](06.html)、*内核内部要素–进程和线程*中所了解的，进程间接口是内核 TGID 的用户空间等价物，而 TID 是内核进程间接口的用户空间等价物。因此，我们可以得出以下结论:
    *   如果 PID 和 TID 匹配，那么它——在那一行看到的线程(第三列有它的名字)——就是进程的主线程。
    *   如果 PID 和 TID 匹配，并且 PID 只出现一次，这是一个单线程进程。
    *   如果我们有相同的 PID 多次(最左边的一列)和不同的 TiD(第二列)，这些是进程的子(或工作)线程。我们的脚本通过向右缩进 TID 数字来显示这一点。
*   请注意，典型的 Linux 盒子上的绝大多数线程(甚至是嵌入式的)都是非实时的(T0 策略)。在典型的桌面、服务器，甚至嵌入式 Linux 上，大部分线程将是`SCHED_OTHER`(默认策略)，少数实时线程(FIFO/RR)。**截止日期** ( **DL** )和 **Stop-Sched** ( **SS** )螺纹确实很少见。
*   请注意以下关于前面输出中显示的实时线程的观察:
    *   我们的脚本通过在最右侧显示一个星号来突出显示任何实时线程(一个带有策略:`SCHED_FIFO`或`SCHED_RR`)。
    *   此外，任何实时优先级为 99(最大可能值)的实时线程将在最右侧有三个星号(这些往往是专用的内核线程)。
*   当`SCHED_RESET_ON_FORK`标志与调度策略进行布尔“或”运算时，具有不允许任何子代(通过`fork(2)`)继承特权调度策略(一种安全措施)的效果。
*   更改线程的调度策略和/或优先级可以通过`chrt(1)`执行；但是，您应该意识到这是一个需要根权限的敏感操作(或者，现在，首选的机制应该是能力模型，`CAP_SYS_NICE`能力是有问题的能力位)。

我们将留给您来检查脚本的代码(`ch10/query_task_sched.sh`)。另外，要注意(小心！)性能和 shell 脚本并没有真正结合在一起(所以在性能方面不要期望太高)。想想看，shell 脚本中发出的每个外部命令(我们这里有几个，比如`awk`、`grep`、`cut`)都涉及到 fork-exec-wait 语义和上下文切换。而且，这些都是在一个循环中执行的。

The `tuna(8)` program can be used to both query and set various attributes; this includes process-/thread-level scheduling policy/priority and a CPU affinity mask, as well as IRQ affinity.

你可能会问，具有`SCHED_FIFO`策略和`99`实时优先级的(少数)线程会一直霸占系统的处理器吗？不，不是真的；现实是这些线程大部分时间都在休眠。当内核确实需要它们执行一些工作时，它会唤醒它们。现在，正是由于他们的实时策略和优先级，几乎可以保证他们将获得一个中央处理器，并执行所需的时间(工作完成后返回睡眠状态)。关键点:当他们需要处理器时，他们会得到它(有点类似于 RTOS，但没有 RTOS 提供的铁定保证和决定论)。

`chrt(1)`实用程序到底是如何查询(和设置)实时调度策略/优先级的？啊，这应该是显而易见的:由于它们驻留在内核**虚拟地址空间** ( **VAS** )中的任务结构内，`chrt`进程必须发出系统调用。执行这些任务的系统调用有几种变体:`chrt(1)`使用的是`sched_getattr(2)`查询，`sched_setattr(2)`系统调用是设置调度策略和优先级。(请务必查看`sched(7)`上的`man`页面，了解这些以及更多与调度程序相关的系统调用的详细信息。)快速`chrt`上的`strace(1)`确实会验证这一点！

```sh
$ strace chrt -p 1
[ ... ] 
sched_getattr(1, {size=48, sched_policy=SCHED_OTHER, sched_flags=0, 
sched_nice=0, sched_priority=0, sched_runtime=0, sched_deadline=0, 
sched_period=0}, 48, 0) = 0 
fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 6), ...}) = 0 
write(1, "pid 1's current scheduling polic"..., 47) = 47 
write(1, "pid 1's current scheduling prior"..., 39) = 39 
[ ... ] $ 
```

现在您已经掌握了查询(甚至设置)线程调度策略/优先级的实用知识，是时候深入挖掘一下了。在下一节中，我们将深入研究 Linux 的中央处理器调度器的内部工作原理。我们知道谁在运行调度程序的代码，以及它什么时候运行。好奇？继续读！

# 了解中央处理器调度内部——第 3 部分

在前面几节中，您了解到核心内核调度代码被锚定在`void schedule(void)`函数中，模块化调度器类被迭代，最终得到一个被上下文切换到的线程。这一切都很好；现在的一个关键问题是:`schedule()`代码路径到底是谁在什么时候运行的？

## 谁运行调度程序代码？

不幸的是，关于调度如何工作的一个微妙而关键的误解被许多人持有:我们想象某种被称为“调度器”的内核线程(或一些这样的实体)存在，它周期性地运行和调度任务。这完全是错误的；在像 Linux 这样的单片操作系统中，调度是由进程上下文本身来执行的，也就是运行在 CPU 上的常规线程！

事实上，调度代码总是由当前正在执行内核代码的进程上下文运行，换句话说，就是由 **`current`运行。**

这可能也是一个适当的时间来提醒你我们将称之为 Linux 内核的*黄金规则之一*:*调度代码绝不能在任何种类的原子或中断上下文中运行*。换句话说，中断上下文代码必须保证不阻塞；这就是为什么您不能在中断上下文中使用`GFP_KERNEL`标志调用`kmalloc()`，它可能会阻塞！但是有了`GFP_ATOMIC`标志，就可以指示内核内存管理代码永远不要阻塞。此外，当调度代码运行时，内核抢占被禁用；这是有道理的。

## 调度程序什么时候运行？

操作系统调度器的工作是仲裁对处理器资源的访问，在想要使用它的竞争实体(线程)之间共享它。但是，如果系统很忙，许多线程不断争夺和获取处理器，该怎么办呢？更正确地说，我们真正的意思是:为了确保任务之间公平地共享 CPU 资源，您必须确保图片中的警察，即调度程序本身，定期在处理器上运行。听起来不错，但是你到底怎么保证呢？

这里有一个(看似)合乎逻辑的方法:当定时器中断触发时调用调度程序；也就是说，它有机会每秒运行`CONFIG_HZ`次(通常设置为值 250)！等等，我们在[第 8 章](08.html)、*模块作者的内核内存分配–第 1 部分*中，在*从不在中断或原子上下文中休眠*部分学到了一个黄金法则:您不能在任何种类的原子或中断上下文中调用调度程序；因此在定时器中断代码路径内调用它肯定是不合格的。那么，操作系统是做什么的？

它的实际实现方式是定时器中断上下文和进程上下文代码路径都被用来进行调度。我们将在下一节简要描述细节。

### 定时器中断部分

在定时器中断内(在`kernel/sched/core.c:scheduler_tick()`的代码中，中断被禁用)，内核执行必要的元工作以保持调度平稳运行；这包括根据需要不断更新每个 CPU 的运行队列、负载平衡工作等等。请注意，实际的`schedule()`功能是*这里没有调用*。最多调用调度类钩子函数(对于被中断的进程上下文`current`)，`sched_class:task_tick()`，如果不为空。例如，对于属于 fair (CFS)类的任何线程来说，`vruntime`成员的更新(虚拟运行时，任务在处理器上花费的(优先级偏向的)时间)在`task_tick_fair()`中完成。

More technically, all this work described in the preceding paragraph occurs within the timer interrupt soft IRQ, `TIMER_SOFTIRQ`.

现在，一个关键点，是调度代码决定了:我们需要抢占`current`吗？在这个定时器中断代码路径中，如果内核检测到当前任务已经超过了它的时间量，或者由于任何原因必须被抢占(也许现在在运行队列上有另一个优先级比它更高的可运行线程)，代码就设置一个名为`need_resched`的“全局”标志。(我们将全局这个词放在引号中的原因是，它实际上不是内核范围的全局；它实际上只是位于名为`TIF_NEED_RESCHED`的`current`实例的`thread_info->flags`位掩码内。为什么呢？这样访问位实际上更快！)值得强调的是，在典型(可能)情况下，不需要抢占`current`，因此`thread_info.flags:TIF_NEED_RESCHED`位将保持清零。如果设置，调度程序将很快激活；但是具体是什么时候？一定要读下去...

### 流程上下文部分

一旦刚刚描述的调度内务工作的定时器中断部分完成(当然，这些事情确实完成得非常快)，控制就被交还给被粗暴中断的进程上下文(线程，`current`)。它现在将运行我们认为的中断退出路径。这里，它检查`TIF_NEED_RESCHED`位是否被置位-`need_resched()`助手例程执行该任务。如果返回`True`，这表明需要立即进行重新调度:内核调用`schedule()`！在这里，这样做很好，因为我们现在是在流程上下文中运行。(请始终记住:我们在这里讨论的所有代码都是由`current`运行的，这是有问题的流程上下文。)

当然，现在关键的问题变成了将识别`TIF_NEED_RESCHED`位是否已经被置位(通过前面描述的定时器中断部分)的代码到底在哪里？啊，这就成了问题的关键:内核安排了几个**调度机会点**出现在内核代码库中。两个调度机会点如下:

*   从系统调用代码路径返回。
*   从中断代码路径返回。

所以，想想看:每次用户空间中运行的任何线程发出系统调用时，该线程都会(上下文)切换到内核模式，现在在内核中运行代码，并拥有内核权限。当然，系统调用的长度是有限的；完成后，他们将遵循一个众所周知的返回路径，以便切换回用户模式并在那里继续执行。在该返回路径上，引入了调度机会点:检查其`thread_info`结构内的`TIF_NEED_RESCHED`位是否被设置。如果是，调度程序被激活。

仅供参考，这样做的代码是依赖于 arch 的；在 x86 上是这样的:`arch/x86/entry/common.c:exit_to_usermode_loop()`。其中，与我们相关的部分如下:

```sh
static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)
{
[...]
 if (cached_flags & _TIF_NEED_RESCHED)
 schedule();
```

类似地，在处理一个(任何)硬件中断(以及需要运行的任何相关的软 IRQ 处理程序)之后，在切换回内核中的进程上下文(内核中的一个工件–`irq_exit()`)之后，但是在将上下文恢复到被中断的任务之前，内核检查`TIF_NEED_RESCHED`位:如果它被设置，则调用`schedule()`。

让我们总结一下前面关于`TIF_NEED_RESCHED`位的设置和识别的讨论:

*   定时器中断(软 IRQ)在以下情况下设置`thread_info:flags TIF_NEED_RESCHED`位:
    *   如果调度类的`scheduler_tick()`钩子函数内的逻辑需要抢占；例如，在 CFS 上，如果当前任务的`vruntime`值超出另一个可运行线程的值一个给定的阈值(通常为 2.25 ms 相关可调是`/proc/sys/kernel/sched_min_granularity_ns`)。
    *   如果较高优先级的线程变得可运行(在同一个中央处理器上，因此运行队列；通过`try_to_wake_up()`)。
*   在进程上下文中，这是发生的情况:在中断返回和系统调用返回路径上，检查`TIF_NEED_RESCHED`的值:
    *   如果设置了(`1`)，则调用`schedule()`；否则，继续处理。

As an aside, these scheduling opportunity points – the return from a hardware interrupt or a system call – also serve as signal recognition points. If a signal is pending on `current`, it is serviced before restoring context or returning to user space.

### 可抢占内核

让我们假设一个情况:你运行在一个只有一个中央处理器的系统上。一个模拟时钟应用程序正在图形用户界面上运行，还有一个 C 程序`a.out`，它的一行代码是(呻吟)`while(1);`。那么，你认为:当 1 进程无限期占用 CPU，从而导致 GUI 时钟应用停止滴答(它的秒针会完全停止移动)时，CPU 占用者*会怎么样？*

一点点思考(和实验)就会发现，事实上，尽管有调皮的 CPU 霍格应用程序，图形用户界面时钟应用程序还是一直在滴答作响！实际上，这就是拥有操作系统级调度器的全部意义:它可以并且确实抢占占用 CPU 空间的用户空间进程。(我们之前简单讨论过 CFS 算法；CFS 将导致激进的 CPU 霍格进程累积一个巨大的`vruntime`值，从而在其 rb 树运行队列上向右移动更多，从而惩罚自己！)所有现代操作系统都支持这种类型的抢占——它被称为**用户模式抢占**。

但是现在，考虑一下这个问题:如果您编写一个内核模块，在单处理器系统上执行相同的`while(1)`无限循环，会怎么样？这可能是一个问题:系统现在将简单地挂起。操作系统将如何抢占自己(正如我们所理解的内核模块以内核特权在内核模式下运行)？好吧，你猜怎么着:多年来，Linux 提供了一个构建时配置选项来使内核可抢占，`CONFIG_PREEMPT`。(实际上，这仅仅是朝着减少延迟和改进内核和调度程序响应的长期目标发展。这项工作的很大一部分来自早期和一些正在进行的工作:低延迟的 T4 补丁、旧的 RTLinux 工作等等。我们将在下一章中详细介绍实时(RTOS) Linux - RTL。)一旦这个`CONFIG_PREEMPT`内核配置选项被打开，内核被构建并启动，我们现在运行在一个可抢占的内核上——操作系统有能力抢占自己。

To check out this option, within `make menuconfig`, navigate to General Setup | Preemption Model.

就抢占而言，基本上有三个可用的内核配置选项:

| **抢占型** | **特征** | **适用于** |
| `CONFIG_PREEMPT_NONE` | 传统模式，面向高整体吞吐量。 | 服务器/企业级和计算密集型系统 |
| `CONFIG_PREEMPT_VOLUNTARY` | 可抢占内核(桌面)；操作系统内更明确的抢占机会点；带来更低的延迟、更好的应用响应。通常是发行版的默认值。 | 工作站/台式机、为台式机运行 Linux 的笔记本电脑 |
| `CONFIG_PREEMPT` | LowLat 内核；(几乎)整个内核都是可抢占的；意味着现在甚至可以不自觉地抢占内核代码路径；以稍低的吞吐量和稍高的运行时开销为代价，产生更低的延迟(平均从几十个用户到几百个用户不等)。 | 快速多媒体系统(台式机、笔记本电脑，甚至现代嵌入式产品:智能手机、平板电脑等) |

`kernel/Kconfig.preempt` kbuild 配置文件包含可抢占内核选项的相关菜单项。(正如您将在下一章中看到的，当将 Linux 构建为 RTOS 时，出现了内核抢占的第四种选择。)

### 中央处理器调度程序入口点

核心内核调度函数`kernel/sched/core.c:__schedule()`中(就在之前)出现的详细注释非常值得通读；它们指定了内核 CPU 调度程序的所有可能入口点。我们只是在这里直接从 5.4 内核代码库中复制了它们，所以一定要看看。请记住:以下代码是由进程(实际上是线程)在进程上下文中运行的，该进程最终将上下文切换到其他线程，从而将自己从 CPU 中踢出！这条线是谁的？为什么，当然是`current`！

`__schedule()`函数有两个局部变量，指向名为`prev`和`next`的结构`task_struct`。名为`prev`的指针设置为`rq->curr`，无非就是`current`！名为`next`的指针将被设置为将被上下文切换到的任务，接下来将运行该任务！所以，你看:`current`运行调度程序代码，执行工作，然后通过上下文切换到`next`将自己踢出处理器！以下是我们提到的大评论:

```sh
// kernel/sched/core.c/*
 * __schedule() is the main scheduler function.
 * The main means of driving the scheduler and thus entering this function are:
 * 1\. Explicit blocking: mutex, semaphore, waitqueue, etc.
 *
 * 2\. TIF_NEED_RESCHED flag is checked on interrupt and user space return
 *    paths. For example, see arch/x86/entry_64.S.
 *
 *    To drive preemption between tasks, the scheduler sets the flag in timer
 *    interrupt handler scheduler_tick().
 *
 * 3\. Wakeups don't really cause entry into schedule(). They add a
 *    task to the run-queue and that's it.
 *
 *    Now, if the new task added to the run-queue preempts the current
 *    task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets
 *    called on the nearest possible occasion:
 *    - If the kernel is preemptible (CONFIG_PREEMPTION=y):
 *
 *    - in syscall or exception context, at the next outmost
 *      preempt_enable(). (this might be as soon as the wake_up()'s
 *      spin_unlock()!)
 *
 *    - in IRQ context, return from interrupt-handler to
 *      preemptible context
 *
 *    - If the kernel is not preemptible (CONFIG_PREEMPTION is not set)
 *      then at the next:
 *       - cond_resched() call
 *       - explicit schedule() call
 *       - return from syscall or exception to user-space
 *       - return from interrupt-handler to user-space
 * WARNING: must be called with preemption disabled!
 */
```

前面的代码是一个很大的注释，详细说明了如何调用内核中央处理器内核调度代码–`__schedule()`。`__schedule()`本身的相关小片段可以在下面的代码中看到，重申了我们一直在讨论的观点:

```sh
static void __sched notrace __schedule(bool preempt)
{
    struct task_struct *prev, *next;
    [...] struct rq *rq;
    int cpu;

    cpu = smp_processor_id();
    rq = cpu_rq(cpu);
    prev = rq->curr;                 *<< this is 'current' ! >>*

    [ ... ]

    next = pick_next_task(rq, prev, &rf);  *<< here we 'pick' the task to run next in an 'object-
                                          oriented' manner, as discussed earlier in detail ... >>*
    clear_tsk_need_resched(prev);
    clear_preempt_need_resched();

    if (likely(prev != next)) {
        [ ... ]
        /* Also unlocks the rq: */
        rq = context_switch(rq, prev, next, &rf);
    [ ... ]
}
```

接下来简单介绍一下实际的上下文切换。

#### 上下文切换

为了结束这一讨论，先简单介绍一下(scheduler)上下文切换。上下文切换的工作(在 CPU 调度器的上下文中)相当明显:在简单地切换到下一个任务之前，OS 必须保存前一个，也就是当前正在执行的任务的状态；换句话说，`current`的状态。您会从[第 6 章](06.html)、*内核内部本质–进程和线程*中回想起，任务结构保存一个内联结构来存储/检索线程的硬件上下文；它是成员`struct thread_struct thread`(在 x86 上，它总是任务结构的最后一个成员)。在 Linux 中，一个内联函数`kernel/sched/core.c:context_switch()`执行任务，从`prev`任务(即从`current`切换到`next`任务，这一轮调度或抢占的赢家。这种切换基本上分两个(特定于拱门的)阶段进行:

*   **内存(MM)开关**:切换一个特定于 arch 的 CPU 寄存器，指向`next`的内存描述符结构(`struct mm_struct`)。在 x86[_64]上，这个寄存器叫做`CR3` ( **控制寄存器 3**)；在 ARM 上，它被称为`TTBR0` ( **翻译表基础寄存器`0`** )寄存器。
*   **实际 CPU 切换**:保存`prev`的栈和 CPU 寄存器状态，将`next`的栈和 CPU 寄存器状态恢复到处理器上，从`prev`切换到`next`；这是在`switch_to()`宏观内完成的。

上下文切换的详细实现不是我们将在这里讨论的内容；查看*进一步阅读*部分获取更多资源。

# 摘要

在本章中，您了解了通用 Linux 内核的 CPU 调度程序的几个方面。首先，您看到了实际的 KSE 是一个线程，而不是一个进程，然后了解了操作系统实现的可用调度策略。接下来，您了解到，为了以超级可扩展的方式支持多个 CPU，内核通过每个调度类每个 CPU 内核使用一个运行队列的设计有力地反映了这一点。然后介绍了如何查询任何给定线程的调度策略和优先级，以及关于 CPU 调度器内部实现的更深入的细节。我们重点讨论了现代调度器如何利用模块化调度类设计，谁确切地运行实际的调度器代码以及何时运行，最后简要说明了上下文切换。

下一章将让您继续这一旅程，获得更多关于内核级 CPU 调度器工作原理的见解和细节。我建议你先完全消化这一章的内容，研究给出的问题，然后进入下一章。干得好！

# 问题

作为我们的总结，这里有一个问题列表，供您测试您对本章材料的知识:[https://github . com/packt publishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions)。你会在这本书的 GitHub repo 中找到一些问题的答案:[https://GitHub . com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions _ to _ assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn)。

# 进一步阅读

为了帮助您用有用的材料更深入地研究这个主题，我们在本书的 GitHub 存储库中的进一步阅读文档中提供了一个相当详细的在线参考资料和链接列表(有时甚至是书籍)。*进一步阅读*文档可在此处获得:[https://github . com/packt publishing/Linux-Kernel-Programming/blob/master/进一步阅读. md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md) 。