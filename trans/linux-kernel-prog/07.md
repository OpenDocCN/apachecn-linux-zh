# 七、内存管理内部原理——要点

内核内部，尤其是关于内存管理，是一个庞大而复杂的话题。在这本书里，我不打算深究内核内存内部的深刻、血淋淋的细节。与此同时，我想为像您这样的初露头角的内核或设备驱动程序开发人员提供足够的背景知识，以成功解决这个关键问题。

因此，本章将帮助您充分理解内存管理在 Linux 操作系统上是如何执行的；这包括深入研究**虚拟内存** ( **VM** )分割，深入研究进程的用户模式和内核部分，并涵盖内核如何管理物理内存的基础知识。实际上，您将开始理解流程和系统的内存映射(虚拟和物理的)。

这些背景知识将大大有助于您正确、高效地管理动态内核内存(重点是使用**可加载内核模块** ( **LKM** )框架编写内核或驱动程序代码；这个方面——动态内存管理——以实用的方式是本书接下来两章的重点)。一个重要的好处是，有了这些知识，你会发现自己在调试用户和内核空间代码方面变得更加熟练。(这个重要性怎么强调都不为过！调试代码既是一门艺术，也是一门科学，更是一种现实。)

在本章中，我们将涵盖以下领域:

*   了解虚拟机拆分
*   检查过程视觉模拟系统
*   检查内核段
*   随机存储布局–ASLR
*   物理内存

# 技术要求

我假设您已经完成了[第 1 章](01.html)、*内核工作区设置*，并且已经适当地准备了一个运行 Ubuntu 18.04 LTS(或更高版本的稳定版本)的来宾 VM，并且安装了所有需要的软件包。如果没有，我建议你先做这个。为了充分利用这本书，我强烈建议您首先设置工作空间环境，包括克隆这本书的 GitHub 代码存储库([https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming))，并以动手的方式进行工作。

我假设您熟悉基本的虚拟内存概念、用户模式进程**虚拟地址空间** ( **VAS** )段布局、用户和内核模式堆栈、任务结构等等。如果你在这个基础上不确定，我强烈建议你先读前一章。

# 了解虚拟机拆分

在本章中，我们将大致了解 Linux 内核如何以两种方式管理内存:

*   基于虚拟内存的方法，其中内存被虚拟化(通常的情况)
*   内核如何组织物理内存(内存页面)的视图

首先，让我们从虚拟内存视图开始，然后在本章后面讨论物理内存组织。

正如我们在前面章节中看到的，在*理解进程虚拟地址空间(VAS)* 部分，进程的一个关键属性 VAS 是它是完全独立的，一个沙盒。你不能看盒子外面。在[第 6 章](06.html)、*内核内部要素–进程和线程*，图 6.2 中，我们看到进程 VAS 的范围从虚拟地址`0`到我们简单称之为高地址。这个高位地址的实际价值是多少？显然，这是 VAS 的最高范围，因此取决于用于寻址的位数:

*   在 32 位处理器上运行的 Linux 操作系统上(或编译为 32 位)，最高虚拟地址将是 *2 <sup>32</sup> = 4 GB* 。

*   在 64 位处理器上运行(并为其编译)的 Linux 操作系统上，最高虚拟地址将是 *2 <sup>64</sup> = 16 EB。* (EB 是 exabyte 的缩写。相信我，这是一个巨大的数量。16 EB 相当于数字 *16 x 10 <sup>18</sup> 。*)

为简单起见，为了使数字易于管理，现在让我们将重点放在 32 位地址空间上(我们当然也会涉及 64 位寻址)。因此，根据我们的讨论，在 32 位系统上，进程 VAS 从 0 到 4gb–该区域包括空白空间(未使用的区域，称为**稀疏区域**或**孔**)和有效内存区域，通常称为**段**(或更准确地说，**映射**)–文本、数据、库和堆栈(所有这些都在[第 6 章](06.html)、*内核内部要素–进程和线程*中有详细介绍)

在我们理解虚拟内存的旅程中，拿起众所周知的`Hello, world` C 程序，在一个 Linux 系统上理解它的内部工作方式是很有用的；这就是下一节要讲的内容！

## 从引擎盖下看——你好，世界 C 计划

对，这里有人知道如何编写规范的`Hello, world` C 程序吗？好吧，非常有趣，让我们看看其中有意义的一行:

```sh
printf("Hello, world.\n");
```

过程是调用`printf(3)`函数。`printf()`的代码写好了吗？“不，当然不是，”你说，“它在标准的`libc` C 库中，典型的是 Linux 上的`glibc` (GNU `libc`)但是等等，除非`printf`的代码和数据(以及类似的所有其他库 API)实际上在过程 VAS 中，否则我们怎么能访问它？(回想一下，你不能在盒子外面看*！)为此，`printf(3)`(实际上是`glibc`库)的代码(和数据)必须在流程*框*内映射——流程 VAS。它确实在流程 VAS 中映射，在库段或映射中(正如我们在[第 6 章](06.html)、*内核内部要素–流程和线程*、 *F* *图 6.1* 中看到的)。这是怎么发生的？*

实际情况是，在应用启动时，作为 C 运行时环境设置的一部分，有一个小的**可执行和可链接格式** ( **ELF** )二进制文件(嵌入到您的`a.out`二进制可执行文件中)，称为**加载程序** ( `ld.so`或`ld-linux.so`)。它很早就被控制了。它检测所有需要的共享库，并通过打开库文件和发出`mmap(2)`系统调用，将所有库——库文本(代码)和数据段——映射到过程 VAS 中。因此，现在，一旦库的代码和数据被映射到流程 VAS 中，流程就可以访问它，因此，等待它`printf()` API 可以被成功调用！(这里我们跳过了内存映射和链接的血淋淋的细节)。

进一步验证这一点，`ldd(1)`脚本(以下输出来自 x86_64 系统)揭示了事实确实如此:

```sh
$ gcc helloworld.c -o helloworld
$ ./helloworld
Hello, world
$ ldd ./helloworld
 linux-vdso.so.1 (0x00007fffcfce3000)
 libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007feb7b85b000)
 /lib64/ld-linux-x86-64.so.2 (0x00007feb7be4e000)
$
```

需要注意的几点:

*   每个单独的 Linux 进程——自动地和默认地——链接到至少两个对象:共享库和程序加载器(不需要显式的链接器开关)。
*   加载程序的名称因体系结构而异。这里，在我们的 x86_64 系统上，是`ld-linux-x86-64.so.2`。
*   在前面的`ldd`输出中，右边括号内的地址是映射位置的虚拟地址。例如，在前面的输出中，`glibc`在**用户虚拟地址** ( **UVA** )处映射到我们的流程 VAS 中，该地址等于`0x00007feb7b85b000`。请注意，它依赖于运行时(这也因**地址空间布局随机化** ( **ASLR** )语义(见下文)而异)。
*   出于安全原因(在 x86 之外的架构上)，最好使用`objdump(1)`实用程序来查找这些细节。

Try performing `strace(1)` on the `Hello, world` binary executable and you will see numerous `mmap()` system calls, mapping in `glibc` (and other) segments!

让我们进一步深入考察我们简单的`Hello, world` 的应用。

### 超越 printf() API

如您所知，`printf(3)`应用编程接口转换为`write(2)`系统调用，这当然会将`"Hello, world"`字符串写入`stdout`(默认情况下，终端窗口或控制台设备)。

我们也理解，由于`write(2)`是系统调用，这意味着当前运行该代码的进程——进程上下文——现在必须切换到内核模式，运行`write(2)`(单片内核架构)的内核代码！的确如此。但是等一下:`write(2)`的内核代码在内核 VAS 中(参考[第 6 章](06.html)，*内核内部要素–进程和线程*，图 6.1)。这里的重点是，如果内核 VAS 在盒子外面，那么我们到底该怎么称呼它呢？

嗯，可以通过将内核放在一个单独的 4 GB VAS 中来完成，但是这种方法会导致非常慢的上下文切换，所以这根本做不到。

它的设计方式是这样的:用户和内核花瓶都生活在同一个“盒子”里——可用的 VAS。具体怎么做？通过*将*用户和内核之间的可用地址空间以某种`User:Kernel :: u:k`比例分割。这被称为**虚拟机分割**(比率`u:k`通常以千兆字节、万亿字节甚至千兆字节表示)。

下图是一个 32 位 Linux 进程的代表，该进程具有 *2:2* 虚拟机分割(以千兆字节为单位)；也就是说，总的 4 GB 进程 VAS 被分成 2 GB 的用户空间和 2 GB 的内核空间。这通常是运行 Linux 操作系统的 ARM-32 系统上典型的虚拟机拆分:

![](Images/4be6ef77-78d0-45f6-aafc-90ce33d29cc0.png)

Figure 7.1 – User:Kernel :: 2:2 GB VM split on an ARM-32 system running Linux

因此，现在内核 VAS 在盒子里，理解这一点突然变得清晰和关键:当用户模式进程或线程发出系统调用时，在同一个进程的 VAS 中有一个到内核 2 GB VAS(各种 CPU 寄存器，包括堆栈指针，得到更新)的上下文切换。发出系统调用的线程现在以特权内核模式在进程上下文中运行其内核代码(并处理内核空间数据)。完成后，它从系统调用返回，上下文切换回非特权用户模式，并且现在在前 2 GB VAS 中运行用户模式代码。

内核 VAS(也称为**内核段***—*开始的确切虚拟地址通常通过内核中的`PAGE_OFFSET`宏来表示。我们将在*宏和描述内核段布局的变量*一节中研究这个以及其他一些关键宏。

关于虚拟机拆分的准确位置和大小，这一决定是在哪里做出的？啊，在 32 位 Linux 上，它是一个内核构建时可配置的。这是作为`make [ARCH=xxx] menuconfig`过程的一部分在内核构建中完成的——例如，为博通 BCM2835(或 BCM2837) **片上系统** ( **SoC** )(树莓皮是这种 SoC 的流行板)配置内核时。下面是官方内核配置文件的一个片段(输出来自树莓 Pi 控制台):

```sh
$ uname -r
5.4.51-v7+
$ sudo modprobe configs      *<< gain access to /proc/config.gz via this LKM >>* $ zcat /proc/config.gz | grep -C3 VMSPLIT
[...]
# CONFIG_BIG_LITTLE is not set
# CONFIG_VMSPLIT_3G is not set
# CONFIG_VMSPLIT_3G_OPT is not set
CONFIG_VMSPLIT_2G=y
# CONFIG_VMSPLIT_1G is not set
CONFIG_PAGE_OFFSET=0x80000000
CONFIG_NR_CPUS=4
[...]
```

正如在前面的片段中看到的那样，`CONFIG_VMSPLIT_2G`内核配置选项被设置为`y`，这意味着默认的虚拟机分割是`user:kernel :: 2:2`。对于 32 位架构，虚拟机拆分位置是**可调的**(如前面的片段`CONFIG_VMSPLIT_[1|2|3]G`所示；`CONFIG_PAGE_OFFSET`对号入座。通过 2:2 虚拟机分割，`PAGE_OFFSET`实际上是在虚拟地址`0x8000 0000` (2 GB)的一半！

IA-32 处理器(英特尔 x86-32)的默认虚拟机分割为 3:1 (GB)。有趣的是，运行在 IA-32 上的(古老的)Windows 3.x 操作系统也有相同的虚拟机分割，这表明这些概念本质上与操作系统无关。在本章的后面，除了其他细节之外，我们将介绍更多的体系结构及其虚拟机拆分。

64 位体系结构无法直接配置虚拟机剥离。现在，我们已经了解了 32 位系统上的虚拟机剥离，接下来让我们来看看它是如何在 64 位系统上完成的。

## 64 位 Linux 系统上的虚拟机拆分

首先，值得注意的是，在 64 位系统中，并非所有 64 位都用于寻址。在 x86_64 的标准或典型 Linux 操作系统配置中，页面大小为(典型的)4 KB，我们使用(T0)最低有效位 ( **最低有效位**)48 位进行寻址。为什么不是全部 64 位？简直太过分了！没有一台现有的计算机接近拥有完整的 *2* *<sup>64</sup>* *的一半= 18，446，744，073，709，551，616* 字节，相当于 16 EB(即 16，384 petabytes)的内存！

"Why," you might well wonder, "do we equate this with RAM?". Please read on – more material needs to be covered before this becomes clear. The *Examining the kernel segment* section is where you will understand this fully.

### 虚拟寻址和地址转换

在深入这些细节之前，清楚地了解几个关键点非常重要。

考虑一个来自 C 程序的小而典型的代码片段:

```sh
int i = 5;
printf("address of i is 0x%x\n", &i);
```

您看到的`printf()`发出的地址是虚拟地址，而不是物理地址。我们区分两种虚拟地址:

*   如果你在用户空间进程中运行这段代码，你将看到的变量`i`的地址是一个 UVA。
*   如果你在内核或者内核模块中运行这段代码(当然，你会使用`printk()`应用编程接口)，你会看到变量`i`的地址是**内核虚拟地址** ( **KVA** )。

接下来，虚拟地址不是绝对值(从`0`开始的偏移量)；这实际上是一个*位掩码*:

*   在 32 位 Linux 操作系统上，32 个可用位被分为所谓的**页面全局目录**(**【PGD】**)值、**页面表** ( **PT** )值和偏移量。
*   这些成为索引，通过这些索引 **MMU** (现代微处理器芯片中的**内存管理单元**)可以访问当前进程上下文的内核页表，执行地址转换。

We do not intend on covering the deep details on MMU-level address translation here. It's also very arch-specific. Do refer to the *Further reading* section for useful links on this topic.

*   正如所料，在 64 位系统上，即使使用 48 位寻址，虚拟地址位掩码中也会有更多的字段。

好吧，如果这种 48 位寻址是 x86_64 处理器上的典型情况，那么 64 位虚拟地址中的位是如何布局的？未使用的 16 个 MSB 位会怎么样？下图回答了问题；它代表了 x86_64 Linux 系统上虚拟地址的分解:

![](Images/0ecf9dc8-57ed-4ca1-8341-169570f72e9b.png)

Figure 7.2 – Breakup of a 64-bit virtual address on the Intel x86_64 processor with 4 KB pages

本质上，对于 48 位寻址，我们使用位 0 到 47(LSB 48 位)并忽略**最高有效位** ( **MSB** ) 16 位，将其视为符号扩展。虽然没有那么快；未使用的符号扩展 MSB 16 位的值随您所在的地址空间而变化:

*   **内核 VAS** : MSB 16 位始终设置为`1`。
*   **用户 VAS** : MSB 16 位始终设置为`0`。

这是有用的信息！了解了这一点，只需查看一个(完整的 64 位)虚拟地址，您就可以分辨出它是 KVA 还是 UVA:

*   64 位 Linux 系统上的 KVAs 始终遵循`0xffff .... .... ....`格式。
*   UVA 总是有格式`0x0000 .... .... ....`。

**A word of caution**: the preceding format holds true only for processors (MMUs, really) that self-define virtual addresses as being KVAs or UVAs; the x86 and ARM family of processors do fall in this bracket.

正如现在可以看到的(我在这里重申)，现实是虚拟地址不是绝对地址(从零开始的绝对偏移量，正如您可能错误地想象的那样)，而是实际上是位掩码。事实上，内存管理是一个复杂的工作共享区域:****OS 负责创建和操作每个进程的分页表，toolchain(编译器)生成虚拟地址，而实际执行运行时地址转换的是处理器 MMU，将给定的(用户或内核)虚拟地址转换为物理(RAM)地址！****

 **关于硬件分页(以及各种硬件加速技术，如**翻译后备缓冲区** ( **TLB** )和 CPU 缓存)的更多细节，我们在本书中不再赘述。本章*进一步阅读*一节中提到的各种其他优秀书籍和参考网站很好地涵盖了这个特定的主题。

回到 64 位处理器上的视觉模拟系统。64 位系统上可用的 VAS 只是一个巨大的*2**<sup>64</sup>**= 16 EB*(*16 x 10**<sup>18</sup>*字节！).故事是这样的，当 AMD 工程师第一次将 Linux 内核移植到 x86_64(或 AMD64) 64 位处理器时，他们必须决定如何在这个巨大的 VAS 中布局进程和内核部分。即使在今天的 x86_64 Linux 操作系统上，达成的决定也或多或少保持不变。这个巨大的 64 位 VAS 被拆分如下。这里，我们假设 48 位寻址，4 KB 页面大小:

*   规范下半部分，对于 128 TB:用户 VAS 和虚拟地址范围从`0x0`到`0x0000 7fff ffff ffff`
*   规范上半部分，对于 128 TB:内核 VAS 和虚拟地址范围从`0xffff 8000 0000 0000`到`0xffff ffff ffff ffff`

The word *canonical* effectively means *as per the law* or as *per common convention*.

x86_64 平台上的 64 位虚拟机分割如下图所示:

![](Images/8ab482cf-6585-4217-9944-d01884f4a8af.png)

Figure 7.3 – The Intel x86_64 (or AMD64) 16 EB VAS layout (48-bit addressing); VM split is User : Kernel :: 128 TB : 128 TB

在上图中，中间未使用的区域——空洞或稀疏区域——也称为**非规范地址**区域。有趣的是，使用 48 位寻址方案时，绝大多数增值服务未被使用。这就是为什么我们称增值服务为非常稀疏。

The preceding figure is certainly not drawn to scale! Always keep in mind that this is all *virtual* memory space, not physical.

为了结束我们对虚拟机拆分的讨论，下图显示了不同 CPU 架构的一些常见`user:kernel`虚拟机拆分比率(我们假设 MMU 页面大小为 4 KB):

![](Images/43afce1c-3e72-4305-b2c1-f9763f166a51.png)

Figure 7.4 – Common user:kernel VM split ratios for different CPU architectures (for 4 KB page size)

我们用红色粗体突出显示第三行，因为它被认为是常见的情况:在 x86_64(或 AMD64)架构上运行 Linux，带有`user:kernel :: 128 TB:128 TB`虚拟机拆分。此外，阅读表格时要小心:第六列和第八列的数字 End vaddr 都是单个 64 位数量，而不是两个数字。这个数字可能只是简单地。例如，在 x86_64 行第 6 列，是*单个*号`0x0000 7fff ffff ffff`而不是两个号。

第三列，Addr Bits，向我们展示了在 64 位处理器上，没有一个真实世界的处理器实际上使用所有 64 位进行寻址。

在 x86_64 下，上表中显示了两个虚拟机拆分:

*   第一个 128 TB : 128 TB (4 级分页)是目前 Linux x86_64 位系统(嵌入式笔记本电脑、个人电脑、工作站和服务器)上使用的典型虚拟机拆分。它将物理地址空间限制为 64 TB(内存)。
*   第二个，64 PB : 64 PB，至少在撰写本文时，还是纯理论的；它支持 4.14 Linux 中所谓的 5 级分页；分配的花瓶(56 位寻址；总共 128 PB 的 VAS 和 4 PB 的物理地址空间！)是如此巨大，以至于在撰写本文时，还没有真正的计算机在使用它。

请注意，运行在 Linux 上的 AArch64 (ARM-64)架构的两行仅仅是代表性的。从事该产品的 BSP 供应商或平台团队可以使用不同的拆分。有趣的是，(旧的)Windows 32 位操作系统上的虚拟机分割为 2:2 (GB)。

内核增值服务中实际存在什么，或者通常所说的内核部分？所有内核代码、数据结构(包括任务结构、列表、内核模式堆栈、分页表等)、设备驱动程序、内核模块等都在这里(如[第 6 章](06.html)、*内核内部要素-进程和线程*图 6.7 下半部分所示；我们在*理解* *内核段*一节中详细介绍了这一点。

It's important to realize that, as a performance optimization on Linux, kernel memory is always non-swappable; that is, kernel memory can never be paged out to a swap partition. User space memory pages are always candidates for paging, unless locked (see the `mlock[all](2)` system calls).

有了这个背景，您现在可以了解完整的过程 VAS 布局。继续读。

## 过程视觉系统——完整视图

再次参考*图 7.1*；它显示了单个 32 位进程的实际进程 VAS 布局。当然，现实是(这也是关键)系统上所有活跃的**进程都有自己独特的用户模式 VAS，但共享相同的内核部分** *。*图 7.1 显示了 2:2 (GB)虚拟机拆分，为了与图 7.1*进行对比，下图显示了典型 IA-32 系统的实际情况，其中虚拟机拆分为 3:1 (GB):*

![](Images/846fede0-f2ab-4232-affc-6fe17540dcf5.png)

Figure 7.5 – Processes have a unique user VAS but share the kernel segment (32-bit OS); IA-32 with a 3:1 VM split

请注意上图中地址空间是如何反映 3:1 (GB)虚拟机拆分的。用户地址空间从`0`延伸至`0xbfff ffff` ( `0xc000 0000`为 3 GB 标记；这是`PAGE_OFFSET`宏设置的)，内核 VAS 从`0xc000 0000` (3 GB)扩展到`0xffff ffff` (4 GB)。

在本章的后面，我们将介绍名为`procmap`的实用工具的用法。它将帮助您详细地可视化花瓶，包括内核花瓶和用户花瓶，类似于我们前面的图。

需要注意的几点:

*   对于图 7.5 所示的例子，`PAGE_OFFSET`的值为`0xc000 0000`。
*   我们在这里显示的数字和数字并不是绝对的，并且对所有架构都有约束力；它们往往非常特定于 arch，许多供应商高度定制的 Linux 系统可能会改变它们。
*   *图 7.5* 详细介绍了 32 位 Linux 操作系统上的虚拟机布局。在 64 位 Linux 上，*概念*保持不变，只是数字(显著)改变了。如前几节中的一些细节所示，x86_64(具有 48 位寻址)Linux 系统上的虚拟机分割成为`User : Kernel :: 128 TB : 128 TB`。

现在已经了解了进程的虚拟内存布局的基本原理，您会发现它在难以调试的情况下对解密和取得进展有很大的帮助。像往常一样，还有更多；接下来的部分将介绍用户空间和内核空间内存映射(内核段)，以及物理内存映射的一些内容。继续读！

# 检查过程视觉模拟系统

我们已经介绍了每个进程的虚拟地址空间由哪些部分或映射组成的布局(参见第 6 章*、* *内核内部要素-进程和线程*中的*了解* *进程* *虚拟地址空间(VAS)* 部分)。我们了解到流程 VAS 由各种映射或段组成，其中包括文本(代码)、数据段、库映射和至少一个堆栈。在这里，我们对这一讨论进行了很大的扩展。

对于像您这样的开发人员以及用户、QA、sysadmin、DevOps 等来说，能够深入内核并看到各种运行时值是一项重要的技能。Linux 内核为我们提供了一个惊人的接口来实现这一点——你猜对了，它就是`proc`文件系统(`procfs`)。

这在 Linux 上一直存在(至少应该存在)，并且安装在`/proc` *下。*`procfs`*系统有两个主要工作:*

 **   提供一组统一的(伪或虚拟)文件和目录，使您能够深入查看内核和硬件内部细节。
*   提供一组统一的根可写文件，允许 sysad 修改关键内核参数。它们出现在`/proc/sys/`下，被称为`sysctl`——它们是 Linux 内核的调节旋钮。

熟悉`proc`文件系统确实是必须的。我劝你去看看，也看看`proc(5)`上的优秀手册。例如，简单地执行`cat /proc/PID/status`(其中`PID`当然是给定进程或线程的唯一进程标识符)会从进程或线程的任务结构中产生一大堆有用的细节！

Conceptually similar to `procfs` is the `sysfs` filesystem, mounted under `/sys` (and under it `debugfs`*,* typicallymounted at `/sys/kernel/debug`). `sysfs` is a representation of 2.6 Linux's new device and driver model; it exposes a tree of all devices on the system, as well as several kernel-tuning knobs.

## 详细检查用户视觉模拟系统

让我们从检查任何给定过程的用户视觉模拟开始。通过`procfs`，特别是通过`/proc/PID/maps`伪文件，可以获得用户视觉模拟系统的相当详细的地图。让我们学习如何使用这个接口来查看进程的用户空间内存映射。我们将看到两种方式:

*   直接通过`procfs`界面的`/proc/PID/maps`伪文件
*   使用一些有用的前端(使输出更易于人类理解)

让我们从第一个开始。

### 使用 procfs 直接查看进程内存图

查找任意进程的内部进程细节确实需要根访问，而查找您所拥有的进程的细节(包括调用方进程本身)则不需要。因此，作为一个简单的例子，我们将通过使用`self`关键字代替 PID 来查找调用过程的 VAS。以下截图显示了这一点(在 x86_64 Ubuntu 18.04 LTS 客户机上):

![](Images/8e968745-d0fe-4cd8-9970-fb250150f58d.png)

Figure 7.6 – Output of the cat /proc/self/maps command

在前面的截图中，您实际上可以看到`cat`流程的用户 VAS——该流程的用户 VAS 的名副其实的内存图！另外，请注意前面的`procfs`输出是按照(用户)虚拟地址(UVA)升序排序的。

Familiarity with using the powerful `mmap(2)` system call will help greatly in understanding further discussions. Do (at least) browse through its man page.

#### 解释/proc/PID/映射输出

要解释图 7.6 的输出，一次读一行。**每条线代表所讨论过程的用户模式 VAS** 的一个片段或映射(在前面的例子中，它属于`cat`过程)。每行由以下字段组成。

为了更简单，我将只显示一行输出，我们将在下面的注释中标记和引用其字段:

```sh
 start_uva  -  end_uva   mode,mapping  start-off   mj:mn inode# image-name 
555d83b65000-555d83b6d000    r-xp      00000000    08:01 524313   /bin/cat
```

这里，整条线代表一个片段，或者更准确地说，代表过程(用户)视觉模拟系统中的一个*映射*。`uva`是用户虚拟地址。每个段的`start_uva`和`end_uva`显示为前两个字段(或列)。因此，映射(段)的长度很容易计算(`end_uva`–`start_uva`字节)。因此，在前一行中，`start_uva`为`0x555d83b65000`，`end_uva`为`0x555d83b6d000`(长度可计算为 32kb)；但是，这是什么片段呢？一定要读下去...

第三个字段`r-xp`实际上是两条信息的组合:

*   前三个字母代表该段的模式(权限)(以通常的`rwx`符号表示)。
*   下一个字母表示映射是私有的(`p`)还是共享的(`s`)。在内部，这是由第四个参数设置的`mmap(2)`系统调用，`flags`；这实际上是**`mmap(2)`**系统调用，它在内部负责创建流程中的每个片段或映射！****
***   因此，对于前面显示的样本段，第三个字段是值`r-xp`，我们现在可以知道它是一个文本(代码)段，并且是一个私有映射(如预期的那样)。**

 **第四个字段`start-off`(这里是值`0`)是从内容已经映射到过程 VAS 的文件开始的开始偏移量。显然，该值仅对文件映射有效。通过浏览倒数第二个(第六个)字段，您可以判断当前段是否是文件映射。对于不是文件映射的映射——称为**匿名映射**——它总是`0`(例如代表堆或栈段的映射)。在我们前面的示例行中，它是一个文件映射(即`/bin/cat`的映射)，与该文件开头的偏移量是`0`字节(正如我们在前面的段落中计算的，映射的长度是 32 KB)。

第五个字段(`08:01`)的形式为`mj:mn`，其中`mj`是主要编号，`mn`是图像所在设备文件的次要编号。类似于第四个字段，它只对文件映射有效，否则它简单地显示为`00:00`；在我们前面的示例行中，它是一个文件映射(即`/bin/cat`的映射)，并且(文件所在的*设备*的主编号和次编号)分别是`8`和`1`。

第六个字段(`524313`)表示图像文件的索引节点号——其内容被映射到进程 VAS 中的文件。索引节点是 **VFS(虚拟文件系统)**的关键数据结构；它保存文件对象的所有元数据，除了它的名称(在目录文件中)之外的所有内容。同样，该值仅对文件映射有效，否则仅显示为`0`。事实上，这是一种判断映射是文件映射还是匿名映射的快速方法！在我们前面的示例映射中，很明显这是一个文件映射(T2 的映射)，索引节点号是`524313`。的确，我们可以证实这一点:

```sh
ls -i /bin/cat
524313 /bin/cat
```

第七个也是最后一个字段表示文件的路径名，该文件的内容正被映射到用户视觉模拟系统中。这里，当我们查看`cat(1)`进程的内存映射时，路径名(对于文件映射的段)当然是`/bin/cat`。如果映射代表一个文件，文件的索引节点号(第六个字段)显示为正数；如果不是，这意味着它是没有后备存储的纯内存或匿名映射，索引节点号显示为`0`，该字段将为空。

现在应该很明显了，但我们还是要指出这一点——这是一个关键点:前面看到的所有地址都是虚拟的，而不是物理的。此外，它们只属于用户空间，因此被称为 UVAs，并且总是通过该进程的唯一分页表来访问(和转换)。另外，前面的截图是在 64 位(x86_64) Linux 客户机上拍摄的。因此，在这里，我们看到 64 位虚拟地址。

Though the way the virtual addresses are displayed isn't as a full 64-bit number – for example, as `0x555d83b65000` and not as `0x0000555d83b65000` – I want you to notice how, because it's a **user virtual address** (a **UVA**), the MSB 16 bits are zero!

没错，这涵盖了如何解释一个特定的片段或映射，但是似乎有一些奇怪的片段或映射——如`vvar`、`vdso`和`vsyscall`映射。让我们看看他们是什么意思。

#### vsyscall 页面

在图 7.6 的输出中，你注意到了一些不寻常的东西吗？那里的最后一行——所谓的`vsyscall`条目——映射了一个内核页面(现在，你知道我们如何分辨:它的开始和结束虚拟地址的 MSB 16 位被设置)。在这里，我们只提到一个事实，这是一个(旧的)执行系统调用的优化。它的工作原理是，减少了实际切换到内核模式的需求，只需要一小群不需要的系统调用。

目前，在 x86 上，这些包括`gettimeofday(2)`、`time(2)`和`getcpu(2)`系统调用。事实上，上面的`vvar`和`vdso`(又名 vDSO)映射是同一主题的(略微)现代变体。如果您有兴趣了解更多信息，请访问本章的*进一步阅读*部分。

因此，您现在已经看到了如何通过直接读取和解释带有 PID 的进程的`/proc/PID/maps`(伪)文件的输出来检查任何给定进程的用户空间内存映射。还有其他方便的前端可以做到这一点；我们现在来看几个。

### 用于查看进程内存映射的前端

除了通过`/proc/PID/maps`的原始或直接格式(我们在上一节中看到了如何解释)，还有一些包装实用程序可以帮助我们更容易地解释用户模式的 VAS。其中包括附加(原始)`/proc/PID/smaps` 伪文件、`pmap(1)`和`smem(8)`实用程序，以及我自己的简单实用程序(命名为`procmap`)。

内核通过`proc`下的`/proc/PID/smaps` 伪文件提供每个片段或映射的详细信息。一定要试着`cat /proc/self/smaps`亲自看看这个。您会注意到，对于每个片段(映射)，都提供了大量的详细信息。`proc(5)`上的手册页有助于解释看到的许多领域。

对于`pmap(1)`和`smem(8)`实用程序，详情请参考它们的手册页。例如，对于`pmap(1)`，手册页告知我们更详细的`-X`和`-XX`选项:

```sh
-X Show even more details than the -x option. WARNING: format changes according to /proc/PID/smaps
-XX Show everything the kernel provides
```

关于`smem(8)`效用，事实是它并没有*而不是*给你展示过程 VAS 相反，这更像是回答一个常见问题:即确定哪个进程占用了最多的物理内存。它使用**常驻集大小** ( **RSS** )、 **P** **比例集大小** ( **PSS** )和 **U** **nique 集大小** ( **USS** )等指标来呈现更清晰的画面。亲爱的读者，我将把对这些实用程序的进一步探索留给你来练习！

现在，让我们继续探索如何使用一个有用的工具——T0——来详细查看任何给定进程的内核和用户内存映射。

#### procmap 过程 VAS 可视化实用程序

作为一个小的学习和教学(和调试期间有帮助！)项目，我在 GitHub 上创作并主持了一个名为`procmap`*的小项目，这里提供:[https://github.com/kaiwan/procmap](https://github.com/kaiwan/procmap)(做`git clone`吧)。其`README.md`文件的一个片段有助于解释其目的:*

```sh
procmap is designed to be a console/CLI utility to visualize the complete memory map of a Linux process, in effect, to visualize the memory mappings of both the kernel and user mode Virtual Address Space (VAS). It outputs a simple visualization, in a vertically-tiled format ordered by descending virtual address, of the complete memory map of a given process (see screenshots below). The script has the intelligence to show kernel and user space mappings as well as calculate and show the sparse memory regions that will be present. Also, each segment or mapping is scaled by relative size (and color-coded for readability). On 64-bit systems, it also shows the so-called non-canonical sparse region or 'hole' (typically close to 16,384 PB on the x86_64).
```

旁白:在撰写本材料时(2020 年 4 月/5 月)，新冠肺炎疫情正在全球大部分地区全面展开。与早期的 *SETI@home* 项目([https://setiathome.berkeley.edu/](https://setiathome.berkeley.edu/))类似， *Folding@home* 项目([https://foldingathome.org/category/covid-19/](https://foldingathome.org/category/covid-19/))是一个分布式计算项目，利用互联网连接的家庭(或任何)计算机来帮助模拟和解决与新冠肺炎治疗相关的问题(其中包括寻找影响我们的其他几种严重疾病的治疗方法)。你可以从[https://foldingathome.org/start-folding/](https://foldingathome.org/start-folding/)下载软件(安装它，它会在你的系统空闲周期运行)。我就是这么做的；这是 FAH 查看器(一个显示蛋白质分子的漂亮 GUI！)在我的(本机)Ubuntu Linux 系统上运行的进程:

```sh
$ ps -e|grep -i FAH
6190 ? 00:00:13 FAHViewer
```

好的，让我们使用`procmap`实用程序来询问它的视觉模拟系统。我们如何调用它？简单，看下面(由于篇幅不够，我就不在这里展示所有的信息、注意事项等；一定要亲自尝试一下):

```sh
$ git clone https://github.com/kaiwan/procmap
$ cd procmap
$ ./procmap
Options:
 --only-user : show ONLY the user mode mappings or segments
 --only-kernel : show ONLY the kernel-space mappings or segments
 [default is to show BOTH]
 --export-maps=filename
     write all map information gleaned to the file you specify in CSV
 --export-kernel=filename
     write kernel information gleaned to the file you specify in CSV
 --verbose : verbose mode (try it! see below for details)
 --debug : run in debug mode
 --version|--ver : display version info.
See the config file as well.
[...]
```

请注意，该`procmap`实用程序与 BSD Unix 提供的`procmap`实用程序不同。此外，这取决于`bc(1)`和`smem(8)`公用事业；请确保它们已安装。

当我只用`--pid=<PID>`运行`procmap`实用程序时，它会显示给定进程的内核和用户空间花瓶。现在，由于我们还没有讨论关于内核 VAS(或段)的细节，我不会在这里显示内核空间的详细输出；让我们推迟到下一节*检查内核部分*。随着我们的继续，您将发现仅来自`procmap`实用程序的用户视觉模拟输出的部分截图。完整的输出可能相当长，当然，这取决于所讨论的过程；一定要亲自尝试一下。

正如您将看到的，它试图提供完整的进程内存映射的基本可视化——内核和用户空间 VAS 都采用垂直平铺格式(如上所述，这里我们只显示截断的截图):

![](Images/7e3f5480-bd1d-40f5-bca5-e1d005e5fd31.png)

Figure 7.7 – Partial screenshot: the first line of the kernel VAS output from the procmap utility

请注意，从前面的(部分)截图中，可以看到几件事:

*   `procmap` (Bash)脚本自动检测到我们正在 x86_64 64 位系统上运行。
*   虽然我们现在没有关注它，但是内核 VAS 的输出首先出现；这很自然，因为我们显示了按虚拟地址降序排列的输出(图 7.1、7.3 和 7.5 重申了这一点)
*   你可以看到第一行(在`KERNEL VAS`头之后)对应于 VAS 最顶端的一个 KVA–值`0xffff ffff ffff ffff`(因为我们在 64 位上)。

转到`procmap`输出的下一部分，让我们来看一下`FAHViewer`过程的用户 VAS 上端的截断视图:

![](Images/3863175c-0c03-4012-84be-26da6809fc07.png)

Figure 7.8 – Partial screenshot: first few lines (high end) of the user VAS output from the procmap utility

图 7.8 是`procmap` 输出的部分截图，展示了用户空间 VAS 在它的最顶端，你可以看到(高端)UVA。

在我们的 x86_64 系统上(回想一下，这是依赖于 arch 的)，这个(高)`end_uva`值是
`0x0000 7fff ffff ffff`，`start_uva`当然是`0x0`。`procmap`如何计算出精确的地址值？啊，它相当复杂:对于内核空间内存信息，它使用内核模块(一个 LKM！)查询内核并根据系统架构建立配置文件；用户空间的细节，当然来自于`/proc/PID/maps`直接`procfs`的伪文件。

As an aside, the kernel component of `procmap`, a kernel module, sets up a way to interface with user space – the `procmap` scripts – by creating and setting up a `debugfs` (pseudo) file.

下面的截图显示了该过程的用户模式 VAS 低端的部分截图，一直到最低的 UVA，`0x0`:

![](Images/9f77ade3-9866-4c1b-8302-27d0d221f168.png)

Figure 7.9 – Partial screenshot: last few lines (low end) of the user VAS output from the procmap utility

最后一个映射，单个页面，正如所料，是空陷阱页面(从 UVA `0x1000`到`0x0`；我们将在即将到来的*空陷阱页面*部分解释其目的。

`procmap`实用程序，如果在其配置文件中启用，将计算并显示一些统计数据；这包括内核和用户模式花瓶的大小，稀疏区域占用的用户空间内存量(在 64 位上，就像前面的例子一样，通常是绝大部分空间！)作为绝对数字和百分比，报告的物理内存量，最后是由`ps(1)`和`smem(8)`实用程序报告的该特定进程的内存使用详细信息。

一般来说，你会发现在 64 位系统上(见图 7.3)，进程 VAS 的*稀疏*(空)内存区域占用了接近 100%的可用地址空间！(通常是 127.99 这样的数字[...]在现有的 128 TB 中，增值服务为 1 TB。)这意味着 99.99[...]%的内存空间是稀疏的(空的)！这就是 64 位系统上巨大的视觉模拟系统的现实。巨大的 128 TB VAS 中只有一小部分(x86_64 上就是这种情况)实际在使用。当然，稀疏和使用的 VAS 的实际数量取决于特定应用进程的大小。

当在更深层次上调试或分析问题时，能够清晰地可视化过程视觉模拟会有很大帮助。

If you're reading this book in its hardcopy format, be sure to download the full-color PDF of diagrams/figures from the publisher's website: [https://static.packt-cdn.com/downloads/9781789953435_ColorImages.pdf](_ColorImages.pdf).

您还将看到输出末尾打印出的统计数据(如果启用)显示了为目标进程设置的**虚拟内存区域** ( **VMAs** )的数量。以下部分简要解释了什么是 VMA。我们开始吧！

## 了解 VMA 基础知识

在`/proc/PID/maps`的输出中，输出的每一行实际上都是从一个叫做 VMA 的内核元数据结构中推断出来的。这其实很简单:内核使用 VMA 数据结构来抽象我们一直称之为段或映射的东西。因此，对于用户视觉模拟系统中的每一个片段，都有一个由操作系统维护的 VMA 对象。请认识到，只有用户空间段或映射由称为 VMA 的内核元数据结构控制；内核段本身没有 VMAs。

那么，给定的流程将有多少个 VMA？嗯，它等于它的用户 VAS 中映射(段)的数量。在我们使用 *FAHViewer* 进程的示例中，它碰巧有 206 个段或映射，这意味着内核内存中有 206 个 VMA 元数据对象——代表该进程的 206 个用户空间段或映射。

从程序上讲，内核通过以`current->mm->mmap`为根的任务结构来维护一个 VMA“链”(出于效率原因，它实际上是一个红黑树数据结构)。指针为什么叫`mmap`？这是经过深思熟虑的:每次执行`mmap(2)`系统调用(即内存映射操作)时，内核都会在调用进程(即`current`实例)VAS 中生成一个映射(或“段”)，并生成一个代表它的 VMA 对象。

VMA 元数据结构类似于包含映射的保护伞，包括内核执行各种内存管理所需的所有信息:服务页面错误(非常常见)、在输入/输出期间将文件内容缓存到内核页面缓存中(或从内核页面缓存中取出)等等。

Page fault handling is a very important OS activity, whose algorithm makes up quite a bit of usage of the kernel VMA objects; in this book, though, we don't delve into these details as it's largely transparent to kernel module/driver authors.

为了让您感受一下，我们将在下面的代码片段中展示内核 VMA 数据结构的一些成员；旁边的评论有助于解释他们的目的:

```sh
// include/linux/mm_types.h
struct vm_area_struct {
    /* The first cache line has the info for VMA tree walking. */
    unsigned long vm_start;     /* Our start address within vm_mm. */
    unsigned long vm_end;       /* The first byte after our end address
    within vm_mm. */

    /* linked list of VM areas per task, sorted by address */
    struct vm_area_struct *vm_next, *vm_prev;
    struct rb_node vm_rb;
    [...]
    struct mm_struct *vm_mm;     /* The address space we belong to. */
    pgprot_t vm_page_prot;       /* Access permissions of this VMA. */
    unsigned long vm_flags;      /* Flags, see mm.h. */
    [...]
    /* Function pointers to deal with this struct. */
    const struct vm_operations_struct *vm_ops;
    /* Information about our backing store: */
    unsigned long vm_pgoff;/* Offset (within vm_file) in PAGE_SIZE units */
    struct file * vm_file;       /* File we map to (can be NULL). */
    [...]
} __randomize_layout
```

现在应该更清楚`cat /proc/PID/maps`到底是如何在引擎盖下工作的:当用户空间确实，比如说`cat /proc/self/maps`时，`read(2)`系统调用由`cat`发出；这导致`cat`进程切换到内核模式，并在内核中以内核特权运行`read(2)`系统调用代码。这里，内核**虚拟文件系统开关** ( **VFS** )将控制重定向到适当的`procfs`回调处理程序(函数)。这段代码迭代(循环)每个 VMA 元数据结构(对于`current`，这当然是我们的`cat`过程)，将相关信息发送回用户空间。`cat`流程然后忠实地将通过读取接收到的数据转储到`stdout`，因此我们看到:流程的所有段或映射——实际上，用户模式 VAS 的内存映射！

好了，至此，我们结束了这一部分，在这一部分中，我们已经介绍了检查流程用户视觉模拟系统的细节。这些知识不仅有助于理解用户模式 VAS 的精确布局，还有助于调试用户空间内存问题！

现在，让我们继续了解内存管理的另一个关键方面——内核 VAS 的详细布局，换句话说，内核部分。

# 检查内核段

正如我们在上一章中所谈到的，正如在*图 7.5* 中所看到的，理解所有进程都有自己独特的用户 VAS 但共享内核空间——我们称之为内核段或内核 VAS——真的很关键。让我们从检查内核部分的一些常见(与 arch 无关)区域开始这一部分。

内核段的内存布局非常依赖于内存。然而，所有的架构都有一些共同之处。下图显示了用户视觉模拟和内核段(水平平铺格式)，如在 x86_32 上看到的 3:1 虚拟机分割:

![](Images/ef5b8d7a-8bd8-4467-855c-0314c4ce0087.png)

Figure 7.10 – User and kernel VASes on an x86_32 with a 3:1 VM split with focus on the lowmem region

让我们逐个检查每个区域:

*   **用户模式 VAS** :这是用户 VAS；我们已经在前一章以及本章前面的章节中详细介绍了它；在这个特殊的例子中，需要 3 GB 的 VAS(从`0x0`到`0xbfff ffff`的 UVAs)。
*   接下来的都属于内核 VAS 或者内核段；在这个特殊的例子中，需要 1 GB 的 VAS(从`0xc000 0000`到`0xffff ffff`的千伏安)；现在让我们检查一下它的各个部分。
*   **低内存区域**:这是平台(系统)内存直接映射到内核的地方。(我们将在*直接映射内存和地址转换*部分*中更详细地讨论这个关键主题。*如果觉得有帮助，可以先看那一节，然后再回到这里)。先跳过一点，让我们理解平台内存映射的内核段中的基本位置是由一个名为`PAGE_OFFSET`的内核宏指定的。这个宏的精确值非常依赖于拱门；我们将把这个讨论留到后面的部分。现在，我们请你们相信，在虚拟机比例为 3:1 的 IA-32 上，`PAGE_OFFSET`的值是`0xc000 0000`。

内核内存区域的长度或大小等于系统的内存量。(嗯，至少内核看到的内存量；例如，启用 kdump 工具会让操作系统提前预留一些内存)。组成该区域的虚拟地址被称为**内核逻辑地址**，因为它们与它们的物理对应物有固定的偏移。核心内核和设备驱动程序可以分配(物理上连续！)通过各种 API 从这个区域获得的内存(我们将在下面的两章中详细介绍这些 API)。内核静态文本(代码)、数据和 BSS(未初始化数据)内存也驻留在这个 lowmem 区域中。

*   **内核虚拟区域**:这是一个完全虚拟的内核虚拟空间区域。核心内核和/或设备驱动程序代码可以使用`vmalloc()`(和朋友)应用编程接口从该区域分配几乎连续的内存。同样，我们将在[第 8 章](08.html)、*模块作者内核内存分配第 1 部分*和[第 9 章](09.html)、*模块作者内核内存分配第 2 部分*中对此进行详细介绍。这也是所谓的`ioremap`空间。

*   **内核模块空间**:为**可加载内核模块** ( **LKMs** )的静态文本和数据占用的内存留出一个内核 VAS 区域。当您执行`insmod(8)`时，产生的`[f]init_module(2)`系统调用的底层内核代码从该区域分配内存(通常通过`vmalloc()`应用编程接口)，并在那里加载内核模块的(静态)代码和数据。

前面的图(图 7.10)故意被简化，甚至有点模糊，因为确切的内核虚拟内存布局非常依赖于内存。我们暂时不打算画详细的图表。相反，为了使这个讨论不那么迂腐，更实用和有用，我们将在接下来的部分中介绍一个内核模块，它查询并打印关于内核段布局的相关信息。只有到那时，一旦我们有了特定架构的内核段的不同区域的实际值，我们才会给出一个详细的图表来描述这一点。

Pedantically (as can be seen in Figure 7.10), the addresses belonging to the lowmem region are termed kernel logical addresses (they're at a fixed offset from their physical counterparts), whereas the addresses for the remainder of the kernel segment are termed KVAs. Though this distinction is made here, please realize that, for all practical purposes, it's a rather pedantic one: we will often simply refer to all addresses within the kernel segment as KVAs.

在此之前，还有其他几条信息需要介绍。让我们从另一个特性开始，这主要是由 32 位架构的限制带来的:所谓的内核段的高内存区域。

## 32 位系统上的高内存

关于我们之前简单讨论过的内核 lowmem 区域，一个有趣的观察随之而来。在 32 位系统上，比如说 3:1 (GB)虚拟机分割(如图 7.10 所示)，一个具有(比如说)512 MB 内存的系统将从`PAGE_OFFSET` (3 GB 或 KVA `0xc000 0000`)开始将其 512 MB 内存直接映射到内核中。这是很清楚的。

但是想想看:如果系统有更多的内存，比如 2 GB，会发生什么？很明显，我们不能直接将整个内存映射到低内存区域。它就是装不下(因为，在这个例子中，整个可用的内核 VAS 只是一个千兆字节，RAM 是 2gb)！因此，在 32 位 Linux 操作系统上，一定量的内存(通常是 IA-32 上的 768 MB)被允许直接映射，因此属于低内存区域。剩余的内存是*间接映射*到另一个名为`ZONE_HIGHMEM`的内存区域(我们认为它是高内存区域或*区域*，而不是低内存；关于内存区域的更多信息，请参见后面的*区域*。更正确地说，由于内核现在发现不可能一次直接映射所有物理内存，它建立了一个(虚拟)区域，可以在其中建立和使用该内存的临时虚拟映射。这就是所谓的高记忆区域。

Don't get confused by the phrase "high memory"; one, it's not necessarily placed "high" in the kernel segment, and two, this is not what the `high_memory` global variable represents – it (`high_memory`) represents the upper bound of the kernel's lowmem region. More on this follows in a later section, *Macros and variables describing the kernel segment layout*.

然而，如今(尤其是随着 32 位系统越来越少被使用)，这些担忧在 64 位 Linux 上完全消失了。想想看:在 64 位 Linux 上，内核段的大小高达 128 TB(！)在 x86_64 上。现有的任何一个系统都没有这么大的内存。因此，所有的平台内存确实可以(很容易地)直接映射到内核部分，并且不再需要`ZONE_HIGHMEM`(或等效物)。

同样，内核文档提供了这个“高内存”区域的详细信息。感兴趣的话就来看看:[https://www.kernel.org/doc/Documentation/vm/highmem.txt](https://www.kernel.org/doc/Documentation/vm/highmem.txt)。

好了，现在让我们着手处理我们一直在等待做的事情——编写一个内核模块(一个 LKM)来深入研究内核部分的一些细节。

## 编写内核模块来显示关于内核段的信息

正如我们所知，内核部分由不同的区域组成。有些是所有体系结构共有的(与 arch 无关):它们包括 lowmem 区域(其中包含未压缩的内核映像—它的代码、数据、BSS)、内核模块区域、`vmalloc` / `ioremap`区域等等。

内核段中这些区域所在的精确位置，以及哪些区域可能存在，非常依赖于内存。为了帮助理解和确定任何给定的系统，让我们开发一个内核模块，它查询和打印关于内核段的各种细节(事实上，如果被要求，它也打印一些有用的用户空间内存细节)。

### 通过 dmesg 查看树莓皮上的仁段

在进入并分析这样一个内核模块的代码之前，事实是，与我们在这里尝试的非常相似的事情——打印内核段/VAS 中各种有趣区域的位置和大小——已经在流行的树莓皮(ARM) Linux 内核的早期引导中执行了。在下面的代码片段中，我们显示了树莓 Pi 3 B+(运行股票(默认)32 位树莓 Pi 操作系统)启动时内核日志的相关输出:

```sh
rpi $ uname -r 4.19.97-v7+ rpi $ journalctl -b -k
[...]
Apr 02 14:32:48 raspberrypi kernel: Virtual kernel memory layout:
                       vector  : 0xffff0000 - 0xffff1000   (   4 kB)
                       fixmap  : 0xffc00000 - 0xfff00000   (3072 kB)
                       vmalloc : 0xbb800000 - 0xff800000   (1088 MB)
                       lowmem  : 0x80000000 - 0xbb400000   ( 948 MB)
                       modules : 0x7f000000 - 0x80000000   (  16 MB)
                         .text : 0x(ptrval) - 0x(ptrval)   (9184 kB)
                         .init : 0x(ptrval) - 0x(ptrval)   (1024 kB)
                         .data : 0x(ptrval) - 0x(ptrval)   ( 654 kB)
                          .bss : 0x(ptrval) - 0x(ptrval)   ( 823 kB)
[...]
```

It's important to note that these preceding prints are very specific to the OS and device. The default Raspberry Pi 32-bit OS prints this information out, while others may not: **YMMV** (**Your Mileage May Vary**!). For example, with the standard 5.4 kernel for Raspberry Pi that I built and ran on the device, these informative prints weren't present. On recent kernels (as seen in the preceding logs on the 4.19.97-v7+ Raspberry Pi OS kernel), for security reasons – that of preventing kernel information leakage – many early `printk` functions will not display a "real" kernel address (pointer) value; you might simply see it prints the `0x(ptrval)` string.

This **`0x(ptrval)`** output implies that the kernel is deliberately not showing even a hashed printk (recall the `%pK` format specifier from [Chapter 5](05.html), *Writing Your First Kernel Module – LKMs Part 2*) as the system entropy is not yet high enough. If you insist on seeing a (weakly) hashed printk, you can always pass the `debug_boot_weak_hash` kernel parameter at boot (look up details on kernel boot parameters here: [https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html)).

有趣的是，(如前一个信息框所述)，打印这个`Virtual kernel memory layout :` 信息的代码是非常具体的树莓皮内核补丁！可以在树莓 Pi 内核源码树这里找到:[https://github . com/raspberrpi/Linux/blob/rpi-5.4 . y/arch/arm/mm/init . c](https://github.com/raspberrypi/linux/blob/rpi-5.4.y/arch/arm/mm/init.c)。

现在，为了让您查询和打印类似的信息，您必须首先熟悉一些关键的内核宏和全局。；让我们在下一节中这样做。

### 描述内核段布局的宏和变量

要编写一个显示相关内核段信息的内核模块，我们需要知道如何确切地询问内核这些细节。在本节中，我们将简要描述内核中代表内核段内存的几个关键宏和变量(在大多数架构上，按照 KVA 的降序排列):

*   **向量表**是一种常见的操作系统数据结构——它是一个函数指针数组(也称为切换或跳转表)。它是特定于 arch 的:ARM-32 使用它来初始化其向量，以便当处理器异常或模式改变(如中断、系统调用、页面故障、MMU 中止等)发生时，处理器知道运行什么代码:

| **宏或变量** | **解读** |
| `VECTORS_BASE` | 通常仅 ARM-32；跨越 1 页的核向量表的开始 KVA |

*   **固定映射区域**是一系列编译时特殊或保留的虚拟地址；它们在引导时被用来将必需的内核元素修复到内核段中，这些内核元素必须有可用的内存。典型的例子包括初始内核页表的设置，早期的`ioremap`和`vmalloc`区域等等。同样，它是一个依赖于拱形的区域，因此在不同的中央处理器上有不同的用法:

| **宏或变量** | **解读** |
| `FIXADDR_START` | 跨越`FIXADDR_SIZE`字节的内核固定映射区域的开始 KVA |

*   **内核模块**在内核段的特定范围内被分配内存——用于其静态文本和数据。内核模块区域的精确位置因架构而异。在 ARM 32 位系统上，事实上，它就放在用户 VAS 的正上方；而在 64 位上，它通常在内核部分更高:

| **内核模块(LKMs)区域** | **从这里分配的内存，用于静态代码 LKMs 的数据** |
| **`MODULES_VADDR`** | 启动内核模块区域的 KVA |
| `MODULES_END` | 内核模块区域的结束 KVA；尺寸为`MODULES_END - MODULES_VADDR` |

*   **KASAN** *:* 现代内核(x86_64 从 4.0 开始，ARM64 从 4.4 开始)采用强大的机制来检测和报告内存问题。它基于用户空间**地址杀毒软件***(****)*代码库，因此被称为**内核地址杀毒软件** ( **KASAN** ) *。*它的能力在于(通过编译时工具)巧妙地检测内存问题，如**空闲后使用**(**【UAF】**)和**越界** ( **OOB** )访问(包括缓冲区溢出/不足流)。然而，它只在 64 位 Linux 上工作*，并且需要相当大的**影子内存区域**(其大小是内核 VAS 的八分之一，如果启用的话，我们会显示其范围)。这是一个内核配置特性(`CONFIG_KASAN`)并且通常只为调试目的而启用(但是在调试和测试期间保持启用真的很关键！):***

 **| **KASAN 阴影存储区(仅 64 位)** | **【可选】(仅适用于 64 位，且仅在定义了 CONFIG_KASAN 的情况下；详见如下)** |
| `KASAN_SHADOW_START` | 开赛地区的 KVA |
| `KASAN_SHADOW_END` | 结束卡桑地区的 KASAN 尺寸为`KASAN_SHADOW_END - KASAN_SHADOW_START` |

*   **vmalloc 区域**是为`vmalloc()`(和朋友)API 分配内存的空间；我们将在接下来的两章中详细介绍各种内存分配 API:

| **vmalloc 区域** | **用于通过 vmalloc()和好友分配的内存** |
| **`VMALLOC_START`** | 启动`vmalloc`区域的 KVA |
| `VMALLOC_END` | 结束`vmalloc`区域的 KVA；尺寸为`VMALLOC_END - VMALLOC_START` |

*   ****低内存区域**——在`1:1 :: physical page frame:kernel page`基础上将内存直接映射到内核段——实际上是 Linux 内核映射和管理(通常)所有内存的区域。此外，它通常在内核中被设置为`ZONE_NORMAL`(稍后我们也会介绍区域):**

 **| **洛门区域** | **直接映射存储区** |
| `PAGE_OFFSET` | 从洛梅姆地区的 KVA 开始；也代表某些架构上内核段的开始，并且(通常)是 32 位上的 VM 拆分值。 |
| `high_memory` | 低内存区域的末端 KVA，直接映射内存的上限；实际上，这个值减去`PAGE_OFFSET`就是系统上的(平台)RAM 的数量(小心，这不一定是所有拱门上的情况)；不要与`ZONE_HIGHMEM`混淆。 |

*   ****高地区域**或地带是可选区域。它可能存在于一些 32 位系统上(通常，内存量大于内核段本身的大小)。在这种情况下，它通常被设置为`ZONE_HIGHMEM`(稍后我们将讨论区域。此外，您可以在前面题为*32 位系统上的高内存*一节中查阅关于这个高内存区域的更多信息:**

 **| **高内存区域(仅适用于 32 位)** | **【可选】一些 32 位系统上可能存在 HIGHMEM】** |
| `PKMAP_BASE` | 启动高内存区域的 KVA，一直运行到`LAST_PKMAP`页；表示所谓的高内存页面的内核映射(较旧，仅适用于 32 位) |

*   (未压缩)**内核映像**本身——其代码、`init`和数据区域——是私有符号，因此对内核模块不可用；我们不打算打印它们:

| **内核(静态)图像** | **未压缩内核镜像的内容(见下图)；未导出，因此无法用于模块** |
| `_text, _etext` | 内核文本(代码)区域的开始和结束千伏安(分别) |
| `__init_begin, __init_end` | 内核`init`部分区域的开始和结束千伏安(分别) |
| `_sdata, _edata` | 内核静态数据区的开始和结束千伏安(分别) |
| `__bss_start, __bss_stop` | 内核 BSS(未初始化数据)区域的开始和结束 KVAs(分别) |

*   **用户 VAS** :最后一项，当然是流程用户 VAS。它位于内核段之下(按虚拟地址降序排序)，大小为`TASK_SIZE`字节。本章前面已经详细讨论过:

| **用户去** | **用户虚拟地址空间(VAS)** |
| (用户模式 VAS 如下)`TASK_SIZE` | (之前通过`procfs`或我们的`procmap`实用程序脚本详细检查过)；内核宏`TASK_SIZE`代表用户 VAS 的大小(字节)。 |

嗯，就是这样；我们已经看到了几个内核宏和变量，它们实际上描述了内核 VAS。

转到我们内核模块的代码，您很快就会看到它的`init`方法调用了两个函数(这很重要):

*   `show_kernelseg_info()`，打印相关内核段详细信息
*   `show_userspace_info()`，打印相关的用户 VAS 详细信息(可选，通过内核参数决定)

我们将从描述内核段函数并查看其输出开始。同样，Makefile 的设置方式是，它链接到我们的内核库代码的对象文件中，`klib_llkd.c`*，并生成一个名为`show_kernel_seg.ko`的内核模块对象。*

 *### 尝试一下——查看内核部分的详细信息

为了清楚起见，我们将在这一部分只显示源代码的相关部分。一定要克隆并使用本书 GitHub 存储库中的完整代码。还有，回想一下前面提到的`procmap`效用；它有一个内核组件，一个 LKM，它确实做了和这个类似的工作——让内核级信息对用户空间可用。随着它变得更加复杂，我们在这里不再深入研究它的代码；看到下面演示内核模块`show_kernel_seg`的代码就足够了:

```sh
// ch7/show_kernel_seg/kernel_seg.c
[...]
static void show_kernelseg_info(void)
{
    pr_info("\nSome Kernel Details [by decreasing address]\n"
    "+-------------------------------------------------------------+\n");
#ifdef CONFIG_ARM
  /* On ARM, the definition of VECTORS_BASE turns up only in kernels >= 4.11 */
#if LINUX_VERSION_CODE > KERNEL_VERSION(4, 11, 0)
    pr_info("|vector table: "
        " %px - %px | [%4ld KB]\n",
        SHOW_DELTA_K(VECTORS_BASE, VECTORS_BASE + PAGE_SIZE));
#endif
#endif
```

前面的代码片段显示了 ARM 向量表的范围。当然是有条件的。输出只出现在 ARM-32 上，因此出现了`#ifdef CONFIG_ARM`预处理器指令。(此外，我们使用`%px` printk 格式说明符确保了代码的可移植性。)

本演示内核模块中使用的`SHOW_DELTA_*()`宏在我们的`convenient.h`头中定义，是帮助器，使我们能够轻松显示传递给它的低值和高值，计算传递的两个量之间的差值(差值)，并显示出来；以下是相关代码:

```sh
// convenient.h
[...]
/* SHOW_DELTA_*(low, hi) :
 * Show the low val, high val and the delta (hi-low) in either bytes/KB/MB/GB, as required.
 * Inspired from raspberry pi kernel src: arch/arm/mm/init.c:MLM()
 */
#define SHOW_DELTA_b(low, hi) (low), (hi), ((hi) - (low))
#define SHOW_DELTA_K(low, hi) (low), (hi), (((hi) - (low)) >> 10)
#define SHOW_DELTA_M(low, hi) (low), (hi), (((hi) - (low)) >> 20)
#define SHOW_DELTA_G(low, hi) (low), (hi), (((hi) - (low)) >> 30)
#define SHOW_DELTA_MG(low, hi) (low), (hi), (((hi) - (low)) >> 20), (((hi) - (low)) >> 30)
```

在下面的代码中，我们展示了发出描述以下区域范围的`printk`函数的代码片段:

*   内核模块区域
*   (可选)KASAN 地区
*   vmalloc 地区
*   最低和可能最高的区域

关于内核模块区域，正如在下面的源代码中的详细注释中所解释的，我们尝试按照 KVAs 降序来保持顺序:

```sh
// ch7/show_kernel_seg/kernel_seg.c
[...]
/* kernel module region
 * For the modules region, it's high in the kernel segment on typical 64- 
 * bit systems, but the other way around on many 32-bit systems 
 * (particularly ARM-32); so we rearrange the order in which it's shown 
 * depending on the arch, thus trying to maintain a 'by descending address' ordering. */
#if (BITS_PER_LONG == 64)
  pr_info("|module region: "
    " %px - %px | [%4ld MB]\n",
    SHOW_DELTA_M(MODULES_VADDR, MODULES_END));
#endif

#ifdef CONFIG_KASAN     // KASAN region: Kernel Address SANitizer
  pr_info("|KASAN shadow: "
    " %px - %px | [%2ld GB]\n",
    SHOW_DELTA_G(KASAN_SHADOW_START, KASAN_SHADOW_END));
#endif

  /* vmalloc region */
  pr_info("|vmalloc region: "
    " %px - %px | [%4ld MB = %2ld GB]\n",
    SHOW_DELTA_MG(VMALLOC_START, VMALLOC_END));

  /* lowmem region */
  pr_info("|lowmem region: "
    " %px - %px | [%4ld MB = %2ld GB]\n"
#if (BITS_PER_LONG == 32)
    "|            (above:PAGE_OFFSET - highmem)     |\n",
#else
    "|                (above:PAGE_OFFSET - highmem) |\n",
#endif
    SHOW_DELTA_MG((unsigned long)PAGE_OFFSET, (unsigned long)high_memory));

  /* (possible) highmem region; may be present on some 32-bit systems */
#ifdef CONFIG_HIGHMEM
  pr_info("|HIGHMEM region: "
    " %px - %px | [%4ld MB]\n",
    SHOW_DELTA_M(PKMAP_BASE, (PKMAP_BASE) + (LAST_PKMAP * PAGE_SIZE)));
#endif
[ ... ]
#if (BITS_PER_LONG == 32) /* modules region: see the comment above reg this */
  pr_info("|module region: "
    " %px - %px | [%4ld MB]\n",
    SHOW_DELTA_M(MODULES_VADDR, MODULES_END));
#endif
  pr_info(ELLPS);
}
```

让我们在 ARM-32 树莓 Pi 3 B+上构建并插入我们的 LKM；下面的截图显示了它的设置，然后是内核日志:

![](Images/0ceba7c6-eb91-4774-814d-c902e8de24c9.png)

Figure 7.11 – Output from the show_kernel_seg.ko LKM on a Raspberry Pi 3B+ running stock Raspberry Pi 32-bit Linux

正如预期的那样，我们收到的关于内核段的输出与引导时树莓皮内核本身打印的内容完全匹配(您可以参考*通过 dmesg 查看树莓皮上的内核段*部分来验证这一点)。从`PAGE_OFFSET`(图 7.11 中的 KVA `0x8000 0000`)的值可以看出，我们的树莓皮内核的虚拟机分割配置为 2:2 (GB)(因为十六进制值`0x8000 0000`的十进制基数为 2 GB。有趣的是，最近的树莓 Pi 4 型设备上的默认树莓 Pi 32 位操作系统配置了 3:1 (GB)虚拟机拆分)。

Technically, on ARM-32 systems, at least, user space is slightly under 2 GB (*2 GB – 16 MB = 2,032 MB*) as this 16 MB is taken as the *kernel module region* just below `PAGE_OFFSET`; indeed, exactly this can be seen in Figure 7.11 (the kernel module region here spans from `0x7f00 0000` to `0x8000 0000` for 16 MB). Also, as you'll soon see, the value of the `TASK_SIZE` macro – the size of the user VAS – reflects this fact as well.

我们在下图中展示了大部分信息:

![](Images/ff00619a-d778-4ecc-a5b6-4b983887ec67.png)

Figure 7.12 – The complete VAS of a process on ARM-32 (Raspberry Pi 3B+) with a 2:2 GB VM split Do note that due to variations in differing models, the amount of usable RAM, or even the device tree, the layout shown in Figure 7.12 may not precisely match that on the Raspberry Pi you have.

好了，现在你知道如何在内核模块中打印相关的内核段宏和变量，帮助你理解任何 Linux 系统上的内核 VM 布局！在下一节中，我们将尝试“查看”(可视化)内核 VAS，这次是通过我们的`procmap`实用程序。

### 通过 procmap 实现内核 VAS

好吧，这很有趣:上图中看到的内存映射布局的细节视图正是我们前面提到的`procmap`实用程序所提供的！如前所述，现在让我们看看运行`procmap`时内核 VAS 的截图(之前我们展示了用户 VAS 的截图)。

为了与当前的讨论保持同步，我们现在将显示`procmap`的截图，该截图在完全相同的树莓皮 3B+系统上提供了内核 VAS 的“可视化”视图(我们可以指定`--only-kernel`开关以仅显示内核 VAS；不过，我们这里不这么做)。由于我们必须在某个流程上运行`procmap`，我们任意选择*系统*PID`1`；我们也使用`--verbose`选项开关。然而，它似乎失败了:

![](Images/f0afd519-4c3e-4af2-8529-b2af57045fe6.png)

Figure 7.13 – Truncated screenshot showing the procmap kernel module build failing

为什么没有构建内核模块(那是`procmap`项目的一部分)？我在项目的`README.md`文件([https://github . com/万凯/proc map/blob/master/readme . MD # proc map](https://github.com/kaiwan/procmap/blob/master/README.md#procmap))中提到了这一点:

```sh
[...]to build a kernel module on the target system, you will require it to have a kernel development environment setup; this boils down to having the compiler, make and - key here - the 'kernel headers' package installed for the kernel version it's currently running upon. [...]
```

我们的*定制* 5.4 内核(树莓皮)的内核头包不可用，因此它失败了。虽然您可以将整个 5.4 树莓皮内核源代码树复制到设备上并设置`/lib/module/<kver>/build`符号链接，但这被认为不是正确的方法。那么，什么是？*交叉编译*`procmap`内核模块，当然是来自你的主机！在[第 3 章](03.html)、*从源代码构建 5.x Linux 内核-第 2 部分*的*树莓皮内核构建*部分，我们已经介绍了树莓皮内核本身交叉编译的细节；当然，它也适用于交叉编译内核模块。

I want to stress this point: the `procmap` kernel module build on the Raspberry Pi only fails due to the lack of a Raspberry Pi-supplied kernel headers package when running a custom kernel. If you are happy to work with the stock (default) Raspberry Pi kernel (earlier called Raspbian OS), the kernel headers package is certainly installable (or already installed) and everything will work. Similarly, on your typical x86_64 Linux distribution, the `procmap.ko` kernel module gets cleanly built and inserted at runtime. Do read the `procmap` project's `README.md` file in detail; within it, the section labeled *IMPORTANT: Running procmap on systems other than x86_64* details how to cross-compile the `procmap` kernel module.

一旦你成功地在你的主机系统上交叉编译了`procmap`内核模块，复制`procmap.ko`内核模块(通过`scp(1)`，也许)到设备，并把它放在`procmap/procmap_kernel`目录下；现在你准备好出发了！

这里是复制的内核模块(在树莓皮上):

```sh
cd <...>/procmap/procmap_kernel
ls -l procmap.ko
-rw-r--r-- 1 pi pi 7909 Jul 31 07:45 procmap.ko
```

(也可以在上面运行`modinfo(8)`实用程序，验证它是为 ARM 构建的。)

有了这些，让我们再次运行`procmap`来显示内核 VAS 的详细信息:

![](Images/a5e97d6b-a3bd-45e6-836a-495876e137c5.png)

Figure 7.14 – Truncated screenshot showing the procmap kernel module successfully inserted and various system details

它现在确实工作了！正如我们为`procmap`指定的`verbose`选项，您可以看到它的详细进度，以及——非常有用的——各种感兴趣的内核变量/宏及其当前值。

好了，让我们继续，看看我们真正追求的是什么——树莓皮 3B+上内核视觉模拟系统的“视觉地图”，由 KVA 按降序排列；下面的截图捕捉到了`procmap`的这个输出:

![](Images/042df779-2a7a-471a-abec-17ba87891b39.png)

Figure 7.15 – Partial screenshot of our procmap utility's output showing the complete kernel VAS (Raspberry Pi 3B+ with 32-bit Linux)

显示完整的内核 VAS–从`end_kva`(值`0xffff ffff`)到内核的开始，`start_kva` ( `0x7f00 0000`，如您所见，这是内核模块区域)。请注意(绿色)某些关键地址右侧的标签，表示它们是什么！为了完整起见，我们还在前面的截图中包含了内核-用户边界(以及内核部分下面的用户 VAS 的上部，就像我们一直说的那样！).由于前面的输出是在 32 位系统上，用户视觉模拟系统会紧跟在内核段之后。但是在 64 位系统上，有一个(巨大的！)内核段开始和用户 VAS 顶部之间的“非规范”稀疏区域。在 x86_64 上(正如我们已经讨论过的)，它跨越了绝大多数的 VAS: 16，383.75 petabytes(总 VAS 为 16，384 petabytes)！

我将把它作为一个练习留给您来运行这个`procmap`项目，并仔细研究输出(在您的 x86_64 或任何盒子或虚拟机上)。它在 3:1 虚拟机分割的 BeagleBone Black 嵌入式板上也能很好地工作，显示出预期的细节。仅供参考，这是一项任务。

I also provide a solution in the form of three (large, stitched-together) screenshots of `procmap`'s output on a native x86_64 system, a BeagleBone Black (AArch32) board, and the Raspberry Pi running a 64-bit OS (AArch64) here: `solutions_to_assgn/ch7`. Studying the code of `procmap`*,* and, especially relevant here, its kernel module component, will certainly help. It's open source, after all!

让我们通过浏览我们早期的演示内核模块–`ch7/show_kernel_seg`–提供的用户细分视图来结束这一部分。

### 尝试一下——用户细分市场

现在，让我们回到我们的`ch7/show_kernel_seg` LKM 演示程序。我们提供了一个名为`show_uservas`的内核模块参数(默认为`0`值)；当设置为`1`时，也会显示一些关于过程上下文的*用户空间*的细节。下面是模块参数的定义:

```sh
static int show_uservas;
module_param(show_uservas, int, 0660);
MODULE_PARM_DESC(show_uservas,
"Show some user space VAS details; 0 = no (default), 1 = show");
```

对，在同一个设备(我们的树莓 Pi 3 B+)上，让我们再次运行我们的`show_kernel_seg`内核模块，这次也请求它显示用户空间细节(通过前面提到的参数)。下面的截图显示了完整的输出:

![](Images/4be8fce6-3eee-40d6-ab91-97dcecad4bc4.png)

Figure 7.16 – Screenshot of our show_kernel_seg.ko LKM's output showing both kernel and user VAS details when running on a Raspberry Pi 3B+ with the stock Raspberry Pi 32-bit Linux OS

这很有用；我们现在可以在一个镜头中看到这个过程(或多或少)完整的内存映射——既有所谓的“上(规范)半”内核空间，也有“下(规范)半”用户空间(是的，没错，尽管`procmap`项目更好、更详细地展示了这一点)。

我将把它作为一个练习留给您来运行这个内核模块，并仔细研究您的 x86_64 上的输出，或者任何一个盒子或虚拟机。一定要仔细检查代码。我们通过从`current`中取消引用`mm_struct`结构(名为`mm`的任务结构成员)，打印了您在前面截图中看到的用户空间详细信息，例如段的开始和结束地址。回想一下，`mm`是流程的用户映射的抽象。下面是一小段代码:

```sh
// ch7/show_kernel_seg/kernel_seg.c
[ ... ]
static void show_userspace_info(void)
{
    pr_info (
    "+------------ Above is kernel-seg; below, user VAS  ----------+\n"
    ELLPS
    "|Process environment "
      " %px - %px | [ %4zd bytes]\n"
    "| arguments "
    " %px - %px | [ %4zd bytes]\n"
    "| stack start %px\n"
    [...],
        SHOW_DELTA_b(current->mm->env_start, current->mm->env_end),
        SHOW_DELTA_b(current->mm->arg_start, current->mm->arg_end),
        current->mm->start_stack,
    [...]
```

还记得用户 VAS 最开始的所谓空陷阱页面吗？(同样，`procmap`的输出–参见*图 7.9*–显示了零陷页面。)让我们在下一节中看看它的用途。

#### 空陷阱页面

你注意到前面的图(图 7.9)和图 7.12 中最左边的图(尽管很小！)，用户空间最开始的单个页面，命名为**空陷阱**页面？这是什么？很简单:虚拟页面`0`没有权限(在硬件 MMU/PTE 级别)。因此，对该页面的任何访问，无论是`r`、`w`还是`x`(读/写/执行)，都将导致 MMU 引发所谓的故障或异常。这将使处理器跳转到操作系统处理程序例程(故障处理程序)。它运行，杀死试图访问没有权限的内存区域的罪犯！

确实很有意思:前面提到的 OS 处理程序在进程上下文中运行，猜猜`current` 是什么:为什么，是进程(或者线程)发起了这个糟糕的`NULL`指针查找！在故障处理程序代码中，`SIGSEGV`信号被传送到故障处理程序(`current`，导致其死亡(通过 segfault)。简而言之，这就是众所周知的`NULL`指针取消引用错误是如何被操作系统捕获的。

### 查看关于内存布局的内核文档

回到内核段；显然，对于 64 位 VAS，内核段*比 32 位上的*大得多。正如我们之前看到的，x86_64 上通常为 128 TB。再次研究之前显示的虚拟机拆分表(图 7.4 中的*虚拟机在 64 位 Linux 系统上的拆分*)；在那里，第四列是不同体系结构的虚拟机拆分。您可以看到，在 64 位英特尔/AMD 和 AArch64 (ARM64)上，数字比 32 位同类产品大得多。有关特定于 arch 的详细信息，请参阅此处关于进程虚拟内存布局的“官方”内核文档:

| **架构** | **内核源代码树中的文档位置** |
| ARM-32 | `Documentation/arm/memory.txt`。 |
| aarh64 足球俱乐部 | `Documentation/arm64/memory.txt`。 |
| x86_64 | `Documentation/x86/x86_64/mm.txt` 注:本文档的可读性最近(截至撰写本文时)因 Linux 4.20 的 commit`32b8976`:[https://github . com/Torvalds/Linux/commit/32b 89760 ddf 4477 da 436 c 272 be 2 ab c 016 e 169031](https://github.com/torvalds/linux/commit/32b89760ddf4477da436c272be2abc016e169031)而大大提高。推荐你浏览一下这个文件:[https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt](https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt)。 |

At the risk of repetition, I urge you to try out this `show_kernel_seg` kernel module – and, even better, the `procmap` project ([https://github.com/kaiwan/procmap](https://github.com/kaiwan/procmap)) – on different Linux systems and study the output. You can then literally see the "memory map" – the complete process VAS – of any given process, which includes the kernel segment! This understanding is critical when working with and/or debugging issues at the system layer.

同样，冒着夸大其词的风险，前面两个部分——涵盖了对*用户和内核花瓶*的详细检查——确实非常重要。一定要花时间复习它们，并完成示例代码和作业。干得好！

继续我们的 Linux 内核内存管理之旅，现在让我们来看看另一个有趣的话题——通过内存布局随机化的[K]ASLR 保护功能。继续读！

# 随机存储布局–KASLR

在信息安全领域，众所周知的事实是，利用 **proc 文件系统** ( **procfs** )和各种强大的工具，一个恶意用户，事先知道各种函数的精确位置(虚拟地址)和/或一个进程的 VAS 的全局，可以设计一个攻击来利用并最终危害给定的系统。因此，为了安全起见，为了使攻击者不可能(或至少很难)依赖“已知的”虚拟地址，用户空间以及内核空间支持 **ASLR(地址空间布局随机化)**和 **KASLR(内核 ASLR)** 技术(通常发音为*Ass-**ler*/*Kass-ler*)。

这里的关键词是*随机化:*该特性，当被启用时，*根据绝对数字改变进程(和内核)存储器布局的部分的位置*，因为它*将存储器的部分*从给定的基址偏移一个随机的(页面对齐的)量。我们到底在说什么“记忆的一部分”？关于用户空间映射(我们将在后面讨论 KASLR)，共享库的起始地址(它们的加载地址)，基于`mmap(2)`的分配(记住，任何 128 KB 以上的`malloc()`函数(`/calloc/realloc` *)* 变成了基于`mmap`的分配，而不是脱离堆)，栈起始，堆，以及 vDSO 页；所有这些都可以在进程运行(启动)时随机化。

因此，攻击者不能依赖于，比如说，`glibc`函数(比如`system(3)`)在任何给定的过程中被映射到特定的固定 UVA 不仅如此，每次流程运行时，位置都会发生变化！在 ASLR 之前，以及在不支持或关闭 ASLR 的系统上，对于给定的架构和软件版本，可以预先确定符号的位置(procfs plus 实用程序，如`objdump`、`readelf`、`nm`等，使这变得非常容易)。

关键是要认识到，ASLR 只是一种统计保护。事实上，通常没有多少比特可用于随机化，因此熵不是很好。这意味着页面大小的偏移量不会太多，即使在 64 位系统上也是如此，因此可能会削弱实现。

现在让我们简要地看一些关于用户模式和内核模式 ASLR(后者被称为 KASLR)的更多细节；以下各节分别涵盖这些领域。

## 用户 ASL 模式

用户模式的 ASLR 通常就是术语 ASLR 的意思。启用它意味着这种保护在每个进程的用户空间映射中都可用。实际上，启用 ASLR 意味着用户模式进程的绝对内存映射在每次运行时都会发生变化。

ASLR 已经在 Linux 上支持了很长时间(从 2005 年 2.6.12 开始)。内核在 procfs 中有一个可调的伪文件，用于查询和设置(作为根)ASLR 状态；在这里:`/proc/sys/kernel/randomize_va_space`。

它可以有三个可能的值；下表显示了这三个值及其含义:

| **可调值** | **在`/proc/sys/kernel/randomize_va_space`**中对该值的解释 |
| `0` | (用户模式)ASLR 关闭；或者可以通过在引导时传递内核参数`norandmaps`来关闭。 |
| `1` | (用户模式)ASLR 开启:基于`mmap(2)`的分配、堆栈和 vDSO 页面被随机化。这也意味着共享库加载位置和共享内存段是随机的。 |
| `2` | (用户模式)ASLR 开:前面所有的(值`1` ) *加上*堆位置是随机的(从 2.6.25 开始)；默认情况下，这是操作系统的值。 |

(如前一节所述，*vsyscall 页面*，vDSO 页面是一个系统调用优化，允许一些频繁发出的系统调用(`gettimeofday(2)`是一个典型的)以更少的开销被调用。如果感兴趣，您可以在 vDSO(7)的手册页上查找更多详细信息:这里:[https://man7.org/linux/man-pages/man7/vdso.7.html](https://man7.org/linux/man-pages/man7/vdso.7.html)。 [)](https://man7.org/linux/man-pages/man7/vdso.7.html)

通过将`norandmaps`参数传递给内核(通过引导加载程序)，用户模式 ASLR 可以在引导时关闭*。*

 *## 越狱漏洞

类似于(用户)KASLR 最近，从 3.14 内核开始——甚至*内核* VAS 也可以通过启用 KASLR 来随机化(在某种程度上)。这里，内核和模块代码在内核段中的基本位置将被随机分配一个相对于内存基本位置的页面对齐的随机偏移量。这对该届会议仍然有效；也就是说，直到电源循环或重启。

存在几个内核配置变量，使平台开发人员能够启用或禁用这些随机化选项。作为 x86 的一个具体例子，以下内容直接引用自`Documentation/x86/x86_64/mm.txt`:

"Note that if CONFIG_RANDOMIZE_MEMORY is enabled, the direct mapping of all physical memory, vmalloc/ioremap space and virtual memory map are randomized. Their order is preserved but their base will be offset early at boot time."

可以在引导时通过将参数传递给内核(通过引导加载程序)来控制 KASLR:

*   通过传递`nokaslr`参数，明确关闭
**   通过传递`kaslr`参数，明确开启*

 **那么，你的 Linux 系统当前的设置是什么？我们能改变它吗？当然可以(前提是我们有*根*访问权限)；下一节将向您展示如何通过 Bash 脚本来实现这一点。

## 用脚本查询/设置 KASLR 状态

我们在`<book-source>/ch7/ASLR_check.sh`提供了一个简单的 Bash 脚本。它检查是否存在(用户模式)ASLR 以及 KASLR，打印(彩色编码！)关于它们的状态信息。它还允许您更改 ASLR 值。

让我们在 x86_64 Ubuntu 18.04 来宾上旋转一下。由于我们的脚本被编程为彩色编码，我们在这里显示了它的输出截图:

![](Images/060b2140-c5fc-4118-93d3-930cc7687608.png)

Figure 7.17 – Screenshot showing the output when our ch7/ASLR_check.sh Bash script runs on an x86_64 Ubuntu guest

它会运行，向您显示(至少在此框中)用户模式和 KASLR 确实都已打开。不仅如此，我们还编写了一个小的“测试”例程来查看 ASLR 的功能。很简单:它运行以下命令两次:

```sh
grep -E "heap|stack" /proc/self/maps
```

从您在前面的章节*中了解到的解释/proc/PID/map 输出*，您现在可以在图 7.17 中看到，堆和堆栈段的 UVA 在每次运行中*是不同的，从而证明了 ASLR 特性确实有效！比如看首发堆 UVA:第一次跑是`0x5609 15f8 2000`，第二次跑是`0x5585 2f9f 1000`。*

接下来，我们将执行一个示例运行，将参数`0`传递给脚本，从而关闭 ASLR；以下屏幕截图显示了(预期的)输出:

![](Images/ba12d9b0-8157-4a77-9d0b-c3d220a58285.png)

Figure 7.18 – Screenshot showing how ASLR is turned off (via our ch7/ASLR_check.sh script on an x86_64 Ubuntu guest)

这一次，我们可以看到 ASLR 默认打开，但我们关闭了它。这在前面的截图中用粗体和红色突出显示。(一定要记得再次打开。)而且，正如预期的那样，当它关闭时，堆和堆栈的 UVA(分别)在两次测试运行中都保持不变，这是不安全的。我会留给你浏览和理解脚本的源代码。

To take advantage of ASLR, applications must be compiled with the `-fPIE` and `-pie` GCC flags (**PIE** stands for **Position Independent Executable**).

ASLR 和 KASLR 都可以抵御某些类型的攻击媒介，返回到 libc、**面向返回的编程**(ROP)就是典型的例子。然而，不幸的是，白帽和黑帽安全是猫捉老鼠的游戏，击败 ASLR 和类似的方法是一些先进的漏洞利用做得很好的事情。更多详细信息，请参考本章的*进一步阅读*部分(在 *Linux 内核安全性*标题下)。

While on the topic of security, many useful tools exist to carry out vulnerability checks on your system. Check out the following:

*   `checksec.sh`脚本([http://www.trapkit.de/tools/checksec.html](http://www.trapkit.de/tools/checksec.html))显示各种“强化”措施及其当前状态(对于单个文件和进程):RELRO、堆栈加那利、支持 NX、PIE、RPATH、RUNPATH、符号的存在和编译器强化。
*   grsecurity 的 PaX 套件。
*   `hardening-check`脚本(checksec 的替代方案)。
*   Perl 脚本([https://github.com/a13xp0p0v/kconfig-hardened-check](https://github.com/a13xp0p0v/kconfig-hardened-check))对照一些预定义的清单检查(并建议)内核配置选项的安全性。
*   其他几个:Lynis、`linuxprivchecker.py`、memory 等等。

因此，下次您在多次运行或会话中看到不同的内核或用户虚拟地址时，您会知道这可能是由于[K]ASLR 保护功能。现在，让我们继续探索 Linux 内核如何组织和使用物理内存来完成这一章。

# 物理内存

现在我们已经详细检查了*虚拟内存*视图，对于用户和内核花瓶，让我们转到 Linux 操作系统上物理内存组织的主题。

## 物理内存组织

Linux 内核在启动时，将物理内存组织和划分成一个树状层次结构，由节点、区域和页面框架组成(页面框架是内存的物理页面)(参见图 7.19 和图 7.20)。节点分为区域，区域由页面框架组成。一个节点抽象出一个物理“内存库”，它将与一个或多个处理器内核相关联。在硬件层面，微处理器连接到随机存取存储器控制器芯片；任何内存控制器芯片，以及任何随机存取存储器，都可以通过互连从任何中央处理器获得。现在，很明显，能够到达物理上最接近线程正在分配(内核)内存的内核的内存将导致性能增强。支持所谓的 NUMA 模型的硬件和操作系统利用了这一想法(稍后将解释其含义)。

### 节点

本质上，*节点*是用于表示系统主板上的物理内存模块及其相关控制器芯片组的数据结构。是的，我们说的是实际的*硬件*，这里是通过软件元数据抽象出来的。它总是与系统主板上的物理插槽(或处理器内核集合)相关联。存在两种层次结构:

*   **非统一内存访问(NUMA)** **系统**:内核分配请求发生在哪个内核上确实很重要(内存被统一处理*非*，导致性能提升

*   **统一内存访问(UMA)** **系统**:内核分配请求发生在哪个内核无关紧要(内存统一处理)

真正的 NUMA 系统是那些硬件是多核的系统(两个或多个中央处理器内核，SMP) *和*有两个或多个物理内存“库”，每个内存库与一个中央处理器(或多个中央处理器)相关联。换句话说，NUMA 系统将总是有两个或更多的节点，而 UMA 系统将只有一个节点(参考文献，抽象一个节点的数据结构称为`pg_data_t`，在此定义为:`include/linux/mmzone.h:pg_data_t`)。

你可能会想，为什么这么复杂？嗯，这——还有什么——都是关于性能的！NUMA 系统(它们通常是相当昂贵的服务器级机器)和它们运行的操作系统(Linux/Unix/Windows，通常)的设计方式是，当特定 CPU 内核上的进程(或线程)想要执行内核内存分配时，软件通过从离内核最近的节点获取所需内存(RAM)来保证它以高性能执行分配(因此被称为 NUMA 名字！).UMA 系统(您典型的嵌入式系统、智能手机、笔记本电脑和台式机)不会获得这些好处，它们也不重要。如今的企业级服务器系统可以有数百个处理器和万亿字节，甚至几千兆字节的内存！这些几乎总是被设计成 NUMA 系统。

然而，按照 Linux 的设计方式——这是一个关键点——即使是普通的 UMA 系统也被内核视为 NUMA(嗯，伪 NUMA)。他们将有*正好一个节点；*所以这是检查系统是 NUMA 还是 UMA 的快速方法——如果有两个或更多节点，这是一个真正的 NUMA 系统；只有一个，这是一个“假 NUMA”或伪 NUMA 盒子。怎么查？`numactl(8)`实用程序是一种方法(尝试执行`numactl --hardware`)。还有其他方法(通过*程序*本身)。坚持一下，你会到达那里的...

所以，一个更简单的方法来形象化这一点:在 NUMA 盒子上，一个或多个中央处理器内核与一个物理内存“库”(硬件模块)相关联。因此，NUMA 系统总是一个对称多处理器系统。

为了让这个讨论变得实际，让我们简单地想象一下一个实际服务器系统的微架构——一个运行 AMD Epyc/锐龙/Threadripper(以及旧的推土机)CPU 的系统。它有以下内容:

*   主板上两个物理插槽(P#0 和 P#1)内共有 32 个 CPU 内核(如操作系统所示)。每个插槽由 8x2 个 CPU 内核组成(8x2，因为实际上有 8 个物理内核，每个内核都是超线程；操作系统甚至将超线程内核视为可用内核)。
*   总共 32 GB 的内存分成四个物理存储体，每个存储体 8 GB。

因此，Linux 内存管理代码在引导时检测到这种拓扑后，将设置*四个节点*来表示它。(这里我们不深究处理器的各种(L1/L2/L3/等)缓存；查看下图后的*提示*框，查看所有这些。)

下面的概念图显示了在运行 Linux 操作系统的一些 AMD 服务器系统上形成的四个树状层次结构的近似值，每个节点一个。图 7.19 概念性地显示了耦合到不同 CPU 内核的系统上每个物理内存库的节点/区域/页面帧:

![](Images/2b2a91fb-31a3-45b1-9204-16661e0886b8.png)

Figure 7.19 – (An approximate conceptual view of an) AMD server: physical memory hierarchy on Linux Use the powerful `lstopo(1)` utility (and its associated `hwloc-*` – hardware locality – utilities) to graphically view the hardware (CPU) topology of your system! (On Ubuntu, install it with `sudo apt install hwloc`). FYI, the hardware topography graphic of the previously mentioned AMD server system, generated by `lstopo(1)`, can be seen here: [https://en.wikipedia.org/wiki/CPU_cache#/media/File:Hwloc.png](https://en.wikipedia.org/wiki/CPU_cache#/media/File:Hwloc.png).

在这里重申关键点:为了性能(这里参考图 7.19)，在进程上下文中运行一些内核或驱动程序代码的线程，比如说，在 CPU #18 或更高版本上，向内核请求一些内存。内核的内存管理层，理解 NUMA，将从 NUMA 节点#2 上的任何区域中的任何空闲内存页面帧(即，从物理内存库#2)服务请求(作为第一优先级)，因为它“最接近”发出请求的处理器内核。以防在 NUMA 节点#2 内的任何区域都没有可用的空闲页面帧，内核有一个智能回退系统。现在，它可能会穿过互连，从另一个节点:区域请求内存页面帧(不用担心，我们将在下一章中更详细地介绍这些方面)。

### 区域

区域可以被认为是 Linux 平滑和处理硬件怪癖的方式。这些在 x86 上激增，当然是 Linux“成长”的地方。他们还处理了一些软件困难(在现在主要是遗留的 32 位 i386 架构上查找`ZONE_HIGHMEM`；我们在前面的章节【32 位系统上的高内存】中讨论了这个概念。

区域由*页面框架*组成–内存的物理页面。从技术上讲，一系列的 **P** **年龄帧号**(**pfn**)被分配给节点内的每个区域:

![](Images/a7d6d028-276d-4cc7-95b3-e09ba3dcfb78.png)

Figure 7.20 – Another view of the physical memory hierarchy on Linux – nodes, zones, and page frames

在图 7.10 中，您可以看到一个带有 *N* 节点的通用(示例)Linux 系统(从`0`到`N-1`，每个节点由(比如说)三个区域组成，每个区域由内存的物理页面组成–*页面框架*。每个节点的区域数量(和名称)由内核在启动时动态确定。你可以通过在 *procfs 下钻研来检查 Linux 系统上的层次结构。*在下面的代码中，我们查看了一个具有 16 GB 内存的原生 Linux x86_64 系统:

```sh
$ cat /proc/buddyinfo 
Node 0, zone     DMA      3      2     4    3    3    1   0   0  1   1   3 
Node 0, zone   DMA32  31306  10918  1373  942  505  196  48  16  4   0   0 
Node 0, zone  Normal  49135   7455  1917  535  237   89  19   3  0   0   0
$ 
```

最左边的一列显示我们只有一个节点–`Node 0`。这告诉我们我们实际上是在一个 *UMA 系统*上，当然 Linux 操作系统会把它当作一个(伪/假)NUMA 系统。这个单个节点`0`分为三个区域，分别标记为`DMA`、`DMA32`和`Normal`，每个区域当然都由页面框架组成。现在，忽略右边的数字；我们将在下一章了解它们的含义。

另一种注意 Linux 如何在 UMA 系统上“伪造”NUMA 节点的方式可以从内核日志中看到。我们在具有 16 GB 内存的同一个本机 x86_64 系统上运行以下命令。为了可读性，我用省略号替换了显示时间戳和主机名的前几列:

```sh
$ journalctl -b -k --no-pager | grep -A7 "NUMA"
 <...>: No NUMA configuration found
 <...>: Faking a node at [mem 0x0000000000000000-0x00000004427fffff]
 <...>: NODE_DATA(0) allocated [mem 0x4427d5000-0x4427fffff]
 <...>: Zone ranges:
   <...>:DMA     [mem 0x0000000000001000-0x0000000000ffffff]
 <...>:   DMA32    [mem 0x0000000001000000-0x00000000ffffffff]
 <...>:   Normal   [mem 0x0000000100000000-0x00000004427fffff]
 <...>:   Device   empty
 $
```

我们可以清楚地看到，由于系统被检测为不是 NUMA(因此，UMA)，内核伪造了一个节点。节点的范围是系统的内存总量(这里是`0x0-0x00000004427fffff`，确实是 16 GB)。我们还可以看到，在这个特定的系统上，内核实例化了三个区域——`DMA`、`DMA32`和`Normal`——来组织内存的可用物理页面框架。这很好，并且符合我们之前看到的`/proc/buddyinfo`输出。仅供参考，这里定义了代表 Linux 上*区域*的数据结构:`include/linux/mmzone.h:struct zone`。我们将有机会在本书的稍后部分参观它。

为了更好地理解 Linux 内核是如何组织内存的，让我们从最开始——启动时开始。

## 直接映射内存和地址转换

在启动时，Linux 内核将所有(可用的)系统内存(又称*平台内存*)直接“映射”到内核段。因此，我们有以下内容:

*   物理页面框架`0`映射到内核虚拟页面`0`。
*   物理页面框架`1`映射到内核虚拟页面`1`。
*   物理页面框架`2`映射到内核虚拟页面`2`等等。

因此，我们称之为 1:1 或直接映射、标识映射内存或线性地址。一个关键点是，所有这些内核虚拟页面都与它们的物理对应物有一个固定的偏移量(并且，正如已经提到的，这些内核地址被称为内核逻辑地址)。固定偏移量为`PAGE_OFFSET`值(此处为`0xc000 0000`)。

所以，想想这个。在具有 3:1 虚拟机分割的 32 位系统上，物理地址`0x0` =内核逻辑地址`0xc000 0000` ( `PAGE_OFFSET`)。如前所述，术语*内核逻辑地址*适用于与物理地址有固定偏移量的内核地址。因此，直接映射内存映射到内核逻辑地址。直接映射内存的这个区域通常被称为内核段内的*低内存*(或简称为**低内存**)区域。

在图 7.10 中，我们已经展示了一个几乎相同的图。在下图中，它被稍微修改，实际上向您展示了内存的前三个(物理)页面帧如何映射到前三个内核虚拟页面(在内核段的 lowmem 区域中):

![](Images/392a747d-3601-4104-83b4-89903d4f557b.png)

Figure 7.21 – Direct-mapped RAM – lowmem region, on 32-bit with a 3:1 (GB) VM split

例如，图 7.21 显示了平台内存到 32 位系统内核段的直接映射，该系统具有 3:1 (GB)虚拟机分割。物理内存地址`0x0`映射到内核的点是`PAGE_OFFSET`内核宏(在上图中，是内核逻辑地址`0xc000 0000`)。注意图 7.21 如何在左侧显示*用户 VAS* ，范围从`0x0`到`PAGE_OFFSET-1`(大小为`TASK_SIZE`字节)。我们已经在前面的*检查内核段*一节中介绍了内核段剩余部分的细节。

理解这种物理页面到虚拟页面的映射可能会诱使您得出这些看似合乎逻辑的结论:

*   给定一个 KVA，要计算相应的**物理地址**(**PA**)–即执行 KVA 到 PA 的计算–只需执行以下操作:

```sh
pa = kva - PAGE_OFFSET
```

*   相反，给定一个功率放大器，要计算相应的 KVA，即执行功率放大器到 KVA 的计算，只需执行以下操作:

```sh
kva = pa + PAGE_OFFSET
```

请务必再次参考图 7.21。内存到内核段的直接映射(从`PAGE_OFFSET`开始)肯定预示了这个结论。所以，这是正确的。但是坚持住，请在这里仔细注意:**这些地址转换计算只适用于直接映射或线性地址**-换句话说，KVAs(技术上，内核逻辑地址)–**在内核的 lowmem 区域内，没有其他！**对于所有 UVA，以及除了之外的任何和所有 KVA*低 mem 区域(包括模块地址、`vmalloc` / `ioremap` (MMIO)地址、KASAN 地址、(可能的)高 mem 区域地址、DMA 存储区域等等)，它不*工作！**

正如您所料，内核确实提供了执行这些地址转换的 APIs 当然，它们的实现依赖于 arch。它们在这里:

| **内核 API** | **它做什么** |
| `phys_addr_t virt_to_phys(volatile void *address)` | 将给定的虚拟地址转换为其物理对应地址(返回值) |
| `void *phys_to_virt(phys_addr_t address)` | 将给定的物理地址转换为虚拟地址(返回值) |

x86 的`virt_to_phys()` API 上面有一条评论，明确鼓吹这个 API(及其同类)是**不被驱动作者**使用；为了清晰和完整，我们在内核源代码中复制了以下评论:

```sh
// arch/x86/include/asm/io.h
[...]
/**
 *  virt_to_phys    -   map virtual addresses to physical
 *  @address: address to remap
 *
 *  The returned physical address is the physical (CPU) mapping for
 *  the memory address given. It is only valid to use this function on
 *  addresses directly mapped or allocated via kmalloc.
 *
 *  This function does not give bus mappings for DMA transfers. In
 *  almost all conceivable cases a device driver should not be using
 *  this function
 */
static inline phys_addr_t virt_to_phys(volatile void *address)
[...]
```

前面的评论提到了(非常常见的)`kmalloc()` API。不用担心，下面两章会深入讨论。当然，对于`phys_to_virt()`应用编程接口也有类似的评论。

So who – sparingly – uses these address conversion APIs (and the like)? The kernel internal *mm* code, of course! As a demo, we do actually use them in at least a couple of places in this book: in the following chapter, in an LKM called `ch8/lowlevel_mem` (well actually, its usage is within a function in our "kernel library" code, `klib_llkd.c`).

FYI, the powerful `crash(8)` utility can indeed translate any given virtual address to a physical address via its `vtop` (virtual-to-physical) command (and vice versa, via its `ptov` command!).

接下来，另一个关键点:通过将所有物理内存映射到其中，不要被误导为内核正在为自己保留*内存。不，它不是；这只是*映射*所有可用的内存，从而使它可以分配给任何想要它的人——核心内核代码、内核线程、设备驱动程序或用户空间应用。这是操作系统工作的一部分；毕竟，它是系统资源管理器。当然，在启动时，内存的某一部分无疑会被静态内核代码、数据、内核页表等占用(分配)，但您应该意识到这是相当小的。例如，在我的具有 1 GB 内存的来宾虚拟机上，内核代码、数据和 BSS 通常总共占用大约 25 MB 的内存。所有内核内存都达到 100 兆左右，而用户空间内存使用量在 550 兆左右！内存占用者几乎总是用户空间。*

*You can try using the `smem(8)` utility with the `--system -p` option switches to see a summary of memory usage as percentages (also, use the `--realmem=` switch to pass the actual amount of RAM on the system).

回到正题:我们知道内核页表是在引导过程的早期建立的。因此，当应用启动时，*内核已经映射了所有内存并可用*，准备分配！因此，我们理解，虽然内核*将*页面帧直接映射到其 VAS 中，但用户模式进程并不那么幸运——它们只能通过操作系统设置的分页表(在进程创建时–`fork(2)`–时间)在每个进程的基础上间接映射页面帧。同样，有趣的是，通过强大的`mmap(2)`系统调用实现内存映射可以提供将文件或匿名页面“直接映射”到用户 VAS 中的错觉。

A few additional points to note:
(a) For performance, kernel memory (kernel pages) can *never be swapped*, even if they aren't in use

(b) Sometimes, you might think, it's quite obvious that *user space memory pages map to (physical) page frames (assuming the page is resident) via the paging tables set up by the OS on a per-process basis*. Yes, but what about kernel memory pages? Please be very clear on this point: *all kernel pages also map to page frames via the kernel "master" paging table. Kernel memory, too, is virtualized, just as user space memory is.* In this regard, for you, the interested reader, a QnA I initiated on Stack Overflow: *How exactly do kernel virtual addresses get translated to physical RAM?:* [http://stackoverflow.com/questions/36639607/how-exactly-do-kernel-virtual-addresses-get-translated-to-physical-ram](http://stackoverflow.com/questions/36639607/how-exactly-do-kernel-virtual-addresses-get-translated-to-physical-ram).(c) Several memory optimization techniques have been baked into the Linux kernel (well, many are configuration options); among them are **Transparent Huge Pages** (**THPs**)and, critical for cloud/virtualization workloads, **Kernel Samepage Merging** (**KSM**, aka memory de-duplication)*.* I refer you to the *Further reading* section of this chapter for more information.

好了，关于物理内存管理的一些方面的报道已经结束了，我们完成了这一章；优秀的进步！

# 摘要

在这一章中，我们相当深入地研究了内核内存管理这个大主题，其详细程度足以让像您这样的内核模块或设备驱动程序作者了解；还有，还会有更多！这个难题的一个关键部分——虚拟机拆分以及它是如何在运行 Linux 操作系统的各种体系结构上实现的——是一个起点。然后，我们深入研究了这种分裂的两个区域:首先是用户空间(进程 VAS)，然后是内核 VAS(或内核段)。在这里，我们介绍了关于如何检查它的许多细节和工具/实用程序(特别是通过非常强大的`procmap`实用程序)。我们构建了一个演示内核模块，可以生成一个相当完整的内核和调用过程的内存映射。还简要讨论了用户和内核内存布局随机化技术(ASLR)。我们通过研究 Linux 中内存的物理组织来结束这一章。

所有这些信息和本章所学的概念实际上*非常有用；*不仅仅是为了设计和编写更好的内核/设备驱动程序代码，也非常适合遇到系统级问题和 bug 的时候。

这一章很长，而且确实很关键；完成它的伟大工作！接下来，在接下来的两章中，您将继续学习如何准确有效地分配(和解除分配)内核内存的关键和实际方面，以及这一常见活动背后的相关重要概念。上，上！

# 问题

作为我们的总结，这里有一个问题列表，供您测试您对本章材料的知识:[https://github . com/packt publishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions)。你会在这本书的 GitHub repo 中找到一些问题的答案:[https://GitHub . com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions _ to _ assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn)。

# 进一步阅读

为了帮助您用有用的材料更深入地研究这个主题，我们在本书的 GitHub 存储库中的进一步阅读文档中提供了一个相当详细的在线参考资料和链接列表(有时甚至是书籍)。*进一步阅读*文档可在此处获得:[https://github . com/packt publishing/Linux-Kernel-Programming/blob/master/进一步阅读. md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md) 。*****************