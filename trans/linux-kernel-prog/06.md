# 六、内核内部原理——进程和线程

内核内部，尤其是那些关于内存管理的内部，是一个庞大而复杂的话题。在本书中，我们不打算深入探究内核和内存内部的血淋淋的细节。与此同时，我想为像您这样的初露头角的内核或设备驱动程序开发人员提供足够的、绝对必要的背景知识，以成功地解决理解内核体系结构所需的关键主题，如进程、线程及其堆栈是如何管理的。您还将能够正确有效地管理动态内核内存(重点是使用**可加载内核模块** ( **LKM** )框架编写内核或驱动程序代码)。作为一个附带的好处，有了这些知识，你会发现自己在调试用户和内核空间代码方面变得更加熟练。

我把关于基本内部的讨论分成了两章，这一章和下一章。本章涵盖了 Linux 内核内部架构的关键方面，特别是关于内核中进程和线程的管理。下一章将关注内存管理内部，这是理解和使用 Linux 内核的另一个重要方面。当然，现实是，所有这些事情并没有真正在一两章中涉及到，而是在本书中展开(例如，关于进程/线程的 CPU 调度的细节将在后面的章节中找到；类似地用于存储器内部、硬件中断、同步等)。

简而言之，这些是本章涵盖的主题:

*   理解进程和中断上下文
*   了解流程虚拟地址空间的基础知识
*   组织进程、线程及其堆栈——用户和内核空间
*   理解和访问内核任务结构
*   通过当前使用任务结构
*   迭代内核的任务列表

# 技术要求

我假设您已经完成了[第 1 章](01.html)、*内核工作区设置*，并适当准备了一个运行 Ubuntu 18.04 LTS(或更高版本的稳定版本)的来宾**虚拟机** ( **VM** )并安装了所有需要的软件包。如果没有，我建议你先做这个。

为了充分利用这本书，我强烈建议您首先设置工作区环境，包括克隆这本书的 GitHub 代码存储库(在这里可以找到:[https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming))并以动手的方式进行处理。

我确实假设您熟悉基本的虚拟内存概念、用户模式流程**虚拟地址空间** ( **VAS** )段布局、堆栈等等。尽管如此，我们还是花了几页来解释这些基础知识(在接下来的*理解过程 VAS* 部分的基础知识)。

# 理解进程和中断上下文

在[第 4 章](04.html)*编写您的第一个内核模块–LKMs，第 1 部分*中，我们展示了一个题为*内核架构 I* 的简短部分(如果您还没有阅读，我建议您在继续之前阅读)。我们现在将对此展开讨论。

理解大多数现代操作系统在设计上是单片的***是至关重要的。*单块*这个词字面意思是一块*单块大石头*。我们将推迟一会儿，看看这到底如何应用于我们最喜欢的操作系统！目前，我们把*单片*理解为这个意思:当一个进程或线程发出系统调用时，它会切换到(特权)内核模式并执行内核代码，可能还会处理内核数据。是的，没有内核或内核线程代表它执行代码；进程(或线程)*本身*执行内核代码。因此，我们说内核代码在用户空间进程或线程的上下文中执行——我们称之为**进程上下文** *。*想一想，内核的相当一部分正是这样执行的，包括很大一部分设备驱动的代码。***

 ***那么，你可能会问，既然你理解了这一点，那么除了进程上下文，内核代码还能如何执行呢？还有一种方法:当硬件中断(来自外围设备——键盘、网卡、磁盘等)触发时，中央处理器的控制单元保存当前上下文，并立即重新引导中央处理器运行中断处理程序的代码(中断服务例程**—**ISR**)。现在这段代码也在内核(特权)模式下运行——实际上，这是切换到内核模式的另一种异步方式！很多设备驱动的中断代码路径都是这样执行的；我们说以这种方式执行的内核代码是在**中断上下文** *中执行的。***

 **因此，任何一段内核代码都是由以下两种上下文之一输入并执行的:

*   **进程上下文**:从系统调用或处理器*异常*进入内核(如页面错误)，执行内核代码，处理内核数据；它是同步的(自上而下)。
*   **中断上下文**:从外围芯片的硬件中断进入内核，执行内核代码，处理内核数据；它是异步的(自下而上)。

*图 6.1* 展示概念视图:用户模式进程和线程在非特权用户上下文中执行；用户模式线程可以通过发出*系统调用*切换到特权内核模式。该图还向我们展示了纯*内核线程*也存在于 Linux 中；它们非常类似于用户模式线程，主要区别在于它们只在内核空间执行；他们甚至不能*看到*用户 VAS。通过系统调用(或处理器异常)同步切换到内核模式的任务现在在*进程上下文中运行内核代码。*(内核线程也在进程上下文中运行内核代码。)然而，硬件中断是另一回事——它们导致执行异步进入内核；它们执行的代码(通常是设备驱动程序的中断处理程序)在所谓的*中断上下文中运行。*

*图 6.1* 展示了更多细节——中断上下文上下半部分、内核线程和工作队列；我们请求您保持耐心，我们将在后面的章节中介绍所有这些以及更多内容:

![](Images/b4d19285-629a-4527-9844-56fbe6e7f89d.png)

Figure 6.1 – Conceptual diagram showing unprivileged user-mode execution and privileged kernel-mode execution with both process and interrupt contexts

在本书的后面，我们将向您展示如何准确地检查您的内核代码当前运行的上下文。继续读！

# 了解过程增值服务的基础

虚拟内存的一个基本“规则”是这样的:所有潜在的可寻址内存都在一个盒子里；也就是*沙盒*。我们认为这个“盒子”是*过程图像*或过程视觉模拟系统。不允许看盒子外面。

Here, we provide only a quick overview of the process user VAS. For details, please refer to the *Further reading* section at the end of this chapter.

用户视觉感知系统被划分为称为*段*的同质记忆区域，或者更确切地说，被划分为*映射。*每个 Linux 进程至少有这些映射(或段):

![](Images/1b865751-5cd4-44cd-a1c8-20b6c85652c6.png)

Figure 6.2 – Process VAS

让我们快速浏览一下这些段或映射的细分:

*   **文本段** *:* 这里是存放机器码的地方；静态(模式:`r-x`)。
*   **数据段** *:* 这是存储全局和静态数据变量的地方(模式:`rw-`)。它在内部分为三个不同的部分:
    *   **初始化数据段** *:* 这里存储预初始化变量；静态的。
    *   **未初始化的数据段**:未初始化的变量存储在这里(运行时自动初始化为`0`；这个地区有时被称为；静态的。
    *   **堆段** *:* 用于内存分配和释放的*库 API*(熟悉的`malloc(3)`系列例程)从这里获取内存。这也不完全正确。在现代系统中，只有`MMAP_THRESHOLD`下面的`malloc()`实例(默认为 128 KB)从堆中获取内存。再高一点，它就被分配为过程 VAS 中的一个单独的“映射”(通过强大的`mmap(2)`系统调用)。它是一个动态段(它可以增长/收缩)。堆中最后一个合法可引用的位置被称为*程序中断。*

*   **库(文本、数据)**:进程动态链接到的所有共享库都被映射(在运行时，通过加载器)到进程 VAS 中(模式:`r-x` / `rw-`)。
*   **堆栈** *:* 使用**后进先出** ( **后进先出**语义的内存区域；堆栈用于*实现高级语言的函数调用*机制。它包括参数传递、局部变量实例化(和销毁)以及返回值传播。这是一个动态的片段。在所有现代处理器(包括 x86 和 ARM 系列)上，*堆栈向更低的地址*增长(称为全递减堆栈)。每次调用一个函数，都会根据需要分配并初始化一个*栈帧*；堆栈框架的精确布局非常依赖于 CPU(您必须参考相应的 CPU **应用二进制接口** ( **ABI** )文档了解这一点；参考参见*进一步阅读*部分)。SP 寄存器(或等效寄存器)总是指向当前帧，即堆栈的顶部；随着堆栈向较低的(虚拟)地址发展，堆栈的顶部实际上是最低的(虚拟)地址！不直观但真实(模式:`rw-`)。

当然，你会明白进程必须包含至少一个*执行线程*(线程是进程内的一个执行路径)；一个线程通常是`main()`函数。在*图 6.2* 中，作为例子，我们展示了三个执行线程–`main`*、* `thrd2`和`thrd3` *。*同样，不出所料，除了堆栈的之外，每个线程都共享 VAS *中的所有内容；如您所知，每个线程都有自己的私有堆栈。`main`栈显示在流程(用户)VAS 的最顶端；`thrd2`和`thrd3`线程的堆栈被显示为在库映射和`main`的堆栈之间，并用两个(蓝色)方块示出。*

I have designed and implemented what I feel is a pretty useful learning/teaching and debugging utility called ***procmap*** ([https://github.com/kaiwan/procmap](https://github.com/kaiwan/procmap))*;* it's a console-based process VAS visualization utility. It can actually show you the process VAS (in quite a bit of detail); we shall commence using it in the next chapter. Don't let that stop you from trying it out right away though; do clone it and give it a spin on your Linux system.

现在您已经了解了过程 VAS 的基础知识，是时候深入研究一下关于过程 VAS、用户和内核地址空间以及它们的线程和堆栈的内核内部了。

# 组织进程、线程及其堆栈——用户和内核空间

传统的 **UNIX 流程模型**–*一切都是流程；如果这不是一个过程，那就是一个文件*——它有很多优点。这一事实充分证明了这一点:在过去近 50 年的时间里，操作系统仍然沿用 T4 的 T5 模式。当然，如今**螺纹** 很重要；*线程仅仅是进程*内的执行路径。线程*共享除堆栈外的所有*进程资源，包括用户 VAS、*。*每个线程都有自己的私有堆栈区域(这很有道理；如果没有，线程怎么可能真正并行运行，因为它是保存执行上下文的堆栈)。

我们关注*线程*而不是流程的另一个原因在[第 10 章](10.html)、*中央处理器调度器第 1 部分* *中有更清晰的说明。*现在，我们只能说:*线程，而不是进程，是内核可调度实体*(也称为 KSE)。这实际上是 Linux 操作系统架构的一个关键方面的附带结果。在 Linux 操作系统上，每个线程——包括内核线程——都映射到一个称为**任务结构**的内核元数据结构。任务结构(也称为*进程描述符*)本质上是内核用作属性结构的大型内核数据结构。对于每一个*线程*活着，内核都会维护一个对应的*任务结构*(见*图 6.3* ，不用担心，我们会在接下来的章节中详细介绍任务结构)。

下一个真正需要把握的关键点是:我们*要求 CPU 支持的每个特权级别每个线程一个堆栈。*在 Linux 等现代操作系统上，我们支持两种特权级别–*非特权用户模式(或用户空间)和特权内核模式(或内核空间)*。因此，在 Linux 上，*每个活跃的用户空间线程都有两个堆栈*:

*   **用户空间堆栈**:当线程执行用户模式代码路径时，该堆栈正在运行。
*   **一个内核空间堆栈**:当线程切换到内核模式(通过系统调用或处理器异常)并执行内核代码路径(在进程上下文中)时，这个堆栈正在运行。

Of course, every good rule has an exception: *kernel threads* are threads that live purely within the kernel and thus have a "view" of *only* kernel (virtual) address space; they cannot "see" userland. Hence, as they will only ever execute kernel space code paths, they have **just one stack** – a kernel space stack.

*图 6.3* 将地址空间分为两个——用户空间和内核空间。在图的上部——用户空间——你可以看到几个进程和它们的*用户花瓶。*在底部——内核空间——你可以看到，对应于每个用户模式线程，一个内核元数据结构(struct `task_struct`，我们稍后会详细介绍)和该线程的内核模式堆栈。此外，我们看到(最底部)三个内核线程(标记为`kthrd1` *、* `kthrd2`和`kthrdn`)；不出所料，它们也有一个表示内部(属性)的`task_struct`元数据结构和一个内核模式堆栈:

![](Images/a96a3955-729e-43ba-8916-29b393edb22f.png)

Figure 6.3 – Processes, threads, stacks, and task structures – user and kernel VAS

为了帮助实现这一讨论，让我们执行一个简单的 Bash 脚本(`ch6/countem.sh`)来计算当前活动的进程和线程的数量。我在我的原生 x86_64 Ubuntu 18.04 LTS 盒子上做到了这一点；请参见以下结果输出:

```sh
$ cd <booksrc>/ch6
$ ./countem.sh
System release info:
Distributor ID: Ubuntu
Description:    Ubuntu 18.04.4 LTS
Release:        18.04
Codename:       bionic

Total # of processes alive               =       362
Total # of threads alive                 =      1234
Total # of kernel threads alive          =       181
Thus, total # of user-mode threads alive =      1053
$ 
```

我让你在这里查找这个简单脚本的代码:`ch6/countem.sh`。研究前面的输出并理解它。当然，你会意识到这是某个时间点的情况的快照。它可以而且确实会改变。

在接下来的部分中，我们将讨论分成两部分(对应于两个地址空间)——我们在图 6.3 中看到的用户空间和在图 6.3 中看到的内核空间。让我们从用户空间组件开始。

## 用户空间组织

参考我们在前面章节中运行的`countem.sh` Bash 脚本，我们现在将对其进行分解并讨论一些关键点，目前仅限于 VAS 的*用户空间部分*。请仔细阅读并理解这一点(我们在以下讨论中提到的数字是指我们在前面部分中的`countem.sh`脚本的样本运行)。为了更好地理解，我在这里放置了图表的用户空间部分:

![](Images/6c434097-8366-4c01-bca2-7f14ca0b2b15.png)

Figure 6.4 – User space portion of overall picture seen in Figure 6.3

在这里(图 6.4)，您可以看到三个单独的过程。每个进程至少有一个执行线程(T7 线程)。在前面的例子中，我们展示了三个进程`P1`*`P2`和`Pn`，其中分别有一个、三个和两个线程，包括`main()`。从我们之前运行的`countem.sh`脚本示例来看，`Pn`应该有 *n* =362。*

*Do note that these diagrams are purely conceptual. In reality, the 'process' with PID 2 is typically a single-threaded kernel thread called `kthreadd`.

每个过程由几个部分组成(技术上，映射*)。*一般来说，用户模式段(映射)如下:

*   **文字**:代码；`r-x`
*   **数据段**:`rw-`；由三个不同的映射组成——初始化的数据段、未初始化的数据段(或`bss`)和“向上增长的”`heap`

*   **库映射**:对于每个共享库的文本和数据，流程动态链接到
*   **向下生长的堆栈**

关于这些堆栈，我们从前面的示例运行中看到，系统上目前有 1，053 个用户模式线程。这意味着也有 1，053 个用户空间堆栈，因为每个用户模式线程都有一个用户模式堆栈。在这些用户空间线程堆栈中，我们可以说:

*   对于`main()`线程，总是存在一个用户空间栈，它将位于用户视觉模拟系统的最顶端——高端；如果进程是单线程的(只有一个`main()`线程)，那么它将只有一个用户模式堆栈；*中的`P1`流程图 6.4* 显示了这种情况。
*   如果进程是多线程的，那么每个活线程(包括`main()`)将有一个用户模式线程栈；图 6.4 中的流程`P2`和`Pn`说明了这种情况。堆栈是在调用`fork(2)`(用于`main()`)或`pthread_create(3)`(用于进程内的剩余线程)时分配的，这导致该代码路径在内核内的进程上下文中执行:

```sh
sys_fork() --> do_fork() --> _do_fork()
```

*   仅供参考，Linux 上的`pthread_create(3)`库 API 调用(非常特定于 Linux 的)`clone(2)`系统调用；这个系统调用最终调用`_do_fork()`；传递的`clone_flags`参数通知内核如何创建“定制流程”；换句话说，一根线！
*   这些用户空间栈当然是动态的；它们可以增长/收缩到堆栈大小资源限制(`RLIMIT_STACK`，通常为 8mb；您可以使用`prlimit(1)`实用程序进行查找)。

已经看到并理解了用户空间部分，现在让我们深入研究一下内核空间方面的事情。

## 核心空间组织

参考我们在上一节中运行的`countem.sh` Bash 脚本继续我们的讨论，现在我们将分解它并讨论一些关键点，将我们自己限制在 VAS 的*内核空间部分*。请注意仔细阅读并理解这一点(在阅读我们之前运行的`countem.sh`脚本示例中输出的数字时)。为了更好地理解，我在这里放置了图表的内核空间部分(图 6.5):

![](Images/917a5b55-bae7-4af5-8a44-3b741bbfdb52.png)

Figure 6.5 – Kernel space portion of overall picture seen in Figure 6.3

同样，从前面的示例运行中，您可以看到系统上有 1，053 个用户模式线程和 181 个内核线程。这总共产生了 1，234 个内核空间堆栈。怎么做？如前所述，每个用户模式线程都有两个堆栈——一个用户模式堆栈和一个内核模式堆栈。因此，我们将为每个用户模式线程提供 1，053 个内核模式堆栈，加上(纯)内核线程的 181 个内核模式堆栈(回想一下，内核线程只有*个内核模式堆栈；他们根本看不到用户空间)。让我们列出内核模式堆栈的一些特征:*

 **   每个活动的应用(用户模式)线程将有一个内核模式堆栈，包括`main()`。

*   **内核模式堆栈*大小固定(静态)，相当小*** 。实际上，它们的大小在 32 位操作系统上为 2 页，在 64 位操作系统上为 4 页(一页的大小通常为 4 KB)。
*   它们是在线程创建时分配的(通常归结为`_do_fork()`)。

同样，让我们非常清楚这一点:每个用户模式线程都有两个堆栈——用户模式堆栈和内核模式堆栈。这个规则的例外是内核线程；它们只有一个内核模式堆栈(因为它们没有用户映射，因此没有用户空间“段”)。在*图 6.5* 的下部，我们显示了三个*内核线程–*`kthrd1`、`kthrd2`和`kthrdn`(在前面的示例运行中，`kthrdn`将具有 *n* =181)。此外，每个内核线程都有一个任务结构和一个在创建时分配给它的内核模式堆栈。

内核模式堆栈在大多数方面与用户模式堆栈相似——每次调用函数时，都会建立一个*堆栈框架*(框架布局是特定于体系结构的，构成了 CPU ABI 文档的一部分；有关这些详细信息，请参见*进一步阅读*部分；中央处理器有一个寄存器来跟踪堆栈的当前位置(通常称为**堆栈指针** ( **SP** ))，堆栈向*更低的*虚拟地址“增长”。但是，与动态用户模式堆栈不同，*内核模式堆栈的大小是固定的，而且很小。*

An important implication of the pretty small (two-page or four-page) kernel-mode stack size for the kernel / driver developer – be very careful to not overflow your kernel stack by performing stack-intensive work (such as recursion).

There exists a kernel configurable to warn you about high (kernel) stack usage at compile time; here's the text from the `lib/Kconfig.debug`file:
`CONFIG_FRAME_WARN:`
`Tell gcc to warn at build time for stack frames larger than this.`
`Setting this too low will cause a lot of warnings.`
`Setting it to 0 disables the warning.`
`Requires gcc 4.4`

### 总结现状

好的，太好了，现在让我们总结一下我们从前面的`countem.sh`脚本示例中的学习和发现:

*   **任务结构**:
    *   每个活着的线程(用户或内核)在内核中都有对应的任务结构(`struct task_struct`)；这是内核跟踪它的方式，它的所有属性都存储在这里(您将在*理解和访问内核任务结构*部分了解更多信息)
    *   关于我们的`ch6/countem.sh`脚本的样本运行:
        *   由于系统上总共有 1，234 个线程(包括用户和内核)，这意味着内核内存(在代码中为`struct task_struct`)中总共有 1，234 个*任务(元数据)结构*，我们可以这样说:
        *   这些任务结构中有 1，053 个代表用户线程。
        *   剩下的 181 个任务结构代表内核线程。
*   **堆叠**:
    *   每个用户空间线程都有两个堆栈:
        *   用户模式堆栈(当线程执行用户模式代码路径时运行)
        *   内核模式堆栈(当线程执行内核模式代码路径时运行)
    *   纯内核线程只有一个堆栈——内核模式堆栈
    *   关于我们的`ch6/countem.sh`脚本的样本运行:
        *   1，053 个用户空间堆栈(在用户土地上)。
        *   1，053 个内核空间堆栈(在内核内存中)。
        *   181 个内核空间堆栈(用于 181 个活动的内核线程)。
        *   总计 1053+1053+181 = 2287 叠！

在讨论用户和内核模式堆栈时，我们还应该简要提及这一点:许多架构(包括 x86 和 ARM64)支持单独的每 CPU 堆栈来处理*中断。*当外部硬件中断发生时，中央处理器的控制单元立即将控制重新定向到最终的中断处理代码(可能在设备驱动程序中)。单独的每 CPU 中断堆栈用于保存中断代码路径的堆栈帧；这有助于避免对中断的进程/线程的现有(小)内核模式堆栈施加太大压力。

好了，现在您已经从进程/线程及其堆栈的角度理解了用户和内核空间的整体组织，接下来让我们看看您如何实际“查看”内核和用户空间堆栈的内容。除了对学习有用之外，这些知识还能极大地帮助你调试环境。

## 查看用户和内核堆栈

*堆栈*通常是调试会话的关键。当然，堆栈保存了进程或线程的当前执行上下文(T2)，这让我们可以推断它在做什么。更重要的是，能够看到并解释线程的*调用堆栈(或调用链/回溯)*使我们能够理解我们是如何到达这里的。所有这些宝贵的信息都存在于堆栈中。但是等等，每个线程都有两个堆栈——用户空间和内核空间堆栈。我们如何看待它们？

在这里，我们将展示查看给定进程或线程的内核和用户模式堆栈的两种广泛方式，首先是通过“传统”方法，然后是更近的现代方法(通过[e]BPF)。一定要读下去。

### 查看堆栈的传统方法

让我们首先学习使用我们称之为“传统”的方法来查看给定进程或线程的内核和用户模式堆栈。让我们从内核模式堆栈开始。

#### 查看给定线程或进程的内核空间堆栈

好消息；这真的很容易。Linux 内核通过向用户空间公开内核内部的常用机制——强大的`proc` 文件系统接口，使堆栈可见。就在`/proc/<pid>/stack`下面偷看。

那么，好吧，让我们来看看我们的 *Bash* 进程的内核模式堆栈。假设在我们的 x86_64 Ubuntu 客户机(运行 5.4 内核)上，我们的 Bash 进程的 PID 为`3085`:

On modern kernels, to avoid *information leakage*, viewing the kernel-mode stack of a process or thread requires *root* access as a security requirement.

```sh
$ sudo cat /proc/3085/stack
[<0>] do_wait+0x1cb/0x230
[<0>] kernel_wait4+0x89/0x130
[<0>] __do_sys_wait4+0x95/0xa0
[<0>] __x64_sys_wait4+0x1e/0x20
[<0>] do_syscall_64+0x5a/0x120
[<0>] entry_SYSCALL_64_after_hwframe+0x44/0xa9
$ 
```

在前面的输出中，每一行代表堆栈上的一个*调用帧*。为了帮助解读内核堆栈回溯，有必要了解以下几点:

*   应该以自下而上的方式(从下往上)阅读。
*   每一行输出代表一个*调用帧*；实际上，是调用链中的一个函数。
*   出现为`??`的函数名意味着内核不能可靠地解释堆栈。忽略它，是内核说它是无效的栈帧(留下的‘blip’)；内核回溯代码通常是正确的！

*   在 Linux 上，任何`foo()`系统调用通常都会成为内核中的一个`SyS_foo()`函数。此外，`SyS_foo()`通常是一个调用“真实”代码`do_foo()`的包装器。一个细节:在内核代码中，你可能会看到`SYSCALL_DEFINEn(foo, ...)`类型的宏；宏成为`SyS_foo()`例程；附加的数字`n`在[0，6]范围内；它是系统调用从用户空间传递给内核的参数数量。

现在再看看前面的输出；应该很清楚:我们的 *Bash* 进程目前正在执行`do_wait()`功能；它是通过系统调用到达那里的，`wait4()`系统调用！这很对；shell 的工作方式是分出一个子进程，然后通过`wait4(2)`系统调用等待它的终止。

Curious readers (you!) should note that the `[<0>]` in the leftmost column of each stack frame displayed in the preceding snippet are the placeholders for the *text (code) address* of that function. Again, for *security* reasons (to prevent information leakage), it is zeroed out on modern kernels. (Another security measure related to the kernel and process layout is discussed in [Chapter 7](07.html), *Memory Management Internals – Essentials*,in the *Randomizing the memory layout – KASLR* and *User-mode ASLR* sections).

#### 查看给定线程或进程的用户空间堆栈

具有讽刺意味的是，查看进程或线程的*用户空间堆栈*在典型的 Linux 发行版上似乎更难做到(与查看内核模式堆栈相反，正如我们在上一节中刚刚看到的)。有一个实用工具可以做到这一点:`gstack(1)`。实际上，它只是一个简单的脚本包装器，在批处理模式下调用`gdb(1)`，让`gdb`调用其`backtrace`命令。

Unfortunately, on Ubuntu (18.04 LTS at least), there seems to be an issue; the `gstack` program was not found in any native package. (Ubuntu does have a `pstack(1)` utility, but, at least on my test VM, it failed to work well.) A workaround is to simply use `gdb` directly (you can always `attach <PID>` and issue the `[thread apply all] bt` command to view the user mode stack(s)).

不过，在我的 x86_64 Fedora 29 来宾系统上，`gstack(1)`实用程序安装干净，运行良好；一个例子如下(我们的 Bash 流程‘PID’这里正好是`12696`):

```sh
$ gstack 12696
#0 0x00007fa6f60754eb in waitpid () from /lib64/libc.so.6
#1 0x0000556f26c03629 in ?? ()
#2 0x0000556f26c04cc3 in wait_for ()
#3 0x0000556f26bf375c in execute_command_internal ()
#4 0x0000556f26bf39b6 in execute_command ()
#5 0x0000556f26bdb389 in reader_loop ()
#6 0x0000556f26bd9b69 in main ()
$ 
```

同样，每一行代表一个呼叫帧。自下而上阅读。显然， *Bash* 执行一个命令，最后调用`waitpid()`系统调用(实际上在现代 Linux 系统上，`waitpid()`只是实际`wait4(2)`系统调用的一个`glibc`包装器！再次，简单地忽略任何标记为`??`的呼叫帧。

Being able to peek into the kernel and user space stacks (as shown in the preceding snippets), and using utilities including `strace(1)` and `ltrace(1)` for tracing system and library calls of a process/thread respectively, can be a tremendous aid when debugging! Don't ignore them.

现在用“现代”的方法来解决这个问题。

### [e]BPF–查看两个堆栈的现代方法

现在——更加令人兴奋！–让我们学习(非常基础的)使用强大的现代方法，利用(在撰写本文时)非常新的技术–称为**扩展伯克利数据包过滤器**(**eBPF**；或者简单地说，BPF。我们在[第 1 章](01.html)、*内核工作空间设置*中的*附加有用项目*部分提到了【e】BPF 项目。)较老的 BPF 已经存在很长时间，并已用于网络数据包跟踪；[e]BPF 是最近的一项创新，仅从 4.x Linux 内核开始提供(这当然意味着您需要在 4.x 或更高版本的 Linux 系统上才能使用这种方法)。

直接使用底层的内核级 BPF 字节码技术是(极其)困难的；因此，好消息是这项技术有几个易于使用的前端(工具和脚本)。(显示当前 BCC 性能分析工具的图表可在[http://www . brendangregg . com/BPF/BCC _ tracing _ tools _ early 2019 . png](http://www.brendangregg.com/BPF/bcc_tracing_tools_early2019.png)上找到；可以在*[【http://www.brendangregg.com/ebpf.html#frontends】](http://www.brendangregg.com/ebpf.html#frontends)*找到[e]BPF 前线的列表；*这些链接来自*布伦丹·格雷格的*博客。)在前端中， **BCC** 和 **bpftrace** 被认为非常有用。在这里，我们将简单地使用名为`stackcount`的 BCC 工具提供一个快速演示(嗯，在 Ubuntu 上，至少它被命名为`stackcount-bpfcc(8)`)。另一个优点:使用这个工具可以同时看到内核和用户模式堆栈；不需要单独的工具。*

*You can install the BCC tools for your *host* Linux distro by reading the installation instructions here: [https://github.com/iovisor/bcc/blob/master/INSTALL.md](https://github.com/iovisor/bcc/blob/master/INSTALL.md). Why not on our guest Linux VM? You can, *when running a distro kernel* (such as an Ubuntu- or Fedora-supplied kernel). The reason: the installation of the BCC toolset includes the installation of the `linux-headers-$(uname -r)` package; the latter exists only for distro kernels (and not for our custom 5.4 kernel that we're running on the guest).

在下面的例子中，我们使用`stackcount` BCC 工具(在我的 x86_64 Ubuntu 18.04 LTS 主机系统上)来查找我们的 VirtualBox Fedora31 来宾进程的堆栈(虚拟机毕竟是主机系统上的一个进程！).对于这个工具，您必须指定一个(或多个)感兴趣的函数(有趣的是，您可以指定用户空间或内核空间函数，并且在这样做时还可以使用“通配符”或正则表达式！);只有当这些函数被调用时，堆栈才会被跟踪和报告。例如，我们选择任何包含名称`malloc`的函数:

```sh
$ sudo stackcount-bpfcc -p 29819 -r ".*malloc.*" -v -d
Tracing 73 functions for ".*malloc.*"... Hit Ctrl-C to end.
^C
 ffffffff99a56811 __kmalloc_reserve.isra.43
 ffffffff99a59436 alloc_skb_with_frags
 ffffffff99a51f72 sock_alloc_send_pskb
 ffffffff99b2e986 unix_stream_sendmsg
 ffffffff99a4d43e sock_sendmsg
 ffffffff99a4d4e3 sock_write_iter
 ffffffff9947f59a do_iter_readv_writev
 ffffffff99480cf6 do_iter_write
 ffffffff99480ed8 vfs_writev
 ffffffff99480fb8 do_writev
 ffffffff99482810 sys_writev
 ffffffff99203bb3 do_syscall_64
 ffffffff99c00081 entry_SYSCALL_64_after_hwframe
   --
 7fd0cc31b6e7     __GI___writev
 12bc             [unknown]
 600000195        [unknown]
 1
[...]
```

[e]BPF programs might fail due to the new *kernel lockdown* feature being merged into the mainline 5.4 kernel (it's disabled by default though). It's a **Linux Security Module** (**LSM**) that enables an extra 'hard' level of security on Linux systems. Of course, security is a double-edged sword; having a very secure system implicitly means that certain things will not work as expected, and this includes some [e]BPF programs. Do refer to the *Further reading* section for more on kernel lockdown.

通过的`-d`选项开关打印定界符`--`；它表示进程的内核模式和用户模式堆栈之间的边界。(不幸的是，由于大多数生产用户模式应用的符号信息将被剥离，大多数用户模式堆栈帧只是显示为“`[unknown]`”。)至少在这个系统上，内核堆栈框架是非常清晰的；甚至所讨论的文本(代码)函数的虚拟地址也被打印在左边。(为了帮助您更好地理解堆栈跟踪:首先，自底向上读取；接下来，如前所述，在 Linux 上，任何`foo()`系统调用通常都会成为内核中的`SyS_foo()`函数，并且通常`SyS_foo()`是实际工作函数`do_foo()`的包装器。)

注意`stackcount-bpfcc`工具只适用于 Linux 4.6+，需要 root 访问权限。有关详细信息，请参见其手册页。

作为第二个更简单的例子，我们编写一个简单的 *Hello，world* 程序(注意它处于无限循环中，这样我们就可以在底层`write(2)`系统调用发生时捕获它们)，在启用符号信息的情况下构建它(也就是说，`gcc -g ...`)，并使用一个简单的 Bash 脚本来执行与之前相同的工作:在执行时跟踪内核和用户模式堆栈。(你可以在`ch6/ebpf_stacktrace_eg/`找到代码。)显示示例运行的截图(好的，这里有一个例外:我已经在 x86_64 Ubuntu *20.04* LTS 主机上运行了该脚本)如下所示:

![](Images/cfaa69a7-c9a3-42e4-a281-8a6d6206af3b.png)

Figure 6.6 – A sample run using the stackcount-bpfcc BCC tool to trace both kernel and user-mode stacks for the write() of our Hello, world process We have merely scratched the surface here; [e]BPF tools such as BCC and `bpftrace` really are the modern, powerful approach to system, app tracing and performance analysis on the Linux OS. Do take the time to learn how to use these powerful tools! (Each BCC tool also has a dedicated man page *with examples*.) We refer you to the *Further reading* section for links on [e]BPF, BCC and `bpftrace`.

让我们通过缩小并查看到目前为止所学内容的概述来结束这一部分！

## 过程视觉模拟的 10，000 英尺视图

在我们结束这一部分之前，重要的是后退一步，看看每个过程的完整花瓶，以及它如何寻找系统的整体；换句话说，就是缩小并看到完整系统地址空间的“万英尺视图”。这就是我们试图用以下相当大而详细的图表(*图 6.7* )来做的，它是我们早期的*图 6.3* 的扩展或超集。

For those of you reading a hard copy of the book, I'd definitely recommend you view the book's figures in full color from this PDF document at [https://static.packt-cdn.com/downloads/9781789953435_ColorImages.pdf](_ColorImages.pdf).

除了您刚才了解和看到的内容——进程用户空间段、(用户和内核)线程以及内核模式堆栈——别忘了内核中还有很多其他元数据:任务结构、内核线程、内存描述符元数据结构等等。它们都是*内核 VAS 的一部分，*通常被称为*内核部分。*内核部分不仅仅是任务和堆栈。它还包含(显然！)静态内核(核心)代码和数据，实际上，内核的所有主要(和次要)*子系统*，特定于 arch 的代码，等等(我们在[第 4 章](04.html) *【编写您的第一个内核模块–LKMs 第 1 部分】*的*内核空间组件*一节中讨论过)。

如前所述，下图试图在一个地方总结和呈现所有(嗯，很多)这些信息:

![](Images/3963b6dd-76cd-4f72-baa0-1a7c043930aa.png)

Figure 6.7 – The 10,000-foot view of the processes, threads, stacks, and task structures of the user and kernel VASes

咻，真了不起，不是吗？上图内核部分的红色方框包含了*核心内核代码和数据*——主要的内核子系统，并显示了任务结构和内核模式堆栈。其余的被认为是非核心的东西；这包括设备驱动程序。(特定于 arch 的代码可以被视为核心代码；我们在这里单独展示。)还有，不要让前面的信息压倒你；只需关注我们现在在这里做的事情——进程、线程、它们的任务结构和堆栈。如果你仍然不清楚，一定要重读前面的材料。

现在，让我们继续实际理解和学习如何引用每个活动线程的关键或“根”元数据结构——任务结构*。*

 *# 理解和访问内核任务结构

正如您现在已经了解到的，Linux 内核中的每个用户和内核空间线程都由一个包含其所有属性的元数据结构内部表示，即**任务结构** *。*任务结构在内核代码中表示为`include/linux/sched.h:struct task_struct`。

不幸的是，它经常被称为“过程描述符”，导致混乱永无止境！谢天谢地，短语*任务结构*好多了；它代表了一个可运行的任务，实际上是一个*线程*。

所以我们有了:在 Linux 设计中，每个进程由一个或多个线程组成，每个线程映射到一个内核数据结构，称为任务结构 ( `struct task_struct` ) **。**

任务结构是线程的“根”元数据结构——它封装了操作系统为该线程所需的所有信息。这包括有关其内存(段、分页表、使用信息等)的信息、CPU 调度详细信息、其当前打开的任何文件、其凭证、功能位掩码、计时器、锁、**异步 I/O** ( **AIO** )上下文、硬件上下文、信令、IPC 对象、资源限制、(可选的)审计、安全和分析信息以及许多其他此类详细信息。

*图 6.8* 是 Linux 内核*任务结构*及其包含的大部分信息(元数据)的概念表示:

![](Images/2c9958c9-f71a-4111-be96-689ca5f8e3b8.png)

Figure 6.8 – Linux kernel task structure: struct task_struct

从*图 6.8* 中可以看出，任务结构保存了大量关于系统中每个活动任务(进程/线程)的信息(我再次重申:这也包括内核线程)。我们在图 6.8 中以划分的概念格式展示了封装在这个数据结构中的不同类型的属性。同样，正如可以看到的，某些属性将被`fork(2)`(或`pthread_create(3)`)上的子进程或线程*继承；某些属性不会被继承，只会被重置。(的内核模式堆栈*

至少现在，只要说内核“理解”任务是进程还是线程就足够了。我们稍后将演示一个内核模块(`ch6/foreach/thrd_showall`)，它确切地揭示了我们如何确定这一点(坚持住，我们会成功的！).

现在让我们开始更详细地了解庞大任务结构中一些更重要的成员；继续读！

Here, I only intend to give you a 'feel' for the kernel task structure; we do not delve deep into the details as it's not required for now. You will find that in later parts of this book, we delve into specific areas as required.

## 查看任务结构

首先，回想一下任务结构本质上是进程或线程的“根”数据结构——它保存任务的所有属性(正如我们前面看到的)。因此，它相当大；功能强大的`crash(8)`实用程序(用于分析 Linux 崩溃转储数据或调查实时系统)报告其在 x86_64 上的大小为 9，088 字节，`sizeof`运算符也是如此。

任务结构在`include/linux/sched.h`内核头中定义(这是一个相当关键的头)。在下面的代码中，我们显示了它的定义，但要注意的是，我们只显示了它的许多成员中的几个。(此外，`<< angle brackets like this >>`中的注释用于非常简要地解释成员):

```sh
// include/linux/sched.h
struct task_struct {
#ifdef CONFIG_THREAD_INFO_IN_TASK
    /*
     * For reasons of header soup (see current_thread_info()), this
     * must be the first element of task_struct.
     */
    struct thread_info      thread_info;   << important flags and status bits >>
#endif
    /* -1 unrunnable, 0 runnable, >0 stopped: */
    volatile long           state;
    [...]
    void                *stack; << the location of the kernel-mode stack >>
    [...]
    /* Current CPU: */
    unsigned int cpu;
    [...]
<< the members that follow are to do with CPU scheduling; some of them are discussed in Ch 9 & 10 on CPU Scheduling >>
    int on_rq;
    int prio;
    int static_prio;
    int normal_prio;
    unsigned int rt_priority;
    const struct sched_class *sched_class;
    struct sched_entity se;
    struct sched_rt_entity rt;
    [...]
```

继续下面代码块中的任务结构，查看与内存管理相关的成员`(mm)`、PID 和 TGID 值、凭证结构、打开的文件、信号处理等等。再说一遍，我无意详细研究(所有的)它们；在适当的地方，在本章的后面部分，也可能在本书的其他章节，我们将重温它们:

```sh
    [...]
    struct mm_struct *mm;      << memory management info >>
    struct mm_struct *active_mm;
    [...]
    pid_t pid;      << task PID and TGID values; explained below >>
    pid_t tgid;
    [...]
    /* Context switch counts: */
    unsigned long nvcsw;
    unsigned long nivcsw;
    [...]
    /* Effective (overridable) subjective task credentials (COW): */
    const struct cred __rcu *cred;
    [...]
    char comm[TASK_COMM_LEN];             << task name >>
    [...]
     /* Open file information: */
    struct files_struct *files;      << pointer to the 'open files' ds >>
    [...]
     /* Signal handlers: */
    struct signal_struct *signal;
    struct sighand_struct *sighand;
    sigset_t blocked;
    sigset_t real_blocked;
    [...]
#ifdef CONFIG_VMAP_STACK
    struct vm_struct *stack_vm_area;
#endif
    [...]
#ifdef CONFIG_SECURITY
    /* Used by LSM modules for access restriction: */
    void *security;
#endif
    [...]
    /* CPU-specific state of this task: */
    struct thread_struct thread;       << task hardware context detail >>
    [...]
};
```

Note that the `struct task_struct` members in the preceding code are shown with respect to the 5.4.0 kernel source; on other kernel versions, the members can and do change! Of course, it should go without saying, this is true of the entire book – all code/data is presented with regard to the 5.4.0 LTS Linux kernel (which will be maintained up to December 2025).

好了，现在你对任务结构中的成员有了更好的了解，你到底如何访问它和它的各种成员呢？继续读。

## 使用当前访问任务结构

您会记得，在前面的`countem.sh`脚本的示例运行中(在*组织进程、线程及其堆栈-用户和内核空间*部分)，我们发现系统上总共有 1，234 个线程(用户和内核)。这意味着内核内存中总共有 1，234 个任务结构对象。

它们需要以内核在需要时可以轻松访问的方式进行组织。因此，内核内存中的所有任务结构对象都被链接在一个名为**任务列表** *的*循环双向链表*上。*这种组织是各种内核代码路径迭代所必需的(通常是`procfs`代码)。即便如此，请思考一下:当一个进程或线程正在运行内核代码(在进程上下文中)时，它如何在内核内存中可能存在的成百上千个`task_struct`中找出哪个`task_struct`属于它？这是一项不平凡的任务。内核开发人员已经开发了一种方法来保证您可以找到代表当前运行内核代码的线程的特定任务结构。这是通过一个叫做`current`的宏实现的。这样想吧:

*   查找`current`得到当前正在运行内核代码的线程`task_struct`的指针，换句话说，*当前正在某个特定处理器内核上运行的进程上下文。*
*   `current`类似于(但当然不完全是)面向对象语言所说的`this`指针。

`current`宏的实现是非常特定于架构的。在这里，我们不深究血淋淋的细节。只要说实现被精心设计得很快就够了(通常通过一个*0(1)*算法)。例如，在一些具有许多通用寄存器的**精简指令集计算机** ( **RISC** )架构上(例如 PowerPC 和 Aarch64 处理器)，一个寄存器专用于保存`current`的值！

I urge you to browse the kernel source tree and see the implementation details of `current` (under `arch/<arch>/asm/current.h`). On the ARM32, an *O(1)* calculation yields the result; on AArch64 and PowerPC it's stored in a register (and thus the lookup is blazing fast). On x86_64 architectures, the implementation uses a `per-cpu` *variable* to hold `current` (avoiding the use of costly locking). Including the `<linux/sched.h>` header is required to include the definition of `current` in your code.

我们可以使用`current`去引用任务结构，并从中剔除信息；例如，进程(或线程)的 PID 和名称可以如下查找:

```sh
#include <linux/sched.h>
current->pid, current->comm
```

在下一节中，您将看到一个成熟的内核模块，它遍历任务列表，打印出沿途遇到的每个任务结构的一些细节。

## 确定上下文

如您现在所知，内核代码在两种环境之一中运行:

*   流程(或任务)上下文
*   中断(或原子)上下文

它们是互斥的——内核代码在任何给定的时间点运行在进程或原子/中断上下文中。

通常，在编写内核或驱动程序代码时，您必须首先弄清楚您正在处理的代码运行在什么上下文中。了解这一点的一种方法是使用以下宏:

```sh
#include <linux/preempt.h>
 in_task()
```

如果您的代码在进程(或任务)上下文中运行，它将返回一个布尔值:`True`，在这个上下文中，它通常是安全的；返回`False`意味着你处于某种原子或中断环境中，在这种环境中睡觉从来都不安全。

You might have come across the usage of the `in_interrupt()` macro; if it returns `True`, your code is within an interrupt context, if `False`, it isn't. However, the recommendation for modern code is to *not* rely on this macro (due to the fact that **Bottom Half** (**BH**) disabling can interfere with this). Hence, we recommend using `in_task()` instead.

坚持住。这可能会变得有点棘手:虽然`in_task()`返回`True`确实意味着您的代码在进程上下文中，但是这个事实本身并不能保证*当前*可以安全进入睡眠状态*。休眠实际上意味着调用调度程序代码和随后的上下文切换(我们在[第 10 章](10.html)、*CPU 调度程序–第 1 部分*和[第 11 章](11.html)、*CPU 调度程序–第 2 部分*中对此进行了详细介绍)。例如，您可能在进程上下文中，但是持有一个自旋锁(内核中使用的非常常见的锁)；锁定和解锁之间的代码——所谓的*临界区*——必须自动运行！这意味着，尽管您的代码可能在进程(或任务)上下文中，但如果它试图发出任何阻塞(休眠)API，它仍然会导致错误！*

另外，注意:`current`只有在*流程上下文*中运行时才被视为有效。

右；到目前为止，您已经了解了关于任务结构的有用背景信息，如何通过`current`宏访问它，以及这样做的注意事项——例如弄清楚您的内核或驱动程序代码当前运行的上下文。现在，让我们实际上写一些内核模块代码来检查一点内核任务结构。

# 通过当前
处理任务结构

在这里，我们将编写一个简单的内核模块来显示任务结构的一些成员，并揭示其*初始化*和*清理*代码路径运行的*进程上下文*。为此，我们设计了一个`show_ctx()`函数，使用`current`来访问任务结构的几个成员并显示它们的值。它从*初始化*和*清除*方法调用，如下所示:

For reasons of readability and space constraints, only key parts of the source code are displayed here. The entire source tree for this book is available in its GitHub repository; we expect you to clone and use it: `git clone https://github.com/PacktPublishing/Linux-Kernel-Programming.git`.

```sh
/* code: ch6/current_affairs/current_affairs.c */[ ... ]
#include <linux/sched.h>     /* current */
#include <linux/cred.h>      /* current_{e}{u,g}id() */
#include <linux/uidgid.h>    /* {from,make}_kuid() */
[...]
#define OURMODNAME    "current_affairs"
[ ... ]

static void show_ctx(char *nm)
{
    /* Extract the task UID and EUID using helper methods provided */
    unsigned int uid = from_kuid(&init_user_ns, current_uid());
    unsigned int euid = from_kuid(&init_user_ns, current_euid());

    pr_info("%s:%s():%d ", nm, __func__, __LINE__);
    if (likely(in_task())) {
                pr_info(
                "%s: in process context ::\n"
                " PID         : %6d\n"
                " TGID        : %6d\n"
                " UID         : %6u\n"
                " EUID        : %6u (%s root)\n"
                " name        : %s\n"
                " current (ptr to our process context's task_struct) :\n"
                "           0x%pK (0x%px)\n"
 " stack start : 0x%pK (0x%px)\n",
                nm, 
                /* always better to use the helper methods provided */
                task_pid_nr(current), task_tgid_nr(current), 
                /* ... rather than the 'usual' direct lookups:
                    current->pid, current->tgid, */
                uid, euid,
                (euid == 0?"have":"don't have"),
                current->comm,
                current, current,
 current->stack, current->stack);
    } else
      pr_alert("%s: in interrupt context [Should NOT Happen here!]\n", nm);
}
```

正如在前面的代码片段中以粗体突出显示的，您可以看到(对于一些成员)我们可以简单地取消引用`current`指针来访问各种`task_struct`成员并显示它们(通过内核日志缓冲区)。

太好了。前面的代码片段确实向您展示了如何通过`current`直接访问一些`task_struct`成员；然而，并不是所有的成员都可以或者应该被直接访问。相反，内核提供了一些辅助方法来访问它们；接下来让我们进入这个话题。

## 内置内核助手方法和优化

在前面的代码中，我们使用了一些内核的*内置助手方法*来提取任务结构的各种成员。这是推荐的方法；比如我们用`task_pid_nr()`来偷看 PID 成员，而不是直接通过`current->pid`。类似地，任务结构中的流程凭证(如我们在前面的代码中显示的`EUID`成员)在`struct cred`中被抽象，对它们的访问是通过助手例程提供的，就像我们在前面的代码中使用的`from_kuid()`一样。以类似的方式，还有其他几个助手方法；在`include/linux/sched.h`中查找它们，就在`struct task_struct`定义的下方。

Why is this the case? Why not simply access task structure members directly via `current-><member-name>`? Well, there are various real reasons; one, perhaps the access requires a *lock* to be taken (we cover details on the key topic of locking and synchronization in the last two chapters of this book). Two, perhaps there's a more optimal way to access them; read on to see more on this...

此外，如前面的代码所示，通过使用`in_task()`宏，我们可以很容易地找出(我们的内核模块的)内核代码是在进程还是中断上下文中运行——如果在进程(或任务)上下文中，它将返回`True`，否则将返回`False`。

有趣的是，我们还使用`likely()`宏(它变成了编译器`__built-in_expect`属性)来提示编译器的分支预测设置，并优化送入 CPU 流水线的指令序列，从而使我们的代码保持在“快速路径”上(更多关于这种使用`likely()/unlikely()`宏的微优化可以在本章的*进一步阅读*一节中找到)。在开发人员分别“知道”代码路径可能或不可能的情况下，您会看到内核代码经常使用`likely()/unlikely()`宏。

The preceding `[un]likely()` macros are a good example of micro-optimization, of how the Linux kernel leverages the `gcc(1)` compiler. In fact, until recently, the Linux kernel could *only* be compiled with `gcc`; recently, patches are slowly making compilation with `clang(1)` a reality. (FYI, the modern **Android Open Source Project** (**AOSP**) is compiled with `clang`.)

好了，现在我们已经理解了内核模块`show_ctx()`函数的工作原理，让我们来试一试。

## 试用内核模块打印进程上下文信息

我们构建我们的`current_affair.ko`内核模块(这里不显示构建输出)，然后像往常一样将其插入内核空间(通过`insmod(8)`)。现在我们用`dmesg(1)`查看内核日志，然后`rmmod(8)`查看并再次使用`dmesg(1)`。下面的截图显示了这一点:

![](Images/62a6a96a-c2b9-4727-a8e0-8cfaae620ae7.png)

Figure 6.9 – The output of the current_affairs.ko kernel module

很明显，从前面的截图可以看出，*进程上下文*——运行`current_affairs.ko:current_affairs_init()`内核代码的进程(或线程)——就是`insmod`进程(见输出:“`name : insmod`”)，执行清理代码的`current_affairs.ko:current_affairs_exit()`进程上下文就是`rmmod`进程！

Notice how the timestamps in the left column (`[sec.usec]`) in the preceding figure help us understand that `rmmod` was called close to 11 seconds after `insmod`.

这个小的演示内核模块比第一眼看到的要多。这对理解 Linux 内核架构其实很有帮助。下一节将解释这是如何发生的。

### 鉴于 Linux 操作系统是单片的

除了使用`current`宏的练习之外，这个内核模块(`ch6/current_affairs`)背后的一个关键点是清楚地向您展示 Linux 操作系统的*单片特性。*在前面的代码中，我们看到当我们在内核模块文件(`current_affairs.ko`)上执行`insmod(8)`过程时，它被插入到内核中，并且它的 *init* 代码路径运行；*谁跑的？*啊，这个问题是通过检查输出来回答的:`insmod`进程本身在进程上下文中运行它，从而证明了 Linux 内核的整体性！(同上`rmmod(8)`流程和*清理*代码路径；它由流程上下文中的`rmmod`流程运行。)

Note carefully and clearly: there is no "kernel" (or kernel thread) that executes the code of the kernel module, it's the user space process (or thread) *itself* that, by issuing system calls (recall that both the `insmod(8)` and `rmmod(8)` utilities issue system calls), switches into kernel space and executes the code of the kernel module. This is how it is with a monolithic kernel.

当然，这种类型的内核代码执行是我们所说的在进程上下文中运行*，而不是在*中断上下文*中运行。然而，Linux 内核并不被认为是纯粹的整体；如果是这样，它将是一个单一的硬编码的内存。相反，像所有现代操作系统一样，Linux 支持*模块化*(通过 LKM 框架)。*

As an aside, do note that you can create and run *kernel threads* within kernel space; they still execute kernel code in process context when scheduled.

### 使用 printk 进行安全编码

在我们之前的内核模块演示(`ch6/current_affairs/current_affairs.c`)中，我希望您注意到了带有“特殊”`%pK`格式说明符的`printk`的用法。我们在这里重复相关的代码片段:

```sh
 pr_info(
 [...]
     " current (ptr to our process context's task_struct) :\n"
     " 0x%pK (0x%px)\n"
     " stack start : 0x%pK (0x%px)\n",
     [...]
     current, (long unsigned)current,
     current->stack, (long unsigned)current->stack); [...]
```

回想一下我们在[第 5 章](05.html)、*编写您的第一个内核模块–LKMs 第 2 部分*中的讨论，在*影响系统日志的 Proc 文件系统可调参数*部分，当打印地址时(首先，您真的不应该在生产中打印地址)，我敦促您不要使用通常的`%p`(或`%px`)而是使用 **`%pK`** 格式说明符。这就是我们在前面的代码中所做的；*这是为了安全*、*防止内核信息泄露*。对于一个经过良好调整的(安全)系统，`%pK`将只显示散列值，而不显示实际地址。为了显示这一点，我们还通过`0x%px`格式说明符显示实际的内核地址，只是为了进行对比。

有趣的是，`%pK`似乎对默认的桌面 Ubuntu 18.04 LTS 系统没有影响。两种格式`%pK`和`0x%px`打印出相同的值(如图 6.9 所示)；这是*而不是*所期待的。然而，在我的 x86_64 Fedora 31 虚拟机上，它确实如预期的那样工作，用`%pK`产生一个散列(不正确的)值，用`0x%px`产生正确的内核地址。以下是我的 Fedora 31 虚拟机的相关输出:

```sh
$ sudo insmod ./current_affairs.ko
[...]
$ dmesg
[...]
name : insmod
 current (ptr to our process context's task_struct) :
 0x0000000049ee4bd2 (0xffff9bd6770fa700)
 stack start : 0x00000000c3f1cd84 (0xffffb42280c68000)
[...]
```

在前面的输出中，我们可以清楚地看到区别。

On production systems (embedded or otherwise) be safe: set `kernel.kptr_restrict` to `1` (or even better, to `2`), thus sanitizing pointers, and set `kernel.dmesg_restrict` to `1` (allowing only privileged users to read the kernel log).

现在，让我们继续进行一些更有趣的事情:在下一节中，您将学习如何迭代 Linux 内核的*任务列表*，从而实际上学习如何获取系统上每个活动进程和/或线程的内核级信息。

# 迭代内核的任务列表

如前所述，所有的任务结构都被组织在内核内存中一个叫做*任务列表*的链表中(允许它们被迭代)。列表数据结构已经演变成非常常用的*循环双链表。*事实上，处理这些列表的核心内核代码已经被分解成一个名为`list.h`的头部；众所周知，它有望用于任何基于列表的工作。

The `include/linux/types.h:list_head` data structure forms the essential doubly linked circular list; as expected, it consists of two pointers, one to the `prev` member on the list and one to the `next` member.

通过版本> = 4.11 的`include/linux/sched/signal.h`头文件中方便提供的宏，您可以轻松地遍历与任务相关的各种列表；请注意，对于 4.10 及更早版本的内核，宏位于`include/linux/sched.h`中。

现在，让我们把这个讨论变成经验和实践。在下面的部分中，我们将编写内核模块，以两种方式迭代内核任务列表:

*   **一**:遍历内核任务列表，活显示所有*进程*。
*   **二**:迭代内核任务列表，显示所有*线程*活动。

我们展示了后一种情况的详细代码视图。继续读下去，一定要自己去尝试！

## 迭代任务列表 I–显示所有进程

内核提供了一个方便的例程，即`for_each_process()`宏，它可以让你轻松地遍历任务列表中的每个*进程*:

```sh
// include/linux/sched/signal.h:
#define for_each_process(p) \
    for (p = &init_task ; (p = next_task(p)) != &init_task ; )
```

很明显，宏扩展到一个`for`循环，允许我们在循环列表中循环。`init_task`是一个方便的“头”或开始指针——它指向第一个用户空间进程的任务结构，传统上是`init(1)`，现在是`systemd(1)`。

Note that the `for_each_process()` macro is expressly designed to only iterate over the `main()` thread of every *process* and not the ('child' or peer) threads.

这里显示了我们的`ch6/foreach/prcs_showall`内核模块输出的一个简短片段(当在我们的 x86_64 Ubuntu 18.04 LTS 客户系统上运行时):

```sh
$ cd ch6/foreach/prcs_showall; ../../../lkm prcs_showall
 [...]
 [ 111.657574] prcs_showall: inserted
 [ 111.658820]      Name       |  TGID  |  PID  |  RUID  |  EUID 
 [ 111.659619] systemd         |       1|      1|       0|       0
 [ 111.660330] kthreadd        |       2|      2|       0|       0
 [...]
 [ 111.778937] kworker/0:5     |    1123|   1123|       0|       0
 [ 111.779833] lkm             |    1143|   1143|    1000|    1000
 [ 111.780835] sudo            |    1536|   1536|       0|       0
 [ 111.781819] insmod          |    1537|   1537|       0|       0
```

Notice how, in the preceding snippet, the TGID and PID of each process are always equal, 'proving' that the `for_each_process()` macro only iterates over the *main* thread of every process (and not every thread). We explain the details in the following section.

我们将把对示例内核模块的学习和尝试留在`ch6/foreach/prcs_showall`作为对您的练习。

## 迭代任务列表二–显示所有线程

为了迭代系统中每个活跃的*线程*，我们可以使用`do_each_thread() { ... } while_each_thread()` *对*宏；我们编写了一个示例内核模块来实现这一点(这里:`ch6/foreach/thrd_showall/`)。

在进入代码之前，让我们构建它，`insmod`它(在我们的 x86_64 Ubuntu 18.04 LTS 来宾上)，并查看它通过`dmesg(1)`发出的输出的底部。由于这里不可能显示完整的输出——它太大了——我在下面的截图中只显示了输出的下部。此外，我们还复制了标题(图 6.9)，这样您就可以理解每一列代表什么:

![](Images/70009eb8-3e62-4016-b4f7-0256ce270f2f.png)

Figure 6.10 – Output from our thrd_showall.ko kernel module In Figure 6.9, notice how all the (kernel-mode) stack start addresses (the fifth column) end in zeroes:
`0xffff .... .... .000`, implying that the stack region is *always aligned on a page boundary* (as `0x1000` is `4096` in decimal). This will be the case as kernel-mode stacks are always fixed in size and a multiple of the system page size (typically 4 KB).

按照惯例，在我们的内核模块中，我们安排如果线程是一个*内核线程*，它的名字显示在方括号中。

在继续代码之前，我们首先需要详细检查任务结构中的 TGID 和 PID 成员。

### 区分过程和线程-TGID 和 PID

想想看:由于 Linux 内核使用唯一的任务结构(`struct task_struct`)来表示每个线程，并且由于其内部的唯一成员有一个 PID，这意味着，在 Linux 内核中，*每个线程都有一个唯一的 PID* 。这就产生了一个问题:同一个进程的多个线程如何共享一个公共的 PID？这违反了 POSIX.1b 标准(*pthreads*；事实上，有一段时间，Linux 不符合标准，造成了移植等问题)。

为了解决这个恼人的用户空间标准问题，Ingo Molnar(红帽)在 2.5 内核系列中提出并引入了一个补丁。一个名为**线程组标识符**或 TGID 的新成员被放入任务结构中。工作原理是这样的:如果进程是单线程的，`tgid`和`pid`的值是相等的。如果是多线程进程，那么*主*线程的`tgid`值等于其`pid`值；该流程的其他线程将继承*主*线程的`tgid`值，但将保留自己独特的`pid`值。

为了更好地理解这一点，让我们从前面的截图中举一个实际的例子。在图 6.9 中，请注意，如果一个正整数出现在右边的最后一列，它是如何表示紧挨着它左边的多线程进程中的线程数量的。

那么，查看图 6.9 中的`VBoxService`流程；为了方便起见，我们将该片段复制如下(注意:我们删除了第一列`dmesg`时间戳，并添加了标题行，以提高可读性):它具有表示其*主*线程(称为`VBoxService`)的`938`的 PID 和 TGID 值；为了清楚起见，我们用粗体显示了它)，总共有*个线程*:

```sh
 PID  TGID        current            stack-start     Thread Name  MT?#
 938   938   0xffff9b09e99edb00  0xffffbaffc0b0c000  VBoxService   9
 938   940   0xffff9b09e98496c0  0xffffbaffc0b14000     RTThrdPP
 938   941   0xffff9b09fc30c440  0xffffbaffc0ad4000      control
 938   942   0xffff9b09fcc596c0  0xffffbaffc0a8c000     timesync
 938   943   0xffff9b09fcc5ad80  0xffffbaffc0b1c000       vminfo
 938   944   0xffff9b09e99e4440  0xffffbaffc0b24000   cpuhotplug
 938   945   0xffff9b09e99e16c0  0xffffbaffc0b2c000   memballoon
 938   946   0xffff9b09b65fad80  0xffffbaffc0b34000      vmstats
 938   947   0xffff9b09b6ae2d80  0xffffbaffc0b3c000    automount
```

九根线是什么？首先当然是*主*线程是`VBoxService`，其下显示的八个，按名称分别是:`RTThrdPP`、`control`、`timesync`、`vminfo`、`cpuhotplug`、`memballoon`、`vmstats`和`automount`。我们如何确定这一点？很简单:仔细看前面代码块中分别表示 TGID 和 PID 的第一列和第二列:如果它们相同，则是流程的主线；*如果 TGID 重复，则该过程是多线程的*并且 PID 值代表“子”线程的唯一 ID。

事实上，通过无处不在的 GNU `ps(1)`命令，通过使用其`-LA`选项(以及其他方式)，完全有可能在用户空间中看到内核的 TGID/PID 表示:

```sh
$ ps -LA
    PID   LWP  TTY          TIME  CMD
      1     1  ?        00:00:02  systemd
      2     2  ?        00:00:00  kthreadd
      3     3  ?        00:00:00  rcu_gp
[...]
    938   938  ?        00:00:00  VBoxService
    938   940  ?        00:00:00  RTThrdPP
    938   941  ?        00:00:00  control
    938   942  ?        00:00:00  timesync
    938   943  ?        00:00:03  vminfo
    938   944  ?        00:00:00  cpuhotplug
    938   945  ?        00:00:00  memballoon
    938   946  ?        00:00:00  vmstats
    938   947  ?        00:00:00  automount
 [...]
```

`ps(1)`标签如下:

*   第一列是`PID`-这实际上代表了这个任务在内核中的任务结构的`tgid`成员
*   第二列是`LWP`(轻量进程或线程！)–这实际上代表了该任务内核中任务结构的`pid`成员。

Note that only with the `ps(1)` GNU can you pass parameters (like `-LA`) and see the threads; this isn't possible with a lightweight implementation of `ps` like that of *busybox*. It isn't a problem though: you can always look up the same by looking under procfs; in this example, under `/proc/938/task`, you'll see sub-folders representing the child threads. Guess what: this is actually how GNU `ps` works as well!

好了，现在进入代码...

## 迭代任务列表三–代码

现在让我们看看`thrd_showall`内核模块的(相关)代码:

```sh
// ch6/foreach/thrd_showall/thrd_showall.c */
[...]
#include <linux/sched.h>     /* current */
#include <linux/version.h>
#if LINUX_VERSION_CODE > KERNEL_VERSION(4, 10, 0)
#include <linux/sched/signal.h>
#endif
[...]

static int showthrds(void)
{
    struct task_struct *g, *t;      // 'g' : process ptr; 't': thread ptr
    [...]
#if 0
    /* the tasklist_lock reader-writer spinlock for the task list 'should'
     * be used here, but, it's not exported, hence unavailable to our 
     * kernel module */
    read_lock(&tasklist_lock);
#endif
    disp_idle_thread();
```

关于前面的代码，需要注意几点:

*   我们使用`LINUX_VERSION_CODE()`宏根据需要有条件地包含一个标题。
*   请暂时忽略*锁定*的工作——使用(或不使用)`tasklist_lock()`和`task_[un]lock()`原料药。
*   别忘了 CPU 空闲线程！每个中央处理器内核都有一个专用的空闲线程(名为`swapper/n`)，当没有其他线程想要运行时(从`0`开始，`n`是内核号)。我们运行的`do .. while`循环并不是从这个线程开始的(T4 也没有显示出来)。我们包括一个小例程来显示它，利用了空闲线程的硬编码任务结构在`init_task`可用并导出的事实(一个细节:`init_task`总是指第一个 CPU 的–核心#`0`–空闲线程)。

让我们继续:为了遍历每个活跃的线程，我们需要使用一对*宏，形成一个循环:`do_each_thread() { ... } while_each_thread()`对宏正是这样做的，允许我们遍历系统上每个活跃的*线程*。以下代码显示了这一点:* 

```sh
    do_each_thread(g, t) {
        task_lock(t);
        snprintf(buf, BUFMAX-1, "%6d %6d ", g->tgid, t->pid);

        /* task_struct addr and kernel-mode stack addr */
        snprintf(tmp, TMPMAX-1, " 0x%px", t);
        strncat(buf, tmp, TMPMAX);
        snprintf(tmp, TMPMAX-1, " 0x%px", t->stack);
        strncat(buf, tmp, TMPMAX);

        [...] *<< see notes below >>*

        total++;
        memset(buf, 0, sizeof(buf));       *<< cleanup >>*
        memset(tmp, 0, sizeof(tmp));
        task_unlock(t);
     } while_each_thread(g, t); #if 0
   /* <same as above, reg the reader-writer spinlock for the task list> */
   read_unlock(&tasklist_lock);
#endif
    return total;
}
```

参考前面的代码，`do_each_thread() { ... } while_each_thread()`对宏形成了一个循环，允许我们迭代系统上的每个*线程*:

*   我们遵循一种策略，使用一个临时变量(名为`tmp`)来获取一个数据项，然后将它附加到一个“结果”缓冲区`buf`，我们在每次循环迭代中打印一次。
*   获取`TGID`、`PID`、`task_struct`和`stack`起始地址很简单——这里，为了简单起见，我们只使用`current`来取消引用它们(当然，您也可以使用我们在本章前面看到的更复杂的内核助手方法；在这里，我们希望保持简单)。还要注意的是，这里我们故意使用*而不是*使用(更安全的)`%pK` printk 格式说明符，而是通用的`%px`说明符，以便显示任务结构和内核模式堆栈的*实际*内核虚拟地址。
*   在循环之前，根据需要进行清理(将线程总数的计数器从`memset()`增加到`NULL`，以此类推)。
*   完成后，我们返回迭代过的线程总数。

在下面的代码块中，我们覆盖了前面代码块中故意遗漏的部分。我们检索线程的名称，如果它是一个内核线程，则将其打印在方括号内。我们还查询进程中的线程数量。解释遵循代码:

```sh
        if (!g->mm) {    // kernel thread
        /* One might question why we don't use the get_task_comm() to
         * obtain the task's name here; the short reason: it causes a
         * deadlock! We shall explore this (and how to avoid it) in
         * some detail in the chapters on Synchronization. For now, we
         * just do it the simple way ...
         */
            snprintf(tmp, TMPMAX-1, " [%16s]", t->comm);
        } else {
            snprintf(tmp, TMPMAX-1, "  %16s ", t->comm);
        }
        strncat(buf, tmp, TMPMAX);

        /* Is this the "main" thread of a multithreaded process?
         * We check by seeing if (a) it's a user space thread,
         * (b) its TGID == its PID, and (c), there are >1 threads in
         * the process.
         * If so, display the number of threads in the overall process
         * to the right..
         */
        nr_thrds = get_nr_threads(g);
        if (g->mm && (g->tgid == t->pid) && (nr_thrds > 1)) {
            snprintf(tmp, TMPMAX-1, " %3d", nr_thrds);
            strncat(buf, tmp, TMPMAX);
        }
```

在前面的代码中，我们可以说:

*   一个*内核线程*没有用户空间映射。`main()`线程的`current->mm`是指向类型为`mm_struct`的结构的指针，代表整个进程的*用户空间*映射；如果`NULL`，很有理由认为这是一个内核线程(因为内核线程没有用户空间映射)；我们相应地检查并打印名称。
*   我们也打印线程的名称(通过查找任务结构的`comm`成员)。你可能会质疑为什么我们不在这里使用`get_task_comm()`例程获取任务名称；简短的理由:它导致了一个*僵局*！我们将在后面关于内核同步的章节中详细探讨这个问题(以及如何避免这个问题)。现在，我们还是用简单的方法。
*   我们通过`get_nr_threads()`宏方便地获取给定进程中的线程数；其余的在前面的代码块中的宏上方的代码注释中有清楚的解释。

太好了。至此，我们完成了对 Linux 内核内部和架构的讨论(目前)，主要集中在进程、线程及其堆栈上。

# 摘要

在本章中，我们介绍了内核内部的关键方面，这些方面将帮助您作为内核模块或设备驱动程序作者更好、更深入地理解操作系统的内部工作。您详细检查了进程及其线程和堆栈(在用户和内核空间中)的组织和关系。我们检查了内核`task_struct`数据结构，并学习了如何通过内核模块以不同的方式迭代*任务列表*。

尽管这可能并不明显，但事实是，理解这些内核内部细节是成为一名经验丰富的内核(和/或设备驱动程序)开发人员的必要步骤。本章的内容将帮助您调试许多系统编程场景，并为我们深入探索 Linux 内核，尤其是内存管理奠定基础。

下一章和接下来的几章确实至关重要:我们将介绍您需要了解的关于内存管理内部的深刻而复杂的主题。我建议你先消化本章内容，浏览感兴趣的进一步阅读链接，做练习(*问题*部分)，然后，进入下一章！

# 问题

作为我们的总结，这里有一个问题列表，供您测试您对本章材料的知识:[https://github . com/packt publishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions)。你会在这本书的 GitHub repo 中找到一些问题的答案:[https://GitHub . com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions _ to _ assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn)。

# 进一步阅读

为了帮助您用有用的材料更深入地研究这个主题，我们在本书的 GitHub 存储库中的进一步阅读文档中提供了一个相当详细的在线参考资料和链接列表(有时甚至是书籍)。*进一步阅读*文档可在此处获得:[https://github . com/packt publishing/Linux-Kernel-Programming/blob/master/进一步阅读. md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md) 。**********