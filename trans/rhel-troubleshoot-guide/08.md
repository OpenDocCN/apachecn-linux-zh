# 八、硬件故障排除

在上一章中，我们确定了 NFS 上的文件系统被挂载为**Read-Only**。 为了确定原因，我们对 NFS 和文件系统进行了大量的故障排除。 我们使用`showmount`等命令查看可用的 NFS 共享，使用`mount`命令显示已挂载的文件系统。

一旦我们确定了问题，我们就可以使用`fsck`命令来执行文件系统检查并恢复文件系统。

在本章中，我们将继续从[第 7 章](07.html#19UOO2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 7. Filesystem Errors and Recovery")、*文件系统错误和恢复*出发，调查一个硬件设备故障。 本章将涵盖许多必要的日志文件和工具，这些文件和工具不仅用于确定是否发生了硬件故障，还用于确定发生故障的原因。

# 从一个日志条目开始

在[第 7 章](07.html#19UOO2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 7. Filesystem Errors and Recovery")，*文件系统错误和恢复*中，当查看`/var/log/messages`日志文件来识别 NFS 服务器文件系统的问题时，我们注意到以下消息:

```sh
Apr 26 10:25:44 nfs kernel: md/raid1:md127: Disk failure on sdb1, disabling device.
md/raid1:md127: Operation continuing on 1 devices.
Apr 26 10:25:55 nfs kernel: md: unbind<sdb1>
Apr 26 10:25:55 nfs kernel: md: export_rdev(sdb1)
Apr 26 10:27:20 nfs kernel: md: bind<sdb1>
Apr 26 10:27:20 nfs kernel: md: recovery of RAID array md127
Apr 26 10:27:20 nfs kernel: md: minimum _guaranteed_  speed: 1000 KB/sec/disk.
Apr 26 10:27:20 nfs kernel: md: using maximum available idle IO bandwidth (but not more than 200000 KB/sec) for recovery.
Apr 26 10:27:20 nfs kernel: md: using 128k window, over a total of 511936k.
Apr 26 10:27:20 nfs kernel: md: md127: recovery done.

```

以上提示信息表示 RAID 设备`/dev/md127`出现故障。 由于前一章只关注文件系统本身的问题，所以我们没有进一步研究 RAID 设备故障。 在本章中，我们将调查以确定原因和解决办法。

为了开始调查，我们应该首先查看原始日志消息，因为这些消息可以告诉我们关于 RAID 设备的状态的相当多信息。

首先，让我们将这些消息分成如下小部分:

```sh
Apr 26 10:25:44 nfs kernel: md/raid1:md127: Disk failure on sdb1, disabling device.
md/raid1:md127: Operation continuing on 1 devices.

```

第一个日志消息实际上很能说明问题。 显示的第一个关键信息是关于`(md/raid1:md127)`的 RAID 设备。

通过这个装置的名字，我们已经知道了很多。 我们知道的第一件事是，这个 RAID 设备是由 Linux 的软件 RAID 系统**多设备驱动程序**(**md**)创建的。 这个系统允许 Linux 取两个独立的磁盘，并对它们应用一个 RAID。

由于我们将在本章主要与 RAID 打交道，我们首先应该理解什么是 RAID 以及它是如何工作的。

# 什么是 RAID?

**独立磁盘冗余阵列**(**RAID**)通常是基于软件或硬件的系统，它允许用户将多个磁盘作为一个设备使用。 RAID 可以通过多种方式配置，从而实现更大的数据冗余和性能。

这种配置通常被称为 RAID 级别。 不同类型的 RAID 级别提供不同的功能，可以更好地了解 RAID 级别。 让我们来探索一些常用的方法。

## RAID 0 -分条

RAID 0 是最容易理解的 RAID 级别之一。 RAID 0 的工作方式是将多个磁盘组合成一个。 当数据写入 RAID 设备时，数据会被分裂，并写入各个硬盘。 为了更好地理解这一点，让我们构建一个简单的场景。

*   如果我们有一个由 5 个 500gb 驱动器组成的简单 RAID 0 设备，我们的 RAID 设备将是所有 5 个驱动器的大小——2500 GB 或 2.5 TB。 如果我们要向 RAID 设备写入一个 50 MB 的文件，那么将有 10 MB 的文件数据同时写入每个磁盘。

这个过程通常称为**分条**。 在相同的上下文中，当从 RAID 设备读取 50mb 的文件时，每个磁盘也将同时处理读取请求。

将一个文件拆分并同时将其部分处理到每个磁盘的能力为写或读请求提供了更好的性能。 事实上，因为我们有 5 个磁盘，所以请求的速度要快 5 的倍数。

一个简单的类比是，如果你让五个人以相同的速度建造一堵墙，他们的速度将是一个人建造同样一堵墙的速度的五倍。

虽然 RAID 0 提供了性能，但它不提供任何数据保护。 如果该 RAID 中的单个硬盘故障，则该硬盘的数据不可用，可能导致 RAID 0 的数据丢失。

## RAID 1 -镜像

RAID 1 是另一个简单 RAID 级别。 与 RAID 0 中的驱动器组合不同，RAID 1 中的驱动器是镜像的。 RAID 1 通常由两个或多个硬盘组成。 当数据写入 RAID 设备时，数据会完整地写入每个设备。

这个过程被称为**镜像**，因为数据基本上是在所有驱动器上镜像的:

*   使用与前面相同的场景，如果在 RAID 1 配置中有 5 个 500gb 磁盘驱动器，那么总磁盘大小将是 500gb。 当我们将相同的 50 MB 文件写入 RAID 设备时，每个驱动器将得到这个 50 MB 文件的自己的副本。
*   这也意味着写请求将只与 RAID 中最慢的驱动器一样快。 对于 RAID 1，每个磁盘都必须完成写请求才能被认为完成。
*   然而，读请求可以由任何一个 RAID 1 驱动器提供。 因此，RAID 1 有时可以更快地提供读请求，因为每个请求可以由 RAID 中的不同驱动器执行。

RAID 1 提供最高级别的数据弹性，因为它只需要一个磁盘驱动器在故障期间保持活动。 使用我们的 5 个磁盘场景，我们可以丢失 5 个磁盘中的 4 个，但仍然可以重建并使用 RAID。 这就是为什么当数据保护比硬盘性能更重要时，应该使用 RAID 1。

## RAID 5 -分条分布式奇偶校验

**RAID 5**是难以理解的 RAID 级别的一个例子。 RAID 5 的工作原理是将数据分条到多个硬盘(例如 RAID 0)，但它也包括奇偶校验。 奇偶校验数据是对写入 RAID 设备的数据进行排他或操作而产生的一种特殊数据。 生成的数据可用于从另一个驱动器重新生成丢失的数据。

*   使用我们之前做过的相同的例子，我们在 RAID 5 配置中有 5 个 500gb 的硬盘，如果我们再次写入一个 50mb 的文件，每个磁盘将接收 10mb 的数据; 这和 RAID 0 完全一样。 然而，与 RAID 0 不同的是，奇偶校验数据也会写入每个磁盘。 由于有额外的奇偶数据，RAID 可用的总数据大小是四个驱动器的总和，其中一个驱动器的数据值分配给奇偶。 在我们的情况下，这意味着 2tb 可用磁盘空间，500 GB 用于奇偶校验。

通常，存在一种误解，认为奇偶校验数据被写入具有 RAID 5 的专用驱动器。 但事实并非如此。 奇偶校验数据的大小相当于一个完整磁盘的空间大小。 然而，这些数据分布在所有磁盘上。

使用 RAID 5 而不是 RAID 0 的一个原因是，如果单个驱动器故障，可以重建数据。 RAID 5 的唯一问题是如果两个硬盘故障，RAID 无法重建，可能会导致数据丢失。

## RAID 6 -分条，双分布式奇偶校验

**RAID 6**本质上是与 RAID 5 相同类型的 RAID; 但是，校验数据增加了一倍。 通过将校验数据加倍，RAID 最多可以支持 2 次硬盘故障。 由于奇偶校验是双倍的，如果我们将 5 个 500gb 的硬盘驱动器放入 RAID 6 配置中，可用磁盘空间将是 1.5 TB，即 3 个驱动器的总和; 另外 1tb 的数据空间将被两组校验数据占用。

## RAID 10 -镜像和条纹

**RAID 10**(通常称为 RAID 1 + 0)是另一个非常常见的 RAID 级别。 RAID 10 本质上是 RAID 1 和 RAID 0 的结合。 在 RAID 10 中，每个磁盘都有一个镜像，数据在所有镜像驱动器上条带化。 为了解释这一点，我们将使用一个类似于上面的例子; 然而，我们将用 6 个 500 GB 的驱动器来做这件事。

*   如果我们要写一个 30 MB 的文件，它将被分成 10 MB 的块，并分条到三个 RAID 设备上。 这些 RAID 设备为 RAID 1 镜像。 本质上，RAID 10 是在 RAID 0 配置中排列在一起的许多 RAID 1 设备。

配置 RAID 10 可以很好地平衡性能和数据保护。 为了使一个完全的失败发生，一个镜像的两面都必须失败; 这意味着 RAID 1 的两个方面。

考虑到 RAID 中的磁盘数量，这种情况发生的可能性比 RAID 5 要小。 从性能的角度来看，RAID 10 仍然受益于分条方法，并且能够通过提高写速度将单个文件的不同块写入每个磁盘。

RAID 10 还得益于拥有两个具有相同数据的硬盘; 与 RAID 1 一样，当发出读请求时，任何一个磁盘都可以处理该请求，允许每个磁盘独立处理并发读请求。

RAID 10 的缺点是，虽然它通常可以满足或超过 RAID 5 的性能，但通常需要更多的硬件来实现这一点，因为每个磁盘都是镜像的，您会将总磁盘空间的一半损失给 RAID。

在前面的示例中，RAID 10 配置中 6 个 500gb 驱动器的可用空间为 1.5 TB。 简单地说，它是磁盘容量的 50%。 对于 RAID 5 来说，4 个硬盘的容量是相同的。

# 回到 RAID 故障诊断

现在我们对 RAID 和不同配置有了更好的理解，让我们回过头来研究我们的错误。

```sh
Apr 26 10:25:44 nfs kernel: md/raid1:md127: Disk failure on sdb1, disabling device.
md/raid1:md127: Operation continuing on 1 devices.

```

从前面的错误中，我们可以看到我们的 RAID 设备是**md127**。 我们还可以看到该设备是 raid1 设备(`md/raid1`)。 消息提示*操作继续在 1 个设备上*，意味着镜像的第二部分仍在运行。

好的方面是，如果镜像的两边都不可用，那么 RAID 将完全失败，并导致更糟糕的问题。

由于我们现在知道了受影响的 RAID 设备、所使用的 RAID 类型，甚至故障的硬盘，因此我们有相当多的关于该故障的信息。 如果我们继续查看`/var/log/messages`的日志条目，我们可以发现更多:

```sh
Apr 26 10:25:55 nfs kernel: md: unbind<sdb1>
Apr 26 10:25:55 nfs kernel: md: export_rdev(sdb1)
Apr 26 10:27:20 nfs kernel: md: bind<sdb1>
Apr 26 10:27:20 nfs kernel: md: recovery of RAID array md127
Apr 26 10:27:20 nfs kernel: md: minimum _guaranteed_  speed: 1000 KB/sec/disk.

```

上面的消息很有趣，因为它们表明 MD Linux 软件 RAID 服务试图恢复 RAID:

```sh
Apr 26 10:25:55 nfs kernel: md: unbind<sdb1>

```

在这部分日志的第一行中，似乎设备`sdb1`已经从 RAID 中移除:

```sh
Apr 26 10:27:20 nfs kernel: md: bind<sdb1>

```

然而，第三行表示设备`sdb1`已经重新添加到RAID 或“**绑定**”到 RAID。

第四行和第五行显示 RAID 开始恢复步骤:

```sh
Apr 26 10:27:20 nfs kernel: md: recovery of RAID array md127
Apr 26 10:27:20 nfs kernel: md: minimum _guaranteed_  speed: 1000 KB/sec/disk.

```

## RAID 恢复原理

前面我们讨论了各种 RAID 级别如何从丢失的设备中重建和恢复数据。 这可以通过校验数据或镜像数据来实现。

当 RAID 设备失去其中一个驱动器，并且该驱动器被替换或重新添加到 RAID 时，RAID 管理器(无论是软件 RAID 还是硬件 RAID)将开始重建数据。 此重建的目标是重新创建丢失驱动器上的数据。

如果该 RAID 组为镜像 RAID 组，则会将可用镜像磁盘上的数据读写到替换后的磁盘上。

对于基于校验的 RAID，重建将基于在 RAID 中分割的幸存数据和 RAID 中的校验数据。

在基于对等的 raid 的重建过程中，任何额外的故障都可能导致重建失败。 对于基于镜像的 raid，只要有一个用于重建的数据的完整副本，就可以在任何磁盘上发生故障。

在捕获的日志消息的末尾，我们可以看到重建是成功的:

```sh
Apr 26 10:27:20 nfs kernel: md: md127: recovery done.

```

根据在前一章中找到的日志消息的结尾，RAID`device /dev/md127`似乎是健康的。

## 检查当前 RAID 状态

虽然`/var/log/messages`是查看服务器上发生了什么事情的很好的方法，但这并不一定意味着这些日志消息对于 RAID 的当前状态是准确的。

为了查看 RAID 设备的当前状态，我们可以运行一些命令。

我们将使用的第一个命令是`mdadm`命令:

```sh
[nfs]# mdadm --detail /dev/md127
/dev/md127:
 Version : 1.0
 Creation Time : Wed Apr 15 09:39:22 2015
 Raid Level : raid1
 Array Size : 511936 (500.02 MiB 524.22 MB)
 Used Dev Size : 511936 (500.02 MiB 524.22 MB)
 Raid Devices : 2
 Total Devices : 1
 Persistence : Superblock is persistent

 Intent Bitmap : Internal

 Update Time : Sun May 10 06:16:10 2015
 State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 0
 Spare Devices : 0

 Name : localhost:boot
 UUID : 7adf0323:b0962394:387e6cd0:b2914469
 Events : 52

 Number   Major   Minor   RaidDevice State
 0       8        1        0      active sync   /dev/sda1
 2       0        0        2      removed

```

`mdadm`命令用于管理 Linux MD 型 raid。 在前面的命令中，我们指定了标志`--detail`，后面跟着一个 RAID 设备。 这告诉`mdadm`打印指定 RAID 设备的详细信息。

`mdadm`命令可以执行的不仅仅是打印状态; 它还可以用于执行 RAID 活动，如创建、销毁或修改 RAID 设备。

为了理解`--detail`标志的输出，让我们将上面的输出分解如下:

```sh
/dev/md127:
 Version : 1.0
 Creation Time : Wed Apr 15 09:39:22 2015
 Raid Level : raid1
 Array Size : 511936 (500.02 MiB 524.22 MB)
 Used Dev Size : 511936 (500.02 MiB 524.22 MB)
 Raid Devices : 2
 Total Devices : 1
 Persistence : Superblock is persistent

```

第一部分告诉我们相当多关于 RAID 本身的信息。 要注意的重要项目是`Creation Time`，在本例中是上午 9:39 的`Wed April 15th` 这告诉我们 RAID 是何时首次创建的。

还注意到`Raid Level`，正如我们在`/var/log/messages`中看到的，它是 RAID 1。 我们还可以看到`Array Size`，它告诉我们 RAID 设备将提供的总可用磁盘空间(524 MB)以及在该 RAID 阵列中使用的`Raid Devices`的数量，在本例中是两个设备。

组成该 RAID 的设备数量非常重要，因为它可以帮助我们理解该 RAID 的状态。

由于我们的 RAID 总共由两个设备组成，如果其中任何一个设备故障，我们知道，如果剩余的磁盘丢失，我们的 RAID 将面临完全故障的风险。 但是，如果我们的 RAID 由三个设备组成，我们就会知道即使丢失两个磁盘也不会导致完全的 RAID 故障。

仅仅从`mdadm`命令的前半部分，我们就可以看到关于这个 RAID 的相当多的信息。 从下半年开始，我们将发现更多的关键信息，如下:

```sh
 Intent Bitmap : Internal

 Update Time : Sun May 10 06:16:10 2015
 State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 0
 Spare Devices : 0

 Name : localhost:boot
 UUID : 7adf0323:b0962394:387e6cd0:b2914469
 Events : 52

 Number   Major   Minor   RaidDevice State
 0       8        1        0      active sync   /dev/sda1
 2       0        0        2      removed

```

`Update Time`很有用，因为它显示了该 RAID 最近一次更改状态的时间，无论该状态更改是添加磁盘还是重新构建。

这个时间戳可能很有用，特别是当我们试图将它与`/var/log/messages`中的日志条目或其他系统事件关联起来时。

另一个关键信息是`RAID Device State`，在我们的例子中，它是干净的、退化的。 降级状态意味着当 RAID 的设备失效时，RAID 本身仍然有效。 降级只是指功能正常但不理想。

如果我们的 RAID 设备现在正在积极地重建或恢复，我们也会看到列出的那些状态。

在当前状态输出下，我们可以看到四个设备类别，它们告诉我们用于该 RAID 的硬盘。 第一个是`Active Devices`; 它告诉我们 RAID 中当前活动的驱动器数量。

二是`Working Devices`; 这告诉我们工作驱动器的数量。 通常，`Working Devices`和`Active Devices`的数量是一样的。

第四项是`Failed Devices`; 这是当前标记为失败的设备数量。 即使我们的 RAID 当前有一个失败的设备，这个数字是`0`。 这有一个合理的原因，但我们将在后面介绍这个原因。

我们列表中的最后一项是`Spare Devices`的数量。 在某些 RAID 系统中，可以创建备用设备，用于在驱动器故障等事件中重建 RAID。

这些备用设备可以派上用场，因为 RAID 系统通常会自动重建 RAID，这减少了 RAID 完全失败的可能性。

通过`mdadm`输出的最后两行，我们可以看到组成 RAID 的驱动器的信息:

```sh
 Number   Major   Minor   RaidDevice State
 0       8        1        0      active sync   /dev/sda1
 2       0        0        2      removed

```

从输出中，我们可以看到有一个磁盘设备`/dev/sda1`当前处于活动的同步状态。 我们还可以看到另一个设备已经从 RAID 中删除。

### 总结关键信息

从`mdadm --detail`的输出可以看出`/dev/md127`是一个 RAID 级别为 1 的 RAID 设备，当前处于降级状态。 我们可以从细节中看到降级状态是由于组成 RAID 的一个驱动器当前被移除的事实。

## 通过/proc/mdstat 查看 md 状态

寻找MD 当前状态的另一个有用的地方是`/proc/mdstat`; 这个文件，像`/proc`中的许多文件一样，由内核不断更新。 如果我们使用`cat`命令读取该文件，我们可以快速查看服务器当前的 RAID 状态:

```sh
[nfs]# cat /proc/mdstat 
Personalities : [raid1] 
md126 : active raid1 sda2[0]
 7871488 blocks super 1.2 [2/1] [U_]
 bitmap: 1/1 pages [4KB], 65536KB chunk

md127 : active raid1 sda1[0]
 511936 blocks super 1.0 [2/1] [U_]
 bitmap: 1/1 pages [4KB], 65536KB chunk

unused devices: <none>

```

`/proc/mdstat`的内容有些神秘，但如果我们将其分解，它包含了相当多的信息。

```sh
Personalities : [raid1]

```

第一行`Personalities`告诉我们这个系统上的内核当前支持哪些 RAID 级别。 在我们的例子中，它是 raid1:

```sh
md126 : active raid1 sda2[0]
 7871488 blocks super 1.2 [2/1] [U_]
 bitmap: 1/1 pages [4KB], 65536KB chunk

```

下一组行是`/dev/md126`的当前状态，这是系统上的另一个 RAID 设备，我们还没有查看它。 这三行实际上可以给我们很多关于`md126`的信息; 事实上，它们给我们的信息与`mdadm --detail`告诉我们的差不多。

```sh
md126 : active raid1 sda2[0]

```

在第一行中，我们可以看到设备名称`md126`。 我们可以看到 RAID 的当前状态，它是活动的。 我们还可以看到 RAID 设备 RAID 1 的 RAID 级别。 最后，我们还可以看到组成这个 RAID 的磁盘设备; 在我们的例子中，它只有`sda2`。

第二行还包含如下关键信息:

```sh
 7871488 blocks super 1.2 [2/1] [U_]

```

具体来说，最后两个值对我们当前的任务最有用，`[2/1]`显示有多少磁盘设备分配给这个 RAID，以及有多少磁盘设备可用。 从示例中的值可以看到，预期有 2 个驱动器，但只有 1 个驱动器可用。

最后一个值`[U_]`显示组成该 RAID的驱动器的当前状态。 状态 U 代表向上，“`_`”代表向下。

在我们的示例中，我们可以看到一个磁盘设备打开，另一个关闭。

根据上述信息，我们能够确定 RAID 设备`/dev/md126`当前处于活动状态; 它正在使用 RAID 级别 1，目前有两个硬盘之一不可用。

如果我们继续查看`/proc/mdstat`文件，我们可以看到`md127`的类似状态。

### 同时使用/proc/mdstat 和 mdadm

经过`/proc/mdstat`和`mdadm --detail`，我们可以看到两者提供了相似的信息。 从的经验来看，我发现使用`mdstat`和`mdadm`都是有用的。 `/proc/mdstat`文件通常是我去一个快捷简单的快照的 RAID 设备系统,而`mdadm`命令通常是我用更深的 RAID 设备细节(如备用驱动器的数量细节,创建时间和最后更新时间)。

# 发现更大的问题

早些时候，当使用`mdadm`查看`md127`的当前状态时，我们可以看到 RAID 设备`md127`从服务中删除了一个磁盘。 在查看`/proc/mdstat`时，我们发现还有另一个 RAID 设备`/dev/md126`，它也从服务中删除了一个磁盘。

我们可以看到的另一个有趣的项目是 RAID 设备`/dev/md126`是一个幸存的磁盘:`/dev/sda1`。 这很有趣，因为`/dev/md127`存活的磁盘是`/dev/sda2`。 如果我们还记得前面的章节`/dev/sda1`和`/dev/sda2`只是来自同一个物理磁盘的两个分区。 假设两个 RAID 设备都有一个丢失的驱动器，并且我们的日志表明`/dev/md127`删除了`/dev/sdb1`并重新添加。 可能`/dev/md127`和`/dev/md126`都在使用来自`/dev/sdb`的分区。

由于`/proc/mdstat`对于 RAID 设备只有两种状态，up 和 down，我们可以使用`--detail`标志来确认第二块硬盘是否已经从`/dev/md126`中实际移除:

```sh
[nfs]# mdadm --detail /dev/md126
/dev/md126:
 Version : 1.2
 Creation Time : Wed Apr 15 09:39:19 2015
 Raid Level : raid1
 Array Size : 7871488 (7.51 GiB 8.06 GB)
 Used Dev Size : 7871488 (7.51 GiB 8.06 GB)
 Raid Devices : 2
 Total Devices : 1
 Persistence : Superblock is persistent

 Intent Bitmap : Internal

 Update Time : Mon May 11 04:03:09 2015
 State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 0
 Spare Devices : 0

 Name : localhost:pv00
 UUID : bec13d99:42674929:76663813:f748e7cb
 Events : 5481

 Number   Major   Minor   RaidDevice State
 0       8        2        0      active sync   /dev/sda2
 2       0        0        2      removed

```

从输出中，我们可以看到`/dev/md126`的当前状态和配置与`/dev/md127`完全相同。 有了这些信息，我们可以假设`/dev/md126`曾经有`/dev/sdb2`作为其 RAID 的一部分。

由于我们怀疑问题可能仅仅是单个硬盘驱动器有问题，我们需要验证是否确实如此。 第一步是确定是否真的存在`/dev/sdb`设备; 最快的方法是使用`ls`命令在`/dev`中执行目录列表:

```sh
[nfs]# ls -la /dev/ | grep sd
brw-rw----.  1 root disk      8,   0 May 10 06:16 sda
brw-rw----.  1 root disk      8,   1 May 10 06:16 sda1
brw-rw----.  1 root disk      8,   2 May 10 06:16 sda2
brw-rw----.  1 root disk      8,  16 May 10 06:16 sdb
brw-rw----.  1 root disk      8,  17 May 10 06:16 sdb1
brw-rw----.  1 root disk      8,  18 May 10 06:16 sdb2

```

我们可以从这个`ls`命令的结果中看到，实际上有一个`sdb`、`sdb1`和`sdb2`设备。 在进一步深入之前，让我们对`/dev`有一个更清晰的了解。

# 理解/开发

目录`/dev`是一个特殊的目录，内核在安装时在其中创建内容。 此目录包含允许用户或应用与物理设备(有时是逻辑设备)交互的特殊文件。

如果我们查看前面的`ls`命令的结果，我们可以看到在`/dev`目录中有几个以`sd`开头的文件。

在前一章中，我们了解到以`sd`开头的文件实际上被视为 SCSI 或 SATA 驱动器。 在我们的例子中，我们有`/dev/sda`和`/dev/sdb`; 这意味着，在这个系统上，有两个物理 SCSI 或 SATA 驱动器。

额外的设备`/dev/sda1`、`/dev/sda2`、`/dev/sdb1`和`/dev/sdb2`只是这些磁盘的分区。 实际上，对于磁盘驱动器，以数字值结尾的设备名称通常是另一个设备的分区，就像`/dev/sdb1`是`/dev/sdb`的分区一样。 当然，这一规则也有一些例外，但在对磁盘驱动器进行故障诊断时，这样做通常是安全的。

## 不仅仅是磁盘驱动器

`/dev/`目录包含的远不止磁盘驱动器。 如果我们看`/dev/`，我们实际上可以看到相当多的常见设备。

```sh
[nfs]# ls -F /dev
autofs           hugepages/       network_throughput  snd/     tty21  tty4   tty58    vcs1
block/           initctl|         null                sr0      tty22  tty40  tty59    vcs2
bsg/             input/           nvram               stderr@  tty23  tty41  tty6     vcs3
btrfs-control    kmsg             oldmem              stdin@   tty24  tty42  tty60    vcs4
bus/             log=             port                stdout@  tty25  tty43  tty61    vcs5
cdrom@           loop-control     ppp                 tty      tty26  tty44  tty62    vcs6
char/            lp0              ptmx                tty0     tty27  tty45  tty63    vcsa
console          lp1              pts/                tty1     tty28  tty46  tty7     vcsa1
core@            lp2              random              tty10    tty29  tty47  tty8     vcsa2
cpu/             lp3              raw/                tty11    tty3   tty48  tty9     vcsa3
cpu_dma_latency  mapper/          rtc@                tty12    tty30  tty49  ttyS0    vcsa4
crash            mcelog           rtc0                tty13    tty31  tty5   ttyS1    vcsa5
disk/            md/              sda                 tty14    tty32  tty50  ttyS2    vcsa6
dm-0             md0/             sda1                tty15    tty33  tty51  ttyS3    vfio/
dm-1             md126            sda2                tty16    tty34  tty52  uhid     vga_arbiter
dm-2             md127            sdb                 tty17    tty35  tty53  uinput   vhost-net
fd@              mem              sdb1                tty18    tty36  tty54  urandom  zero
full             mqueue/          sdb2                tty19    tty37  tty55  usbmon0
fuse             net/             shm/                tty2     tty38  tty56  usbmon1
hpet             network_latency  snapshot            tty20    tty39  tty57  vcs

```

从这个`ls`的结果中，我们可以看到在`/dev`目录中有许多文件、目录和符号链接。

下面列出了一些常见的设备或目录，对了解和理解它们很有用:

*   **/dev/cdrom**:这通常是一个到`cdrom`设备的符号链接。 CD-ROM 的实际设备遵循类似于硬盘的命名约定，它以`sr`开始，然后是设备的编号。 我们可以看到`/dev/cdrom`符号链接与`ls`命令的位置:

    ```sh
    [nfs]# ls -la /dev/cdrom
    lrwxrwxrwx. 1 root root 3 May 10 06:16 /dev/cdrom -> sr0

    ```

*   **/dev/console**:该设备不一定链接到特定的硬件设备，如`/dev/sda`或`/dev/sr0`。 控制台设备用于与系统控制台交互，系统控制台可能是实际的监视器，也可能不是。
*   **/dev/cpu**:这实际上是一个目录，其中包含系统上每个 CPU 的附加目录。 在这些目录中有一个`cpuid`文件，用于查询 CPU 信息:

    ```sh
    [nfs]# ls -la /dev/cpu/0/cpuid 
    crw-------. 1 root root 203, 0 May 10 06:16 /dev/cpu/0/cpuid

    ```

*   **/dev/md**:这是另一个包含符号链接的目录，这些符号链接具有用户友好的名称，链接到实际的 RAID 设备。 如果我们使用`ls`，我们可以看到这个系统上可用的 RAID 设备:

    ```sh
    [nfs]# ls -la /dev/md/
    total 0
    drwxr-xr-x.  2 root root   80 May 10 06:16 .
    drwxr-xr-x. 20 root root 3180 May 10 06:16 ..
    lrwxrwxrwx.  1 root root    8 May 10 06:16 boot -> ../md127
    lrwxrwxrwx.  1 root root    8 May 10 06:16 pv00 -> ../md126

    ```

*   **/dev/random**和**/dev/urandom**:这两个设备用于生成随机数据。 `/dev/random`和`/dev/urandom`设备都将从内核的熵池中抽取随机数据。 这两个之间的一个区别是，当系统的熵计数较低时，`/dev/random`设备将等待，直到有足够的熵被重新添加。

如前所述，`/dev/`目录有许多有用的文件和目录。 然而，回到我们最初的问题，我们已经确定了`/dev/sdb`存在，并且有两个分区`/dev/sdb1`和`/dev/sdb2`。

然而，我们还没有确定`/dev/sdb`是否最初是两个 RAID 设备的一部分，目前处于降级状态。 为此，我们可以利用`dmesg`设备。

# 带有 dmesg 的设备消息

对于故障诊断硬件问题，`dmesg`命令是一个很好的命令。 当系统最初启动时，内核将标识该系统可用的各种硬件设备。

当内核识别这些设备时，信息被写入内核的环形缓冲区。 这个环形缓冲区实际上是内核的内部日志。 可以使用`dmesg`命令打印该环形缓冲区。

下面是来自`dmesg`命令的示例输出; 在本例中，我们将使用`head`命令将输出缩短为仅前 15 行:

```sh
[nfs]# dmesg | head -15
[    0.000000] Initializing cgroup subsys cpuset
[    0.000000] Initializing cgroup subsys cpu
[    0.000000] Initializing cgroup subsys cpuacct
[    0.000000] Linux version 3.10.0-229.1.2.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) ) #1 SMP Fri Mar 27 03:04:26 UTC 2015
[    0.000000] Command line: BOOT_IMAGE=/vmlinuz-3.10.0-229.1.2.el7.x86_64 root=/dev/mapper/md0-root ro rd.lvm.lv=md0/swap crashkernel=auto rd.md.uuid=bec13d99:42674929:76663813:f748e7cb rd.lvm.lv=md0/root rd.md.uuid=7adf0323:b0962394:387e6cd0:b2914469 rhgb quiet LANG=en_US.UTF-8 systemd.debug
[    0.000000] e820: BIOS-provided physical RAM map:
[    0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable
[    0.000000] BIOS-e820: [mem 0x000000000009fc00-0x000000000009ffff] reserved
[    0.000000] BIOS-e820: [mem 0x00000000000f0000-0x00000000000fffff] reserved
[    0.000000] BIOS-e820: [mem 0x0000000000100000-0x000000001ffeffff] usable
[    0.000000] BIOS-e820: [mem 0x000000001fff0000-0x000000001fffffff] ACPI data
[    0.000000] BIOS-e820: [mem 0x00000000fffc0000-0x00000000ffffffff] reserved
[    0.000000] NX (Execute Disable) protection: active
[    0.000000] SMBIOS 2.5 present.
[    0.000000] DMI: innotek GmbH VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006

```

我们之所以将的输出限制为15 行，是因为`dmesg`命令将输出相当多的数据。 为了把它放在透视图中，我们可以再次运行该命令，但这一次将输出发送到`wc -l`，它将计算打印的行数:

```sh
[nfs]# dmesg | wc -l
597

```

如我们所见，`dmesg`命令返回`597`行。 读取内核环形缓冲区的所有 597 行并不是一个快速的过程。

由于我们的目标是找到关于`/dev/sdb`的信息，我们可以再次运行`dmesg`命令，这一次使用`grep`命令过滤出`/dev/sdb`相关信息的输出:

```sh
[nfs]# dmesg | grep -C 5 sdb
[    2.176800] scsi 3:0:0:0: CD-ROM            VBOX     CD-ROM           1.0  PQ: 0 ANSI: 5
[    2.194908] sd 0:0:0:0: [sda] 16777216 512-byte logical blocks: (8.58 GB/8.00 GiB)
[    2.194951] sd 0:0:0:0: [sda] Write Protect is off
[    2.194953] sd 0:0:0:0: [sda] Mode Sense: 00 3a 00 00
[    2.194965] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
[    2.196250] sd 1:0:0:0: [sdb] 16777216 512-byte logical blocks: (8.58 GB/8.00 GiB)
[    2.196279] sd 1:0:0:0: [sdb] Write Protect is off
[    2.196281] sd 1:0:0:0: [sdb] Mode Sense: 00 3a 00 00
[    2.196294] sd 1:0:0:0: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
[    2.197471]  sda: sda1 sda2
[    2.197700] sd 0:0:0:0: [sda] Attached SCSI disk
[    2.198139]  sdb: sdb1 sdb2
[    2.198319] sd 1:0:0:0: [sdb] Attached SCSI disk
[    2.200851] sr 3:0:0:0: [sr0] scsi3-mmc drive: 32x/32x xa/form2 tray
[    2.200856] cdrom: Uniform CD-ROM driver Revision: 3.20
[    2.200980] sr 3:0:0:0: Attached scsi CD-ROM sr0
[    2.366634] md: bind<sda1>
[    2.370652] md: raid1 personality registered for level 1
[    2.370820] md/raid1:md127: active with 1 out of 2 mirrors
[    2.371797] created bitmap (1 pages) for device md127
[    2.372181] md127: bitmap initialized from disk: read 1 pages, set 0 of 8 bits
[    2.373915] md127: detected capacity change from 0 to 524222464
[    2.374767]  md127: unknown partition table
[    2.376065] md: bind<sdb2>
[    2.382976] md: bind<sda2>
[    2.385094] md: kicking non-fresh sdb2 from array!
[    2.385102] md: unbind<sdb2>
[    2.385105] md: export_rdev(sdb2)
[    2.387559] md/raid1:md126: active with 1 out of 2 mirrors
[    2.387874] created bitmap (1 pages) for device md126
[    2.388339] md126: bitmap initialized from disk: read 1 pages, set 19 of 121 bits
[    2.390324] md126: detected capacity change from 0 to 8060403712
[    2.391344]  md126: unknown partition table

```

当执行上述示例时，`–C`(上下文)标志被用来告诉`grep`在输出中包含 5 行上下文。 通常，当`grep`不带标志运行时，只打印包含搜索字符串(在本例中为“`sdb`”)的行。 将上下文标志设置为 5 时，`grep`命令将在包含搜索字符串的每行之前打印 5 行，在每行之后打印 5 行。

使用`grep`不仅可以看到包含字符串`sdb`的行，还可以看到可能包含额外信息的前后行。

现在我们有了这些额外的信息，让我们分解它来更好地理解它告诉我们什么:

```sh
[    2.176800] scsi 3:0:0:0: CD-ROM            VBOX     CD-ROM           1.0  PQ: 0 ANSI: 5
[    2.194908] sd 0:0:0:0: [sda] 16777216 512-byte logical blocks: (8.58 GB/8.00 GiB)
[    2.194951] sd 0:0:0:0: [sda] Write Protect is off
[    2.194953] sd 0:0:0:0: [sda] Mode Sense: 00 3a 00 00
[    2.194965] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
[    2.196250] sd 1:0:0:0: [sdb] 16777216 512-byte logical blocks: (8.58 GB/8.00 GiB)
[    2.196279] sd 1:0:0:0: [sdb] Write Protect is off
[    2.196281] sd 1:0:0:0: [sdb] Mode Sense: 00 3a 00 00
[    2.196294] sd 1:0:0:0: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
[    2.197471]  sda: sda1 sda2
[    2.197700] sd 0:0:0:0: [sda] Attached SCSI disk
[    2.198139]  sdb: sdb1 sdb2
[    2.198319] sd 1:0:0:0: [sdb] Attached SCSI disk

```

以上信息似乎是关于`/dev/sdb`的标准信息。 我们可以从这些消息中看到关于`/dev/sda`和`/dev/sdb`的一些基本信息。

从前面的信息中我们可以看到一个有用的东西是这些驱动器的大小:

```sh
[    2.194908] sd 0:0:0:0: [sda] 16777216 512-byte logical blocks: (8.58 GB/8.00 GiB)
[    2.196250] sd 1:0:0:0: [sdb] 16777216 512-byte logical blocks: (8.58 GB/8.00 GiB)

```

我们可以看到每个驱动器的大小为`8.58`GB。 虽然这些信息一般来说是有用的，但对我们目前的情况是没有用的。 然而，有用的是前面代码片段的最后四行:

```sh
[    2.197471]  sda: sda1 sda2
[    2.197700] sd 0:0:0:0: [sda] Attached SCSI disk
[    2.198139]  sdb: sdb1 sdb2
[    2.198319] sd 1:0:0:0: [sdb] Attached SCSI disk

```

最后四行显示了`/dev/sda`和`/dev/sdb`上的可用分区，以及一条消息，说明每个磁盘都是`Attached`。

这个信息非常有用，因为它在最基本的层面上告诉我们这两个驱动器正在工作。 这是`/dev/sdb`的问题所在，因为我们怀疑 RAID 系统已将其从服务中删除。

到目前为止，`dmesg`命令已经为我们提供了一些有用的信息; 让我们继续浏览这些数据，以便更好地理解这些磁盘。

```sh
[    2.200851] sr 3:0:0:0: [sr0] scsi3-mmc drive: 32x/32x xa/form2 tray
[    2.200856] cdrom: Uniform CD-ROM driver Revision: 3.20
[    2.200980] sr 3:0:0:0: Attached scsi CD-ROM sr0

```

如果我们对我们的 CD-ROM 设备进行故障诊断，那么前面的三行将非常有用。 然而，对于我们的磁盘问题，它们并不有用，只是由于 grep 的上下文被设置为 5 而被包含在内。

然而，下面几行将告诉我们关于磁盘驱动器的相当多的信息:

```sh
[    2.366634] md: bind<sda1>
[    2.370652] md: raid1 personality registered for level 1
[    2.370820] md/raid1:md127: active with 1 out of 2 mirrors
[    2.371797] created bitmap (1 pages) for device md127
[    2.372181] md127: bitmap initialized from disk: read 1 pages, set 0 of 8 bits
[    2.373915] md127: detected capacity change from 0 to 524222464
[    2.374767]  md127: unknown partition table
[    2.376065] md: bind<sdb2>
[    2.382976] md: bind<sda2>
[    2.385094] md: kicking non-fresh sdb2 from array!
[    2.385102] md: unbind<sdb2>
[    2.385105] md: export_rdev(sdb2)
[    2.387559] md/raid1:md126: active with 1 out of 2 mirrors
[    2.387874] created bitmap (1 pages) for device md126
[    2.388339] md126: bitmap initialized from disk: read 1 pages, set 19 of 121 bits
[    2.390324] md126: detected capacity change from 0 to 8060403712
[    2.391344]  md126: unknown partition table

```

dmesg 输出的最后一部分告诉我们很多关于 RAID 设备和`/dev/sdb`的信息。 因为有相当多的数据，我们需要将其分解，以真正理解所有:

```sh
The first few lines show use information about /dev/md127.
[    2.366634] md: bind<sda1>
[    2.370652] md: raid1 personality registered for level 1
[    2.370820] md/raid1:md127: active with 1 out of 2 mirrors
[    2.371797] created bitmap (1 pages) for device md127
[    2.372181] md127: bitmap initialized from disk: read 1 pages, set 0 of 8 bits
[    2.373915] md127: detected capacity change from 0 to 524222464
[    2.374767]  md127: unknown partition table

```

这一行似乎是在引导期间创建的信息，因为这些消息表明 RAID 正在初始化。 它还显示，当 RAID 被初始化时，它检测到两个可用磁盘中只有一个绑定到 RAID。

虽然这个信息本身对于我们的故障诊断并不新鲜，但它告诉我们的是系统是在这个状态下启动的。 这个意味着`/dev/sdb` 发生的事情可能发生在这个系统最近一次重新启动之前。

从这个片段的其余部分来看，有类似的消息为`/dev/md126`; 然而，还有一些更多的信息包括在这些消息:

```sh
[    2.376065] md: bind<sdb2>
[    2.382976] md: bind<sda2>
[    2.385094] md: kicking non-fresh sdb2 from array!
[    2.385102] md: unbind<sdb2>
[    2.385105] md: export_rdev(sdb2)
[    2.387559] md/raid1:md126: active with 1 out of 2 mirrors
[    2.387874] created bitmap (1 pages) for device md126
[    2.388339] md126: bitmap initialized from disk: read 1 pages, set 19 of 121 bits
[    2.390324] md126: detected capacity change from 0 to 8060403712
[    2.391344]  md126: unknown partition table

```

前面的消息看起来非常类似于来自`/dev/md127`的消息; 然而，有一些行在`/dev/md127`的消息中不存在:

```sh
[    2.376065] md: bind<sdb2>
[    2.382976] md: bind<sda2>
[    2.385094] md: kicking non-fresh sdb2 from array!
[    2.385102] md: unbind<sdb2>

```

如果我们查看这些消息，可以看到`/dev/md126`试图在 RAID 阵列中使用`/dev/sdb2`; 然而，它发现驱动器不新鲜。 非新鲜消息很有趣，因为它可能解释了为什么`/dev/sdb`没有被包含到 RAID 设备中。

## 总结 dmesg 提供的内容

在 RAID 集中，每个磁盘为每个写请求维护一个事件计数。 RAID 使用这个事件计数来确保每个磁盘都收到了适当数量的写请求。 这允许 RAID 验证整个 RAID 的一致性。

当 RAID 重新启动时，RAID 管理器将检查每个磁盘的事件数，确保它们是一致的。

从前面的消息可以看出，`/dev/sda2`可能比`/dev/sdb2`具有更高的事件计数。 这表明在`/dev/sda1`上发生了一些写操作，而在`/dev/sdb2`上从未发生过。 对于镜像阵列，这将是不正常的，并表明有`/dev/sdb2`的问题。

我们如何检查事件计数是否不同? 使用`mdadm`命令，我们可以显示每个磁盘设备的事件计数。

# 使用 mdadm 检查超级块

要查看事件计数，我们将使用带有`--examine`标志的`mdadm`命令检查磁盘设备:

```sh
[nfs]# mdadm --examine /dev/sda1
/dev/sda1:
 Magic : a92b4efc
 Version : 1.0
 Feature Map : 0x1
 Array UUID : 7adf0323:b0962394:387e6cd0:b2914469
 Name : localhost:boot
 Creation Time : Wed Apr 15 09:39:22 2015
 Raid Level : raid1
 Raid Devices : 2

 Avail Dev Size : 1023968 (500.07 MiB 524.27 MB)
 Array Size : 511936 (500.02 MiB 524.22 MB)
 Used Dev Size : 1023872 (500.02 MiB 524.22 MB)
 Super Offset : 1023984 sectors
 Unused Space : before=0 sectors, after=96 sectors
 State : clean
 Device UUID : 92d97c32:1f53f59a:14a7deea:34ec8c7c

Internal Bitmap : -16 sectors from superblock
 Update Time : Mon May 11 04:08:10 2015
 Bad Block Log : 512 entries available at offset -8 sectors
 Checksum : bd8c1d5b - correct
 Events : 60

 Device Role : Active device 0
 Array State : A. ('A' == active, '.' == missing, 'R' == replacing)

```

除了`--detail`用于打印 RAID 设备的详细信息外，`--examine`标志与`--detail`非常相似。 `--examine`用于从组成 RAID 的单个磁盘上打印 RAID 的详细信息。 `--examine`打印的详细信息实际上来自磁盘上的超级块详细信息。

当 Linux RAID 将磁盘用作 RAID 设备的一部分时，RAID 系统将在磁盘上为**超级块**预留一些空间。 这个超级块只是用来存储关于磁盘和 RAID 的元数据。

在上面的命令中，我们简单地打印了`/dev/sda1`中的 RAID超级块信息。 为了更好地理解 RAID 超级块，让我们看看`--examine`标志提供的细节:

```sh
/dev/sda1:
 Magic : a92b4efc
 Version : 1.0
 Feature Map : 0x1
 Array UUID : 7adf0323:b0962394:387e6cd0:b2914469
 Name : localhost:boot
 Creation Time : Wed Apr 15 09:39:22 2015
 Raid Level : raid1
 Raid Devices : 2

```

该输出的第一部分提供了相当多的有用信息。 例如，这个神奇的数字被用作超级块头。 这是一个用来指示超级块开始的值。

另一个有用的信息是`Array UUID`。 这是该磁盘所属 RAID 的唯一标识符。 如果我们打印 RAID`md127`的详细信息，我们可以看到来自`/dev/sda1`的 Array UUID 和来自`md127`的 UUID 匹配:

```sh
[nfs]# mdadm --detail /dev/md127 | grep UUID
 UUID : 7adf0323:b0962394:387e6cd0:b2914469

```

当设备名称发生更改，并且您需要识别属于特定 RAID 的磁盘时，这可能很有用。 这方面的一个例子是，如果有人在硬件维护期间不小心将驱动器放入了错误的插槽。 如果驱动器仍然包含 UUID，则可以识别错位驱动器所属的 RAID。

下面三行`Creation Time`、`RAID Level`和`RAID Devices`在与`--detail`的输出一起使用时也非常有用。

第二个信息片段对于确定关于磁盘设备的信息很有用:

```sh
Avail Dev Size : 1023968 (500.07 MiB 524.27 MB)
 Array Size : 511936 (500.02 MiB 524.22 MB)
 Used Dev Size : 1023872 (500.02 MiB 524.22 MB)
 Super Offset : 1023984 sectors
 Unused Space : before=0 sectors, after=96 sectors
 State : clean
 Device UUID : 92d97c32:1f53f59a:14a7deea:34ec8c7c

```

在这个代码片段中，我们可以在前三行中看到单个磁盘和数组的大小。 如果有关于阵列中每个磁盘的大小的问题，这个信息可能非常有用。 除了大小，我们还可以看到 RAID 当前的`State`。 这个状态与我们从`/dev/md127`的`--detail`输出中看到的状态相匹配。

```sh
[nfs]# mdadm --detail /dev/md127 | grep State
 State : clean, degraded

```

来自`--examine`输出的信息的下一部分对于我们的问题非常有用:

```sh
Internal Bitmap : -16 sectors from superblock
 Update Time : Mon May 11 04:08:10 2015
 Bad Block Log : 512 entries available at offset -8 sectors
 Checksum : bd8c1d5b - correct
 Events : 60

 Device Role : Active device 0
 Array State : A. ('A' == active, '.' == missing, 'R' == replacing)

```

在本节中，我们可以查看`Events`信息，该信息显示了该磁盘上的当前事件计数值。 我们还可以看到`/dev/sda1`的`Array State`值。 `A`的值。 表示从`/dev/sda1`的角度来看，其镜像伙伴缺失。

当我们检查`/dev/sdb1`下的超级块的细节时，我们将看到`Array State`和`Events`值之间有趣的差异:

```sh
[nfs]# mdadm --examine /dev/sdb1
/dev/sdb1:
 Magic : a92b4efc
 Version : 1.0
 Feature Map : 0x1
 Array UUID : 7adf0323:b0962394:387e6cd0:b2914469
 Name : localhost:boot
 Creation Time : Wed Apr 15 09:39:22 2015
 Raid Level : raid1
 Raid Devices : 2

 Avail Dev Size : 1023968 (500.07 MiB 524.27 MB)
 Array Size : 511936 (500.02 MiB 524.22 MB)
 Used Dev Size : 1023872 (500.02 MiB 524.22 MB)
 Super Offset : 1023984 sectors
 Unused Space : before=0 sectors, after=96 sectors
 State : clean
 Device UUID : 5a9bb172:13102af9:81d761fb:56d83bdd

Internal Bitmap : -16 sectors from superblock
 Update Time : Mon May  4 21:09:30 2015
 Bad Block Log : 512 entries available at offset -8 sectors
 Checksum : cd226d7b - correct
 Events : 48

 Device Role : Active device 1
 Array State : AA ('A' == active, '.' == missing, 'R' == replacing)

```

从结果来看，我们已经回答了不少关于`/dev/sdb1`的问题。

我们的第一个问题是/`dev/sdb1`是否是RAID 的一部分。 由于该设备有一个 RAID 超级块，并且该信息可以通过`mdadm`打印，因此我们可以安全地说`yes`。

```sh
 Array UUID : 7adf0323:b0962394:387e6cd0:b2914469

```

通过查看`Array UUID`，我们还可以确定这个设备是否像我们怀疑的那样是`/dev/md127`的一部分:

```sh
[nfs]# mdadm --detail /dev/md127 | grep UUID
 UUID : 7adf0323:b0962394:387e6cd0:b2914469

```

从表面上看，`/dev/sdb1`在某种程度上是`/dev/md127`的一部分。

我们需要回答的最后一个问题是`/dev/sda1`和`/dev/sdb1`之间的`Events`值是否不同。 从`/dev/sda1`的`--examine`信息中，我们可以看到事件计数被设置为 60。 在上述代码中，`--examine`由`/dev/sdb1`产生; 我们可以看到事件数要低得多-`48`:

```sh
 Events : 48

```

考虑到差异，我们可以有把握地说，`/dev/sdb1`比`/dev/sda1`落后 12 个事件。 这是一个非常显著的差异，也是 MD 拒绝将`/dev/sdb1`添加到 RAID 阵列的合理原因。

有趣的是，如果我们看一下`/dev/sdb1`的`Array State`，我们可以看到它仍然认为它是`/dev/md127`阵列中的活动磁盘:

```sh
 Array State : AA ('A' == active, '.' == missing, 'R' == replacing)

```

这是由于事实，因为设备不再是 RAID 的部分，它没有被更新为当前状态。 我们也可以在更新时间中看到:

```sh
 Update Time : Mon May  4 21:09:30 2015

```

`/dev/sda1`的`Update Time`是最近的; 因此，它应该被信任在磁盘`/dev/sdb1`之上。

## 检查/dev/sdb2

既然我们知道了没有将`/dev/sdb1`添加到`/dev/md127`中的原因，那么我们应该确定`/dev/sdb2`和`/dev/md126`是否存在同样的情况。

因为我们已经知道`/dev/sda2`是健康的并且是`/dev/md126`数组的一部分，所以我们将只关注捕获它的`Events`值:

```sh
[nfs]# mdadm --examine /dev/sda2 | grep Events
 Events : 7517

```

与`/dev/sda1`相比，`/dev/sda2`的事件数相当高。 由此，我们可以确定`/dev/md126`可能是一个非常活跃的 RAID 设备。

现在我们有了事件计数，让我们来看看/`dev/sdb2`的细节:

```sh
[nfs]# mdadm --examine /dev/sdb2
/dev/sdb2:
 Magic : a92b4efc
 Version : 1.2
 Feature Map : 0x1
 Array UUID : bec13d99:42674929:76663813:f748e7cb
 Name : localhost:pv00
 Creation Time : Wed Apr 15 09:39:19 2015
 Raid Level : raid1
 Raid Devices : 2

 Avail Dev Size : 15742976 (7.51 GiB 8.06 GB)
 Array Size : 7871488 (7.51 GiB 8.06 GB)
 Data Offset : 8192 sectors
 Super Offset : 8 sectors
 Unused Space : before=8104 sectors, after=0 sectors
 State : clean
 Device UUID : 01db1f5f:e8176cad:8ce68d51:deff57f8

Internal Bitmap : 8 sectors from superblock
 Update Time : Mon May  4 21:10:31 2015
 Bad Block Log : 512 entries available at offset 72 sectors
 Checksum : 98a8ace8 - correct
 Events : 541

 Device Role : Active device 1
 Array State : AA ('A' == active, '.' == missing, 'R' == replacing)

```

同样，从我们能够从`/dev/sdb2`打印超级块信息的事实，我们已经确定这个设备实际上是 RAID 的一部分:

```sh
 Array UUID : bec13d99:42674929:76663813:f748e7cb

```

如果我们将`/dev/sdb2`的`Array UUID`与`/dev/md126`的`UUID`进行比较，我们也会看到它实际上是 RAID 阵列的一部分:

```sh
[nfs]# mdadm --detail /dev/md126 | grep UUID
 UUID : bec13d99:42674929:76663813:f748e7cb

```

这就回答了我们关于`/dev/sdb2`是否是`md126`RAID 的一部分的问题。 如果我们看看`/dev/sdb2`的事件计数，我们也可以回答为什么它目前不是 RAID 的一部分的问题:

```sh
Events : 541

```

假设`/dev/sda2`的`Events`计数为 7517，`/dev/sdb2`的`Events`计数为 541，那么这个设备似乎错过了发送到`md126`RAID 的写事件。

# 到目前为止我们所学到的

通过到目前为止所采取的故障排除步骤，我们已经收集了相当多的关键数据片段。 让我们来看看我们所学到的，以及我们可以从这些发现中推断出什么:

*   On our system, we have two RAID devices.

    使用`mdadm`命令和`/proc/mdstat`的内容，我们能够确定这个系统有两个 RAID 设备`—/dev/md126`和`/dev/md127`。

*   Both RAID devices are a RAID 1 and missing a mirrored device.

    通过`mdadm`命令和`dmesg`输出，我们能够确定两个 RAID 设备都被设置为 RAID 1 设备。 除此之外，我们还发现两个 RAID 设备都缺少一个磁盘; 两个丢失的设备都是来自`/dev/sdb`硬盘的分区。

*   Both `/dev/sdb1` and `/dev/sdb2` have mismatched event counts.

    通过`mdadm`命令，我们可以检查`/dev/sdb1`和`/dev/sdb2`设备的`superblock`细节。 在此期间，我们能够看到这些设备的事件计数与`/dev/sda`上的活动分区不匹配。

    因此，RAID 不会将`/dev/sdb`设备重新添加到各自的 RAID 阵列中。

*   The disk `/dev/sdb` seems to be functional.

    RAID 没有将`/dev/sdb1`或`/dev/sdb2`加入到各自的 RAID 阵列中，并不意味着设备`/dev/sdb`故障。

从`dmesg`中的消息中，我们没有看到`/dev/sdb`设备本身的任何错误。 我们还可以使用`mdadm`检查这些驱动器上的分区。 从我们目前所做的一切来看，这些驱动器似乎是有效的。

# 重新添加驱动器到阵列

`/dev/sdb`磁盘似乎是正常的，除了事件计数的差异外，我们看不到 RAID 拒绝设备的任何原因。 我们的下一步将尝试重新将移除的设备添加到它们的 RAID 阵列中。

我们将尝试使用的第一个 RAID 是`/dev/md127`:

```sh
[nfs]# mdadm --detail /dev/md127
/dev/md127:
 Version : 1.0
 Creation Time : Wed Apr 15 09:39:22 2015
 Raid Level : raid1
 Array Size : 511936 (500.02 MiB 524.22 MB)
 Used Dev Size : 511936 (500.02 MiB 524.22 MB)
 Raid Devices : 2
 Total Devices : 1
 Persistence : Superblock is persistent

 Intent Bitmap : Internal

 Update Time : Mon May 11 04:08:10 2015
 State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 0
 Spare Devices : 0

 Name : localhost:boot
 UUID : 7adf0323:b0962394:387e6cd0:b2914469
 Events : 60

 Number   Major   Minor   RaidDevice State
 0       8        1        0      active sync   /dev/sda1
 2       0        0        2      removed

```

重新添加驱动器的最简单的方法是在`mdadm`中使用`-a`(add)标志。

```sh
[nfs]# mdadm /dev/md127 -a /dev/sdb1
mdadm: re-added /dev/sdb1

```

上面的命令将告诉`mdadm`将设备`/dev/sdb1`添加到 RAID 设备`/dev/md127`。 由于`/dev/sdb1`已经是 RAID 阵列的一部分，MD 服务只需重新添加磁盘并重新同步`/dev/sda1`中丢失的事件。

如果我们查看带有`--detail`标志的 RAID 细节，我们就可以看到这一点:

```sh
[nfs]# mdadm --detail /dev/md127
/dev/md127:
 Version : 1.0
 Creation Time : Wed Apr 15 09:39:22 2015
 Raid Level : raid1
 Array Size : 511936 (500.02 MiB 524.22 MB)
 Used Dev Size : 511936 (500.02 MiB 524.22 MB)
 Raid Devices : 2
 Total Devices : 2
 Persistence : Superblock is persistent

 Intent Bitmap : Internal

 Update Time : Mon May 11 16:47:32 2015
 State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 0
 Spare Devices : 1

 Rebuild Status : 50% complete

 Name : localhost:boot
 UUID : 7adf0323:b0962394:387e6cd0:b2914469
 Events : 66

 Number   Major   Minor   RaidDevice State
 0       8        1        0      active sync   /dev/sda1
 1       8       17        1      spare rebuilding   /dev/sdb1

```

从前面的输出中，我们可以看到与前面的示例有一些不同。 一个非常重要的区别是`Rebuild Status`:

```sh
Rebuild Status : 50% complete

```

使用`mdadm --detail`，我们可以看到驱动器重新同步的完成状态。 如果在这个过程中有任何错误，我们也可以看到。 如果我们看看底部的三条线，我们还可以看到哪些设备是活动的，哪些正在重建。

```sh
 Number   Major   Minor   RaidDevice State
 0       8        1        0      active sync   /dev/sda1
 1       8       17        1      spare rebuilding   /dev/sdb1

```

几秒钟后，如果我们再次运行`mdadm --detail`，我们应该会看到 RAID 设备已经重新同步:

```sh
[nfs]# mdadm --detail /dev/md127
/dev/md127:
 Version : 1.0
 Creation Time : Wed Apr 15 09:39:22 2015
 Raid Level : raid1
 Array Size : 511936 (500.02 MiB 524.22 MB)
 Used Dev Size : 511936 (500.02 MiB 524.22 MB)
 Raid Devices : 2
 Total Devices : 2
 Persistence : Superblock is persistent

 Intent Bitmap : Internal

 Update Time : Mon May 11 16:47:32 2015
 State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
 Spare Devices : 0

 Name : localhost:boot
 UUID : 7adf0323:b0962394:387e6cd0:b2914469
 Events : 69

 Number   Major   Minor   RaidDevice State
 0       8        1        0      active sync   /dev/sda1
 1       8       17        1      active sync   /dev/sdb1

```

现在我们可以看到两个驱动器都以`active sync`状态列出，而RAID`State`只是`clean`。

前面的输出是一个正常的 RAID 1 设备应该是什么样的。 此时，我们可以考虑解决了`/dev/md127`后的问题。

## 添加新的磁盘设备

有时您会发现自己处于这样一种情况:您的磁盘驱动器实际上是故障的，而实际的物理硬件必须被替换。 在这种情况下，一旦分区`/dev/sdb1`和`/dev/sdb2`被重新创建，设备就可以简单地添加到 RAID 中，步骤与我们前面使用的相同。

当执行`mdadm <raid device> -a <disk device>`命令时，`mdadm`首先检查磁盘设备是否曾经是 RAID 的一部分。

它通过读取磁盘设备上的超级块信息来实现这一点。 如果设备以前是 RAID 的一部分，它只需重新添加它并开始重建以重新同步驱动器。

如果磁盘设备从来不是 RAID 的一部分，它将作为备用设备添加，如果 RAID 降级，备用设备将用于将 RAID 恢复到干净状态。

## 未干净添加磁盘时

在以前的工作环境中，当我们更换硬盘驱动器时，在生产环境中使用硬盘驱动器替换故障驱动器之前，总是对硬盘驱动器进行质量测试。 通常，这种质量测试涉及创建分区并将这些分区添加到现有 RAID 中。

因为这些设备上已经有一个 RAID 超级块，`mdadm`将拒绝将设备添加到 RAID 中。 可以使用`mdadm`命令清除已存在的 RAID`superblock`:

```sh
[nfs]# mdadm --zero-superblock /dev/sdb2

```

上面的命令将告诉`mdadm`从指定的磁盘上删除 RAID`superblock`信息——在本例中是`/dev/sdb2`:

```sh
[nfs]# mdadm --examine /dev/sdb2
mdadm: No md superblock detected on /dev/sdb2.

```

使用`--examine`，我们可以看到设备上现在没有超级块了。

应该谨慎使用`--zero-superblock`标志，并且只在不再需要设备数据时使用。 一旦这个超级块信息被移除，RAID 将这个磁盘视为一个空白磁盘，并且在任何重新同步过程中，现有的数据将被覆盖。

一旦超级块被移除，可以执行相同的步骤将其添加到 RAID 阵列:

```sh
[nfs]# mdadm /dev/md126 -a /dev/sdb2
mdadm: added /dev/sdb2

```

## 观察重建状态的另一种方式

之前我们使用`mdadm --detail`来显示`md127`的重建状态。 另一种查看此信息的方法是通过`/proc/mdstat`:

```sh
[nfs]# cat /proc/mdstat
Personalities : [raid1] 
md126 : active raid1 sdb2[2] sda2[0]
 7871488 blocks super 1.2 [2/1] [U_]
 [>....................]  recovery =  0.0% (1984/7871488) finish=65.5min speed=1984K/sec
 bitmap: 1/1 pages [4KB], 65536KB chunk

md127 : active raid1 sdb1[1] sda1[0]
 511936 blocks super 1.0 [2/2] [UU]
 bitmap: 0/1 pages [0KB], 65536KB chunk

unused devices: <none>

```

一段时间后，RAID 将完成重新同步; 现在，两个 RAID 阵列都处于健康状态:

```sh
[nfs]# cat /proc/mdstat 
Personalities : [raid1] 
md126 : active raid1 sdb2[2] sda2[0]
 7871488 blocks super 1.2 [2/2] [UU]
 bitmap: 0/1 pages [0KB], 65536KB chunk

md127 : active raid1 sdb1[1] sda1[0]
 511936 blocks super 1.0 [2/2] [UU]
 bitmap: 0/1 pages [0KB], 65536KB chunk

unused devices: <none>

```

# 总结

在上一章，[第七章](07.html#19UOO2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 7. Filesystem Errors and Recovery")，*文件系统错误和恢复*中，我们注意到在`/var/log/messages`日志文件中有一个简单的 RAID 失败消息。 在本章中，我们使用了`Data Collector`方法来调查该失败消息的原因。

在使用 RAID 管理命令`mdadm`进行调查之后，我们发现有几个 RAID 设备处于降级状态。 使用`dmesg`，我们能够确定哪些硬盘驱动器设备受到了影响，以及磁盘在某个时间点被从服务中删除。 我们还发现磁盘**事件计数**不匹配，阻止了自动重新添加磁盘。

我们使用`dmesg`验证设备没有物理故障，并选择将其重新添加到 RAID 阵列。

虽然本章主要关注 RAID 和磁盘故障，但`/var/log/messages`和`dmesg`都可以用于排除其他设备故障。 然而，对于硬盘以外的设备，解决方案通常是简单的更换。 当然，像大多数事情一样，这取决于所经历的失败类型。

在下一章中，我们将展示如何排除自定义用户应用的故障，以及如何使用系统工具来执行一些高级故障排除。