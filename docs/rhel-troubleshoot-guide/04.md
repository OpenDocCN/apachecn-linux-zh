# 四、故障诊断性能问题

在第 3 章,*故障排除一个 Web 应用*我们走过故障排除一个 Web 应用的问题通过使用故障排除方法覆盖[第一章](01.html#DB7S1-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 1. Troubleshooting Best Practices"),*故障诊断最佳实践。 我们还使用了在[第二章](02.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 2. Troubleshooting Commands and Sources of Useful Information")、*故障诊断命令和有用信息来源*中找到的几个基本故障诊断命令和资源。*

 *# 性能问题

对于本章，我们将继续在[第 3 章](03.html#KVCC1-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 3. Troubleshooting a Web Application")、*故障诊断 Web 应用*中讨论的场景，在这个场景中，我们是一家新公司的新系统管理员。 当我们到达目的地开始一天的工作时，一位系统管理员同事要求我们检查服务器是否“慢”。

当被问及详细信息时，我们的同事只能提供主机名和被认为“慢”的服务器 IP。 我们的同行提到一个用户报告了它，并且用户没有提供很多细节。

在这个场景中，不像在[第 3 章](03.html#KVCC1-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 3. Troubleshooting a Web Application")，*故障诊断 Web 应用*中讨论的场景，我们一开始没有太多的信息。 似乎我们也无法向用户询问故障排除问题。 要求系统管理员用很少的信息对问题进行故障排除的情况并不少见。 事实上，这种情况很常见。

## 它很慢

“它太慢了”是有问题的。 抱怨服务器或服务慢的最大问题是，“慢”是相对于用户体验的问题。

在处理任何关于性能的抱怨时，需要理解的一个重要区别是环境设计的基准。 在某些环境中，以 30%的 CPU 利用率运行的系统可能是正常的业务活动，而其他环境可能保持其系统以 10%的 CPU 利用率运行，30%的利用率峰值将表明存在问题。

在进行故障排除和调查性能问题时，重要的是回顾系统的历史性能指标，以确保您拥有围绕正在收集的度量的上下文。 这将有助于确定当前系统的利用率是预期的还是异常的。

# 性能

一般来说，性能问题可以分为五个方面:

*   应用
*   CPU
*   内存
*   磁盘
*   网络

任何一个领域的瓶颈往往也会影响到其他领域; 因此，理解每一个主题都是一个好主意。 通过理解这些资源的访问和交互方式，您将能够找到消耗多个资源的问题的根源。

由于报告的问题不包括性能问题的任何细节，我们将探索并了解这些领域中的每一个。 完成之后，我们将查看收集的数据和历史统计数据，以确定性能是否符合预期，或者系统性能是否真的下降。

## 应用

在创建性能类别列表时，我将它们按我最常看到的区域排序。 每个环境都是不同的，但根据我的经验，应用常常是性能问题的主要来源。

虽然本章旨在讨论性能问题，但是[第 9 章](09.html#1Q5IA1-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 9. Using System Tools to Troubleshoot Applications")，*使用系统工具解决应用问题*致力于使用系统工具解决应用问题，包括性能问题。 在本章中，我们将假设我们的问题与应用无关，并特别关注系统性能。

## CPU

CPU 是一个非常常见的性能瓶颈。 有时，问题是严格基于 CPU 的，而在其他时候，CPU 使用率的增加是另一个问题的症状。

调查 CPU 利用率的最常见命令是 top 命令。 这个命令的主要作用是识别进程的 CPU 利用率。 在[第 2 章](02.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 2. Troubleshooting Commands and Sources of Useful Information")、*故障诊断命令和有用信息来源*中，我们讨论了使用`ps`命令进行这类活动。 在这一节中，我们将通过使用 top 和 ps 来调查我们的 CPU 利用率，来调查我们对慢的抱怨。

### Top -单命令查看一切

top**命令是系统管理员和用户为了查看整体系统性能而运行的首批命令之一。 这样做的原因是，top 不仅显示了平均负载、CPU 和内存的细分，而且还显示了利用这些资源的排序的进程列表。**

 **`top`最好的部分是，当没有任何标志运行时，这些细节每 3 秒更新一次。

下面是在没有任何标志的情况下运行`top`时的输出示例。

```sh
top - 17:40:43 up  4:07,  2 users,  load average: 0.32, 0.43, 0.44
Tasks: 100 total,   2 running,  98 sleeping,   0 stopped,   0 zombie
%Cpu(s): 37.3 us,  0.7 sy,  0.0 ni, 62.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:    469408 total,   228112 used,   241296 free,      764 buffers
KiB Swap:  1081340 total,        0 used,  1081340 free.    95332 cached Mem

 PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
 3023 vagrant   20   0    7396    720    504 S 37.6  0.2  91:08.04 lookbusy
 11 root      20   0       0      0      0 R  0.3  0.0   0:13.28 rcuos/0
 682 root      20   0  322752   1072    772 S  0.3  0.2   0:05.60 VBoxService
 1 root      20   0   50784   7256   2500 S  0.0  1.5   0:01.39 systemd
 2 root      20   0       0      0      0 S  0.0  0.0   0:00.00 kthreadd
 3 root      20   0       0      0      0 S  0.0  0.0   0:00.24 ksoftirqd/0
 5 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kworker/0:0H
 6 root      20   0       0      0      0 S  0.0  0.0   0:00.04 kworker/u2:0
 7 root      rt   0       0      0      0 S  0.0  0.0   0:00.00 migration/0
 8 root      20   0       0      0      0 S  0.0  0.0   0:00.00 rcu_bh
 9 root      20   0       0      0      0 S  0.0  0.0   0:00.00 rcuob/0
 10 root      20   0       0      0      0 S  0.0  0.0   0:05.44 rcu_sched

```

有相当多的位信息显示，只有默认的`top`输出。 在本节中，我们将只关注 CPU 利用率信息。

```sh
%Cpu(s): 37.3 us,  0.7 sy,  0.0 ni, 62.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st

```

在`top`命令输出的第一部分中，有一行显示了当前 CPU 利用率的细分情况。 这个列表中的每一项都代表了使用 CPU 的不同方式。 为了更好地理解输出，让我们看看这些值的含义:

*   **us - User**:这个数字是在用户模式下被进程消耗的 CPU 百分比。 在这种模式下，应用无法访问底层硬件，需要使用系统 api(也就是系统调用)来执行特权执行。 当执行这些系统调用时，执行将成为系统 CPU 利用率的一部分。
*   **sy - System**:这个数字是内核模式执行所消耗的 CPU 的百分比。 在这种模式下，系统可以直接访问底层硬件; 该模式通常为受信任的 OS 进程保留。
*   **ni - Nice 用户进程**:这个数字是拥有一个 Nice 值的用户进程所消耗的 CPU 时间的百分比。 `us%`值是专门针对那些没有从原始值修改其 nice 值的过程的。
*   **id - Idle**:这个数字是 CPU 空闲时间的百分比。 本质上，它是没有被利用的 CPU 时间的数量。
*   **wa - Wait**:这个数字是 CPU 等待时间的百分比。 当许多进程正在等待 I/O 设备时，这个值通常很高。 I/O 等待状态不仅仅是指硬盘，而是指包括硬盘在内的所有 I/O 设备。
*   **hi -硬件中断**:这个数字是被硬件中断消耗的 CPU 时间的百分比。 硬件中断是来自系统硬件(如硬盘驱动器或网络设备)的信号，这些信号被发送到 CPU。 这些中断表明存在需要 CPU 时间的事件。
*   **si -软件中断**:这个数字是被软件中断消耗的 CPU 时间的百分比。 软件中断类似于硬件中断; 然而，它们是由正在运行的进程向内核发送的信号触发的。
*   **st - Stolen**:这个数字特别适用于作为虚拟机运行的 Linux 系统。 这个数字是主机从这台机器窃取的 CPU 时间的百分比。 这个数字通常在主机本身出现 CPU 争用时出现。 在某些云环境中，作为一种强制资源限制的方法，这种情况也会发生。

前面我提到，默认情况下，`top`的输出每 3 秒刷新一次。 CPU 百分比行也每 3 秒刷新一次; `top`将显示自上次刷新间隔以来每个状态的 CPU 时间百分比。

#### 关于我们的问题，这个输出告诉了我们什么?

如果我们回顾前面的`top`命令的输出，我们可以确定关于这个系统的很多信息。

```sh
%Cpu(s): 37.3 us,  0.7 sy,  0.0 ni, 62.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st

```

从前面的输出，我们可以看到 CPU 时间的`37.3%`被用户模式下的进程所消耗。 另一个 CPU 时间`0.7%`被内核执行模式的进程占用; 这是基于`us`和`sy`值。 `id`值告诉我们剩余的 CPU 没有被利用，这意味着总的来说，该服务器上有足够的 CPU 可用。

`top`命令显示的另一个事实是，没有花费多少 CPU 时间来等待 I/O。 我们可以从`wa`值为`0.0`看出这一点。 这很重要，因为它告诉我们报告的性能问题不可能是由于高 I/O。 在本章的后面，当我们开始探索磁盘性能时，我们将深入探索 I/O 等待。

#### 从上到下的个别过程

`top`命令输出中的 CPU 行是对整个服务器的总结，但 top 还包括各个进程的 CPU 利用率。 为了获得更清晰的焦点，我们可以再次执行 top，但这一次，让我们关注正在运行的进程`top`。

```sh
$ top -n 1
top - 15:46:52 up  3:21,  2 users,  load average: 1.03, 1.11, 1.06
Tasks: 108 total,   3 running, 105 sleeping,   0 stopped,   0 zombie
%Cpu(s): 34.1 us,  0.7 sy,  0.0 ni, 65.1 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st
KiB Mem:    502060 total,   220284 used,   281776 free,      764 buffers
KiB Swap:  1081340 total,        0 used,  1081340 free.    92940 cached Mem

 PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 3001 vagrant   20   0    7396    720    504 R  98.4  0.1 121:08.67 lookbusy
 3002 vagrant   20   0    7396    720    504 S   6.6  0.1  19:05.12 lookbusy
 1 root      20   0   50780   7264   2508 S   0.0  1.4   0:01.69 systemd
 2 root      20   0       0      0      0 S   0.0  0.0   0:00.01 kthreadd
 3 root      20   0       0      0      0 S   0.0  0.0   0:00.97 ksoftirqd/0
 5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
 6 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kworker/u4:0
 7 root      rt   0       0      0      0 S   0.0  0.0   0:00.67 migration/0

```

这一次，当执行`top`命令时，使用了`–n`(number)标志。 这个标志告诉`top`只刷新指定的次数，在本例中是 1 次。 当试图捕获`top`的输出时，这个技巧可能很有帮助。

如果我们检查上面的`top`命令的输出，我们可以看到一些非常有趣的东西。

```sh
 PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 3001 vagrant   20   0    7396    720    504 R  98.4  0.1 121:08.67 lookbusy

```

默认情况下，`top`命令按照进程所使用的 CPU 百分比对进程进行排序。 这意味着列表中的第一个进程是在该时间间隔内消耗最多 CPU 的进程。

如果我们查看在进程 id 为`3001`下运行的顶级进程，我们会发现它正在使用`98.4%`的 CPU 时间。 但是，根据 top 命令系统范围的 CPU 统计信息，CPU 时间的`65.1%`处于空闲状态。 对于许多系统管理员来说，这种场景实际上是一个常见的困惑来源。

```sh
%Cpu(s): 34.1 us,  0.7 sy,  0.0 ni, 65.1 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st

```

单个进程如何利用几乎 100%的 CPU 时间，而系统本身却显示 65%的 CPU 时间是空闲的? 答案其实很简单; 当`top`在其 header 中显示 CPU 利用率时，该比例以整个系统为基础。 然而，对于单个进程，CPU 利用率是针对一个 CPU 的。 这意味着我们的进程 3001 实际上占用了几乎一个完整的 CPU，而我们的系统很可能有多个 CPU。

经常可以看到能够利用多个 cpu 的进程显示的百分比高于 100%。 例如，一个充分利用三个 cpu 的进程将显示 300%。 对于不熟悉`top`命令、服务器总数和每个进程输出的用户来说，这也会造成相当多的混淆。

### 确定可用 cpu 数量

以前，我们确定这个系统必须有多个可用的 cpu。 我们没有确定的是有多少。 确定可用 cpu 数量的最简单方法是读取`/proc/cpuinfo`文件。

```sh
# cat /proc/cpuinfo
processor  : 0
vendor_id  : GenuineIntel
cpu family  : 6
model    : 58
model name  : Intel(R) Core(TM) i7-3615QM CPU @ 2.30GHz
stepping  : 9
microcode  : 0x19
cpu MHz    : 2348.850
cache size  : 6144 KB
physical id  : 0
siblings  : 2
core id    : 0
cpu cores  : 2
apicid    : 0
initial apicid  : 0
fpu    : yes
fpu_exception  : yes
cpuid level  : 5
wp    : yes
flags    : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl pni ssse3 lahf_lm
bogomips  : 4697.70
clflush size  : 64
cache_alignment  : 64
address sizes  : 36 bits physical, 48 bits virtual
power management:

processor  : 1
vendor_id  : GenuineIntel
cpu family  : 6
model    : 58
model name  : Intel(R) Core(TM) i7-3615QM CPU @ 2.30GHz
stepping  : 9
microcode  : 0x19
cpu MHz    : 2348.850
cache size  : 6144 KB
physical id  : 0
siblings  : 2
core id    : 1
cpu cores  : 2
apicid    : 1
initial apicid  : 1
fpu    : yes
fpu_exception  : yes
cpuid level  : 5
wp    : yes
flags    : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl pni ssse3 lahf_lm
bogomips  : 4697.70
clflush size  : 64
cache_alignment  : 64
address sizes  : 36 bits physical, 48 bits virtual
power management:

```

文件`/proc/cpuinfo`包含了大量关于系统可用 cpu 的有用信息。 它显示了 CPU 的类型、型号、可用的标志、CPU 的速度，以及最重要的是可用的 CPU 数量。

系统可用的每个 CPU 将在`cpuinfo`文件中列出。 这意味着您可以简单地计算`cpuinfo`文件中可用的处理器数量，从而确定服务器可用的 cpu 数量。

从上面的示例中，我们可以确定该服务器有 2 个可用 cpu。

#### 线程和内核

一个有趣的警告使用`cpuinfo`来确定可用的 cpu 数量时，当使用具有多个核和超线程的 cpu 时，细节会有点误导。 `cpuinfo`文件将 CPU 上的一个核心和一个线程报告为它可以利用的处理器。 这意味着，即使您的系统上可能安装了一个物理芯片，如果该芯片是一个四核超线程 CPU，那么`cpuinfo`文件将显示 8 个处理器。

#### lscu -查看 CPU 信息的另一种方式

而`/proc/cpuinfo`是许多管理员和用户用来确定 CPU 信息的方法; 在基于 rhel 的发行版上，还有另一个命令也将显示此信息。

```sh
$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                2
On-line CPU(s) list:   0,1
Thread(s) per core:    1
Core(s) per socket:    2
Socket(s):             1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 58
Model name:            Intel(R) Core(TM) i7-3615QM CPU @ 2.30GHz
Stepping:              9
CPU MHz:               2348.850
BogoMIPS:              4697.70
L1d cache:             32K
L1d cache:             32K
L2d cache:             6144K
NUMA node0 CPU(s):     0,1

```

`/proc/cpuinfo`和`lscpu`命令内容之间的区别在于`lscpu`可以很容易地识别内核、套接字和线程的数量。 从`/proc/cpuinfo`文件中识别相同的信息通常有点困难。

### ps -使用 ps 深入研究各个进程

虽然可以使用`top`命令查看各个进程，但我个人认为`ps`命令更适合于调查正在运行的进程。 在[第二章](02.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 2. Troubleshooting Commands and Sources of Useful Information")、*故障诊断命令和有用信息来源*中，我们介绍了`ps`命令，以及如何使用它来查看正在运行的进程的许多不同方面。

在本章中，我们将使用`ps`命令来更深入地研究进程`3001`，我们用`top`命令确定它是占用最多 CPU 时间的进程。

```sh
$ ps -lf 3001
F S UID        PID  PPID  C PRI  NI ADDR SZ WCHAN  STIME TTY TIME CMD
1 S vagrant   3001  3000 73  80   0 -  1849 hrtime 01:34 pts/1 892:23 lookbusy --cpu-mode curve --cpu-curve-peak 14h -c 20-80

```

在[第二章](02.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 2. Troubleshooting Commands and Sources of Useful Information")、*故障诊断命令和有用信息来源*中，我们讨论了使用`ps`命令显示正在运行的进程。 在前面的示例中，我们指定了两个标志，分别在[第二章](02.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 2. Troubleshooting Commands and Sources of Useful Information")、*故障诊断命令和有用信息来源*、`-l`(长列表)和`–f`(完整格式)中显示。 在本章中，我们讨论了这些标志如何为显示的进程提供额外的细节。

为了更好地理解上述过程，让我们分解这两个标志提供的额外细节。

*   当前状态:`S`(可中断睡眠)
*   用户:`vagrant`
*   进程 ID:`3001`
*   父进程 ID:`3000`
*   优先级值:`80`
*   美好程度:`0`
*   正在执行的命令:`lookbusy –cpu-mode-curve –cpu-curve-peak 14h –c 20-80`

在前面的`top`命令中，该进程几乎占用了一个满的 CPU，这意味着该进程可能与所报告的慢速有关。 通过查看上面的细节，我们可以确定关于这个过程的一些事情。

首先，它是流程`3000`的子流程; 是由父进程 ID 决定的。 第二种情况是，当我们运行`ps`命令时，它正在等待任务完成; 我们可以通过进程当前所处的可中断睡眠状态来确定这一点。

除了这两项之外，我们可以看出该进程的调度优先级并不高。 我们可以通过查看优先级值来确定这一点，在本例中优先级值为 80。 调度优先级系统的工作原理如下:序号越高，进程在系统调度程序中的优先级越低。

我们还可以看到，nice 级别被设置为默认的`0`。 这意味着用户没有将 nice 级别调整到更高(或更低)的优先级。

这些都是需要收集的关于这个过程的重要的数据点，但它们本身并不能回答这个过程是否是所报道的缓慢的原因。

#### 使用 ps 来确定进程 CPU 利用率

因为我们知道【显示】流程`3001`是`3000`孩子的过程,我们不应该只看同样的信息过程`3000`也使用`ps`识别多少 CPU 过程`3000`使用。 我们可以通过使用`ps`的`-o`(选项)标志在一个命令中完成这一切。 这个标志允许你指定自己的输出格式; 它还允许您查看不总是通过常见的`ps`标志可见的字段。

在下面的命令中，使用`–o`标志格式化`ps`命令的输出，其中包含`%cpu`字段。 这个额外的字段将显示进程的 CPU 利用率。 该命令还将使用`–p`标志指定进程`3000`和进程`3001`。

```sh
$ ps -o state,user,pid,ppid,nice,%cpu,cmd -p 3000,3001
S USER       PID  PPID  NI %CPU CMD
S vagrant   3000  2980   0  0.0 lookbusy --cpu-mode curve --cpu- curve-peak 14h -c 20-80
R vagrant   3001  3000   0 71.5 lookbusy --cpu-mode curve --cpu- curve-peak 14h -c 20-80

```

虽然上面的命令相当长，但它显示了`–o`标志的有用性。 有了正确的选项，仅使用`ps`命令就有可能找到关于进程的大量信息。

从上面命令的输出中，我们可以看到进程`3000`是`lookbusy`命令的另一个实例。 我们还可以看到，进程`3000`是进程`2980`的子进程。 在进一步讨论之前，我们应该尝试确定与过程`3001`相关的所有过程。

我们可以通过使用带有`--forest`标志的`ps`命令来做到这一点，该命令告诉`ps`以树格式打印父进程和子进程。 当提供了`–e`(一切)标志时，`ps`命令将以这种树格式打印所有进程。

### 提示

默认情况下，`ps`命令只打印与执行该命令的用户相关的进程。 标志将此行为更改为打印所有可能的进程。

下面的输出被截断以明确标识`lookbusy`进程。

```sh
$ ps --forest -eo user,pid,ppid,%cpu,cmd
root      1007     1  0.0 /usr/sbin/sshd -D
root      2976  1007  0.0  \_ sshd: vagrant [priv]
vagrant   2979  2976  0.0      \_ sshd: vagrant@pts/1
vagrant   2980  2979  0.0          \_ -bash
vagrant   3000  2980  0.0              \_ lookbusy --cpu-mode curve - -cpu-curve-peak 14h -c 20-80
vagrant   3001  3000 70.4                  \_ lookbusy --cpu-mode curve --cpu-curve-peak 14h -c 20-80
vagrant   3002  3000 14.6                  \_ lookbusy --cpu-mode curve --cpu-curve-peak 14h -c 20-80

```

从上面的`ps`输出可以看到，ID 为`3000`的`lookbusy`进程产生了两个进程，即`3001`和`3002`。 我们还可以看到当前通过 SSH 登录的流浪用户启动了`lookbusy`进程。

由于使用`ps`中的`–o`标志来显示 CPU 利用率，我们可以看到进程`3002`正在使用单个 CPU 的`14.6%`。

### 提示

需要注意的是，`ps`命令还显示单个处理器的 CPU 时间百分比，这意味着使用多个处理器的进程的值可能高于 100%。

## 把它们放在一起

现在我们已经学习了识别系统 CPU 利用率的命令，让我们将它们放在一起总结一下所发现的内容。

### 快速看一看

确定与 CPU 性能相关的问题的第一步是执行`top`命令。

```sh
$ top

top - 01:50:36 up 23:41,  2 users,  load average: 0.68, 0.56, 0.48
Tasks: 107 total,   4 running, 103 sleeping,   0 stopped,   0 zombie
%Cpu(s): 34.5 us,  0.7 sy,  0.0 ni, 64.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:    502060 total,   231168 used,   270892 free,      764 buffers
KiB Swap:  1081340 total,        0 used,  1081340 free.    94628 cached Mem

 PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 3001 vagrant   20   0    7396    724    508 R  68.8  0.1 993:06.80 lookbusy
 3002 vagrant   20   0    7396    724    508 S   1.0  0.1 198:58.16 lookbusy
 12 root      20   0       0      0      0 S   0.3  0.0   3:47.55 rcuos/0
 13 root      20   0       0      0      0 R   0.3  0.0   3:38.85 rcuos/1
 2718 vagrant   20   0  131524   2536   1344 R   0.3  0.5   0:02.28 sshd

```

从`top`的输出可以看出:

*   总的来说，系统大约有 60%-70%处于空闲状态
*   有两个进程在运行`lookbusy`命令/程序，其中一个似乎占用了单个 CPU 的 70%
    *   考虑到这个单独进程的 CPU 利用率和系统 CPU 利用率，所讨论的服务器很可能有多个 CPU
    *   我们可以使用`lscpu`命令确认存在多个 cpu
*   进程 3001 和 3002 是这个系统上占用 CPU 最多的两个进程
*   CPU 等待状态百分比为 0，这意味着问题不太可能与磁盘 I/O 相关

#### 深度挖掘与 ps

由于我们从`top`命令的输出中将`3001`和`3002`确定为可疑的进程，我们可以使用`ps`命令进一步调查这些进程。 为了保持快速的调查，我们将使用带有`–o`和`--forest`标志的`ps`命令，用一个命令识别最大可能的信息。

```sh
$ ps --forest -eo user,pid,ppid,%cpu,cmd
root      1007     1  0.0 /usr/sbin/sshd -D
root      2976  1007  0.0  \_ sshd: vagrant [priv]
vagrant   2979  2976  0.0      \_ sshd: vagrant@pts/1
vagrant   2980  2979  0.0          \_ -bash
vagrant   3000  2980  0.0              \_ lookbusy --cpu-mode curve --cpu-curve-peak 14h -c 20-80
vagrant   3001  3000 69.8                  \_ lookbusy --cpu-mode curve --cpu-curve-peak 14h -c 20-80
vagrant   3002  3000 13.9                  \_ lookbusy --cpu-mode curve --cpu-curve-peak 14h -c 20-80

```

从这个输出，我们可以确定以下内容:

*   进程 3001 和 3002 是进程 3000 的子进程
*   进程 3000 由`vagrant`用户启动
*   `lookbusy`命令似乎是一个占用大量 CPU 的命令
*   用于启动`lookbusy`的方法不是指示一个系统进程，而是指示一个用户运行一个特别命令

根据上面的信息，有可能`vagrant`用户启动的`lookbusy`进程是性能问题的根源。 如果系统通常以较低的 CPU 利用率运行，那么这是一个合理的根本原因假设。 然而，考虑到我们对这个系统不是很熟悉，也有可能`lookbusy`进程使用了几乎一个完整的 CPU 是正常的。

考虑到我们不熟悉系统的正常运行情况，在得出结论之前，我们应该继续调查其他可能的性能问题来源。

## 内存

除应用和 CPU 利用率外，内存利用率是性能下降的一个非常常见的原因。 在 CPU 部分，我们非常广泛地使用了`top`，虽然`top`也可以用来识别系统和进程的内存利用率，但在本节中，我们将使用其他命令。

### free -查看空闲内存和已用内存

正如在[第二章](02.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 2. Troubleshooting Commands and Sources of Useful Information")、*故障诊断命令和有用信息来源*中所讨论的，`free`命令只是打印系统当前的内存可用性和使用情况。

当不带标志执行时，`free`命令将以千字节为单位输出其值。 要获得以兆字节为单位的输出，只需使用-m(兆字节)标志执行`free`命令。

```sh
$ free -m
 total       used       free     shared    buffers     cached
Mem:     490         92        397          1          0         17
-/+ buffers/cache:         74        415
Swap:         1055         57        998

```

`free`命令显示了关于这个系统的相当多的信息，以及正在使用的内存数量。 为了更好地理解这个命令，让我们分解一下输出。

由于输出中有多行，我们将从输出头文件后的第一行开始:

```sh
Mem:         490        92       397         1         0         17

```

这一行中的第一个值是系统可用的**物理内存**的总量。 在我们的示例中，这是 490 MB。第二个值是系统使用的**内存数量**。 第三个值是系统上未使用的**内存量**; 注意，我使用的术语是“未使用的”而不是“可用的”。 第四个值是**共享内存**所使用的内存量; 除非您的系统经常使用共享内存，否则这个数字通常很低。

第五个值是用于**buffers**的内存数量。 Linux 经常试图通过将经常使用的磁盘信息放入物理内存来加速磁盘访问。 缓冲区内存通常是文件系统元数据。 缓存内存**正好是第六个值，它是频繁访问文件的内容。**

 **#### Linux 内存缓冲区和缓存

Linux 通常会尝试使用“未使用的”内存作为缓冲区和缓存。 这意味着为了提高效率，Linux 内核将经常访问的文件数据和文件系统元数据存储在未使用的内存中。 这允许系统利用本来不会用于增强通常比系统内存慢的磁盘访问的内存。

这就是为什么第三个值“未使用的”内存通常比预期的要少。

然而，当系统的未使用内存不足时，Linux 内核将根据需要释放缓冲区和缓存内存。 这意味着，即使在技术上，缓冲区和缓存所使用的内存也被使用了，但在技术上，当系统需要时，它是可用的。

这把我们带到自由输出的第二行。

```sh
-/+ buffers/cache:         74        415

```

第二行有两个值，第一行是**Used**列的一部分，第二行是**Free**或“unused”列的一部分。 在考虑了缓冲区和缓存内存的可用性之后，这些值是 Used 或 Free 内存值。

用更简单的术语解释，第二行上的 Used 值是第一行中使用的内存值减去缓冲区和缓存值的结果。 对于我们的示例，它是 92 MB(使用)减去 17 MB(缓存)。

第二行中的空闲值是第一行中添加缓冲区和缓存内存的空闲值的结果。 使用我们示例中的值，这将是 397 MB(空闲)加上 17 MB(缓存)。

#### 交换内存

`free`命令的输出中的第三行用于交换内存。

```sh
Swap:         1055         57        998

```

在这一行中，有三个列:可用的、使用的和免费的。 交换内存值是相当容易理解的。 可用交换值是系统可用的交换内存数量，使用值是当前分配的交换内存数量，而空闲值实质上是可用交换内存数量减去分配的交换内存数量。

在许多环境中，不赞成分配大量的交换空间，因为这通常表明系统已经耗尽了内存，并使用交换空间进行补偿。

#### 免费告诉我们什么关于我们的系统

如果我们再次查看 free 的输出，我们可以确定关于这个服务器的许多事情。

```sh
$ free -m
 total       used       free     shared    buffers     cached
Mem:       490        105        385          1          0         25
-/+ buffers/cache:         79        410
Swap:         1055         56        999

```

我们可以确定只有少量的内存(79 MB)在实际使用。 这意味着总的来说，系统应该有足够的内存供进程使用。

然而，还有一个有趣的事实，在第三行，它显示了**56**MB 的内存被写入了交换。 虽然系统上目前有足够的可用内存，但已经写入了 56 MB 用于交换。 这意味着在过去的某个时候，系统的内存可能很低，低到系统必须将内存页面从物理内存交换到交换内存。

### 检查 oomkill

当 Linux 系统耗尽物理内存时，它首先尝试重用分配给缓冲区和缓存的内存。 如果没有额外的内存可以从这些源中回收，那么内核将从物理内存中提取旧的内存页，并将它们写入交换内存。 一旦分配了物理内存和交换内存，内核将启动**内存耗尽杀手**(**oomkill**)进程。 `oomkill`进程被设计用来寻找占用大量内存的进程并杀死(停止)它们。

通常，在大多数环境中都不需要`oomkill`进程。 当被调用时，`oomkill`进程可以杀死许多不同类型的进程。 无论进程是系统的一部分还是在用户级别，`oomkill`都具有杀死它们的能力。

对于可能影响内存利用率的性能问题，检查`oomkill`进程最近是否被调用总是一个好主意。 确定`oomkill`最近是否运行的最简单的方法是查看系统控制台，因为该进程的启动被直接记录到系统控制台。 然而，在云和虚拟环境中，控制台可能不可用。

确定最近是否调用了`oomkill`的另一种好方法是搜索`/var/log/messages`日志文件。 我们可以通过执行`grep`命令并搜索字符串`Out of memory`来实现这一点。

```sh
# grep "Out of memory" /var/log/messages

```

对于我们的示例系统，最近没有`oomkill`调用。 如果我们的系统已经调用了`oomkill`进程，我们可以期望得到类似如下的消息:

```sh
# grep "Out of memory" /var/log/messages
Feb  7 19:38:45 localhost kernel: Out of memory: Kill process 3236 (python) score 838 or sacrifice child

```

在[第 11 章](11.html#26I9K2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 11. Recovering from Common Failures")，*从常见故障中恢复*中，我们将再次研究内存问题，深入了解`oomkill`及其工作原理。 对于本章，我们可以得出结论，系统并没有完全耗尽可用内存。

### ps -检查各个进程的内存利用率

到目前为止，这个系统上的内存使用量似乎很小，但是我们从 CPU 验证步骤中知道，运行`lookbusy`的进程是可疑的，可能会导致我们的性能问题。 既然我们怀疑`lookbusy`进程是一个问题，我们也应该看看这些进程使用了多少内存。 为此，我们可以再次使用带有`-o`标志的`ps`命令。

```sh
$ ps -eo user,pid,ppid,%mem,rss,vsize,comm | grep lookbusy
vagrant   3000  2980  0.0     4   7396 lookbusy
vagrant   3001  3000  0.0   296   7396 lookbusy
vagrant   3002  3000  0.0   220   7396 lookbusy
vagrant   5380  2980  0.0     8   7396 lookbusy
vagrant   5381  5380  0.0   268   7396 lookbusy
vagrant   5382  5380  0.0   268   7396 lookbusy
vagrant   5383  5380 40.7 204812 212200 lookbusy
vagrant   5531  2980  0.0    40   7396 lookbusy
vagrant   5532  5531  0.0   288   7396 lookbusy
vagrant   5533  5531  0.0   288   7396 lookbusy
vagrant   5534  5531 34.0 170880 222440 lookbusy

```

然而，这一次我们运行`ps`命令的方式略有不同，因此得到了不同的结果。 在执行`ps`命令的这个时间，我们使用`–e`(一切)标志来显示所有进程。 然后，结果通过管道传输到`grep`，以便将其过滤到仅匹配模式`lookbusy`的进程。

这是使用`ps`命令的一种非常常见的方式; 事实上，它甚至比在命令行上指定进程 ID 更常见。 除了使用`grep`之外，这个`ps`命令示例还引入了一些新的格式化选项。

*   **%mem**:这是进程正在使用的系统内存的百分比。
*   **rss**:这是进程的驻留站点大小，本质上意味着进程所使用的不可交换的内存数量。
*   **vsize**:这是虚拟内存大小; 它包含进程正在完全使用的内存量，而不管该内存是物理内存的一部分还是交换内存的一部分。
*   **comm**:这个选项类似于 cmd，只是它不显示命令行参数。

`ps`示例显示了有趣的信息，特别是以下几行:

```sh
vagrant   5383  5380 40.7 204812 212200 lookbusy
vagrant   5534  5531 34.0 170880 222440 lookbusy

```

似乎已经启动了几个其他的`lookbusy`进程，这些进程正在使用 40%和 34%的系统内存(通过使用`%mem`列)。 从 rss 栏中，我们可以看到这两个进程使用了总共 490 MB 的物理内存中的 374 MB。

似乎在我们开始调查之后，这些进程开始使用大量的内存。 最初，我们的空闲输出声明只有 70 MB 的内存在使用; 然而，这些过程似乎要利用更多。 我们可以通过再次自由奔跑来证实这一点。

```sh
$ free -m
 total       used       free     shared    buffers     cached
Mem:       490        453         37          0          0          3
-/+ buffers/cache:        449         41
Swap:         1055        310        745

```

实际上，我们的系统正在利用几乎所有的内存; 实际上，我们还使用了 310mb 的交换空间。

### vmstat -监控内存分配和交换

由于该系统似乎有波动的内存利用率，因此有一个非常有用的命令，它显示内存分配和回收，以及定期交换的页面数量。 这个命令称为`vmstat`。

```sh
$ vmstat -n 10 5
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
5  0 204608  31800      0   7676    8    6    12     6  101  131 44 1 55  0  0
1  0 192704  35816      0   2096 1887  130  4162   130 2080 2538 53 6 39  2  0
1  0 191340  32324      0   3632 1590   57  3340    57 2097 2533 54 5 41  0  0
4  0 191272  32260      0   5400  536    2  2150     2 1943 2366 53 4 43  0  0
3  0 191288  34140      0   4152  392    0   679     0 1896 2366 53 3 44  0  0

```

在上面的例子中,`vmstat`命令执行`-n`(一个头)旗其次是延迟秒(10)和报告生成的数量(5)。这些选项告诉`vmstat`为这个执行只输出一个标题行而不是一个新的标题行对于每一个报告,报告每 10 秒,运行 并将报告数量限制在 5 份以内。 如果忽略了对报告数量的限制，那么`vmstat`将简单地连续运行，直到使用*CTRL*+*C*停止。

起初，`vmstat`的输出可能有点让人不知所措，但如果我们分解输出，就会更容易理解。 `vmstat`的输出有六个输出类别，分别是 Procs、Memory、Swap、IO、System 和 CPU。 在本节中，我们将重点讨论其中的两个类别:内存和交换。

*   **内存**
    *   `swpd`:写入交换的内存数量
    *   `free`:未使用的内存数量
    *   `buff`:用作缓冲区的内存数量
    *   `cache`:用作缓存的内存数量
    *   `inact`:未激活内存数量
    *   `active`:活动内存数量
*   **Swap**
    *   `si`:从磁盘交换的内存数量
    *   `so`:交换到磁盘的内存数量

现在我们有了这些值的定义，让我们看看关于这个系统的内存使用，`vmstat`的输出告诉了我们什么。

```sh
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 5  0 204608  31800      0   7676    8    6    12     6  101  131 44  1 55  0  0
 1  0 192704  35816      0   2096 1887  130  4162   130 2080 2538 53  6 39  2  0

```

如果我们比较第一行和`vmstat's`输出的第二行，我们可以看到一个相当大的差异。 特别地，我们可以看到在第一个间隔中，缓存内存是`7676`，而在第二个间隔中，这个值是 2096。 我们还可以看到，第一行中的`si`或交换值是 8，而第二行是 1887。

产生这种差异的原因是`vmstat`的第一个报告总是自上次重启以来的统计摘要，而第二个报告是自上次报告以来的统计摘要。 随后的每个报告都是前一个报告的摘要，这意味着第三个报告将总结自第二次报告以来的统计数据。 `vmstat`的这种行为常常会引起新系统管理员和用户的混淆; 因此，它通常被认为是一种高级故障排除工具。

由于`vmstat`生成第一份报告的方法，通常的做法是将其丢弃，从第二份报告开始。 我们将遵循这一理念，具体地看一看第二和第三份报告。

```sh
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 5  0 204608  31800      0   7676    8    6    12     6  101  131 44  1 55  0  0
 1  0 192704  35816      0   2096 1887  130  4162   130 2080 2538 53  6 39  2  0
 1  0 191340  32324      0   3632 1590   57  3340    57 2097 2533 54  5 41  0  0

```

在第二和第三份报告中，我们可以看到一些有趣的数据。

首先要注意的是，从第一个报告的生成时间到第二个报告的生成时间，总共交换了 1887 页，交换了 130 页。 第二个报告还显示只有 35 MB 的内存是空闲的，其中 0 MB 内存在缓冲区中，2 MB 内存在缓存中。 根据 Linux 使用内存的方式，这意味着该系统上实际上只有 37mb 可用内存。

这种低数量的可用内存解释了为什么我们的系统要交换大量的页面。 从第三行我们可以看到，这个趋势还在继续，我们继续交换了相当多的页面，我们的可用内存已经减少到大约 35mb。

从这个`vmstat`的示例中，我们可以看到我们的系统现在正在耗尽物理内存。 因此，我们的系统从物理 RAM 中获取内存页并将其写入交换设备。

### 把它们放在一起

现在我们已经了解了对内存利用率进行故障排除所需的工具，让我们将它们放在一起来解决系统性能变慢的问题。

#### 查看系统的空闲内存利用率

提供系统内存利用率快照的第一个命令是`free`命令。 这个命令将告诉我们从哪里进一步查找内存使用问题。

```sh
$ free -m
 total       used       free     shared    buffers     cached
Mem:       490        293        196          0          0         18
-/+ buffers/cache:        275        215
Swap:         1055        183        872

```

从`free`的输出中，我们可以看到目前有 215 MB 可用内存。 我们可以通过第二行的`free`列看到这一点。 我们还可以看到，总的来说，这个系统有 183mb 的内存已经交换到我们的交换设备。

#### 观察 vmstat 发生了什么

由于系统在某个时刻已经交换了(或者更确切地说，分页)，我们可以使用`vmstat`命令来查看系统是否正在交换。

这一次在执行`vmstat`时，我们将省略报告值的数量，这将导致`vmstat`持续报告内存统计信息，类似于 top 命令的输出。

```sh
$ vmstat -n 10
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 4  0 188008 200320      0  19896   35    8    61     9  156    4 44 1 55  0  0
 4  0 188008 200312      0  19896    0    0     0     0 1361 1314 36 2 62  0  0
 2  0 188008 200312      0  19896    0    0     0     0 1430 1442 37 2 61  0  0
 0  0 188008 200312      0  19896    0    0     0     0 1431 1418 37 2 61  0  0
 0  0 188008 200280      0  19896    0    0     0     0 1414 1416 37 2 61  0  0
 2  0 188008 200280      0  19896    0    0     0     0 1456 1480 37 2 61  0  0

```

这个`vmstat`输出与前面的执行不同。 从这个输出中，我们可以看到，虽然有相当多的内存交换，但系统目前没有交换。 我们可以通过`si`(swap in)和`si`(swap out)列中的 0 值来确定。

事实上，在这个`vmstat`运行期间，内存利用率看起来很稳定。 每个`vmstat`报告之间的`free`内存值以及缓存和缓冲内存统计数据相当一致。

#### 使用 ps 查找占用内存最多的进程

我们的系统有 490 MB 的物理内存，`free`和`vmstat`都显示大约有 215 MB 可用内存。 这意味着超过一半的系统内存目前被利用; 在这种使用级别下，最好找出哪些进程正在使用系统的内存。 如果没有其他作用，这些数据将有助于显示系统的当前状态。

要确定使用最多内存的进程，可以使用`ps`命令以及 sort 和 tail 命令。

```sh
# ps -eo rss,vsize,user,pid,cmd | sort -nk 1 | tail -n 5
 1004 115452 root      5073 -bash
 1328 123356 root      5953 ps -eo rss,vsize,user,pid,cmd
 2504 525652 root       555 /usr/sbin/NetworkManager --no-daemon
 4124  50780 root         1 /usr/lib/systemd/systemd --switched-root --system --deserialize 23
204672 212200 vagrant  5383 lookbusy -m 200MB -c 10

```

上面的示例使用管道将`ps`的输出重定向到 sort 命令。 sort 命令对第一列(`-k 1`)执行数字(`-n`)排序。 这将产生对输出进行分类的效果，将具有最高`rss`尺寸的过程放在底部。 在`sort`命令之后，输出还通过管道传递到`tail`命令，当使用`-n`(number)标志加上一个数字进行指定时，输出将限制为只包含指定数量的结果。

### 提示

如果将命令与管道链接在一起的概念是新的，那么我强烈建议您实践这种方法，因为它对于日常的`sysadmin`任务以及故障排除期间非常有用。 我们将在本书中讨论这个概念并多次提供例子。

```sh
204672 212200 vagrant  5383 lookbusy -m 200MB -c 10

```

从`ps`的输出中，我们可以看到进程 5383 使用了大约 200 MB 的内存。 我们还可以看到，该进程是另一个`lookbusy`进程，它也是由流浪用户生成的。

由 free，`vmstat`，`ps`的输出，我们可以得出:

*   系统目前大约有 200 MB 可用内存
*   虽然系统目前没有交换，但它过去曾经交换过，并且根据我们之前从`vmstat`中看到的情况，我们知道它最近正在交换
*   我们发现进程`5383`占用了大约 200 MB 的内存
*   我们还可以看到进程`5383`是由`vagrant`用户启动的，并且正在运行`lookbusy`进程
*   使用`free`命令，我们可以看到这个系统有 490 MB 的物理内存

根据上述信息，似乎由`vagrant`用户执行的`lookbusy`进程不仅是 CPU 的可疑用户，而且也是内存的可疑用户。

## 磁盘

磁盘利用率是另一个常见的性能瓶颈。 通常，性能问题很少是由于磁盘空间的数量造成的。 虽然我见过由于大量文件或大尺寸文件而导致的性能问题，但通常情况下，磁盘性能受到磁盘写入和读取量的限制。 因此，虽然在诊断性能问题时知道文件系统是否已满很重要，但单单文件系统使用情况并不总是表明是否存在问题。

### iostat - CPU 和设备的输入/输出统计

`iostat`命令是诊断磁盘性能问题的基本命令，在使用情况和它提供的信息方面与 vmstat 类似。 与`vmstat`一样，`iostat`在执行时后面跟着两个数字，第一个是生成报告的延迟时间，第二个是生成报告的数量。

```sh
$ iostat -x 10 3
Linux 3.10.0-123.el7.x86_64 (blog.example.com)   02/08/2015 _x86_64_  (2 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 43.58    0.00    1.07    0.16    0.00   55.19

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda              12.63     3.88    8.47    3.47   418.80   347.40 128.27     0.39   32.82    0.80  110.93   0.47   0.56
dm-0              0.00     0.00   16.37    3.96    65.47    15.82 8.00     0.48   23.68    0.48  119.66   0.09   0.19
dm-1              0.00     0.00    4.73    3.21   353.28   331.71 172.51     0.39   48.99    1.07  119.61   0.54   0.43

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 20.22    0.00   20.33   22.14    0.00   37.32

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.10    13.67  764.97  808.68 71929.34 78534.73 191.23    62.32   39.75    0.74   76.65   0.42  65.91
dm-0              0.00     0.00    0.00    0.10     0.00     0.40 8.00     0.01   70.00    0.00   70.00  70.00   0.70
dm-1              0.00     0.00  765.27  769.76 71954.89 78713.17 196.31    64.65   42.25    0.74   83.51   0.43  66.46

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 18.23    0.00   15.56   29.26    0.00   36.95

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.10     7.10  697.50  440.10 74747.60 42641.75 206.38    74.13   66.98    0.64  172.13   0.58  66.50
dm-0              0.00     0.00    0.00    0.00     0.00     0.00 0.00     0.00    0.00    0.00    0.00   0.00   0.00
dm-1              0.00     0.00  697.40  405.00 74722.00 40888.65 209.74    75.80   70.63    0.66  191.11   0.61  67.24

```

在上面的示例中，提供了`–x`(扩展统计信息)标志来打印扩展统计信息。 扩展的统计信息非常有用，它提供了对于识别性能瓶颈至关重要的附加信息。

#### CPU 细节

`iostat`命令将显示 CPU 统计信息以及 I/O 统计信息。 这是另一个用来解决 CPU 利用率问题的命令。 当 CPU 利用率显示高 I/O 等待时间时，这特别有用。

```sh
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 20.22    0.00   20.33   22.14    0.00   37.32

```

以上信息与从`top`命令中显示的信息相同; 在 Linux 中，找到多个输出类似信息的命令并不少见。 由于这些细节已经在 CPU 故障排除一节中介绍过，所以我们将重点关注`iostat`命令的 I/O 统计部分。

#### 查看 I/O 统计信息

为了开始查看 I/O 统计数据，让我们从前两个报告开始。 我将 CPU 利用率包括在下面，以帮助说明每个报告的起始位置，因为它是每个统计报告的第一项。

```sh
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 43.58    0.00    1.07    0.16    0.00   55.19

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda              12.63     3.88    8.47    3.47   418.80   347.40 128.27     0.39   32.82    0.80  110.93   0.47   0.56
dm-0              0.00     0.00   16.37    3.96    65.47    15.82 8.00     0.48   23.68    0.48  119.66   0.09   0.19
dm-1              0.00     0.00    4.73    3.21   353.28   331.71 172.51     0.39   48.99    1.07  119.61   0.54   0.43

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 20.22    0.00   20.33   22.14    0.00   37.32

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.10    13.67  764.97  808.68 71929.34 78534.73 191.23    62.32   39.75    0.74   76.65   0.42  65.91
dm-0              0.00     0.00    0.00    0.10     0.00     0.40 8.00     0.01   70.00    0.00   70.00  70.00   0.70
dm-1              0.00     0.00  765.27  769.76 71954.89 78713.17 196.31    64.65   42.25    0.74   83.51   0.43  66.46

```

通过比较前两份报告，我们发现它们之间有很大的差异。 第一次报告中`sda`设备的`%util`值为`0.56`，第二次报告中为`65.91`。

产生这种差异的原因是，与`vmstat`的情况一样，`iostat`第一次执行的统计数据基于服务器最后一次重新启动的时间。 第二个报告是基于第一个报告之后的时间。 这意味着第二个报告的输出基于第一次和第二次报告生成之间的 10 秒。 这是在`vmstat`中看到的相同行为，也是收集性能统计数据的其他工具的常见行为。

与`vmstat`一样，我们将丢弃第一份报告，只看第二份报告。

```sh
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 20.22    0.00   20.33   22.14    0.00   37.32

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.10    13.67  764.97  808.68 71929.34 78534.73 191.23    62.32   39.75    0.74   76.65   0.42  65.91
dm-0              0.00     0.00    0.00    0.10     0.00     0.40 8.00     0.01   70.00    0.00   70.00  70.00   0.70
dm-1              0.00     0.00  765.27  769.76 71954.89 78713.17 196.31    64.65   42.25    0.74   83.51   0.43  66.46

```

从上面，我们可以确定关于这个系统的几个事情。 第一个也是最重要的是 CPU 行中的`%iowait`值。

```sh
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 20.22    0.00   20.33   22.14    0.00   37.32

```

在前面执行 top 命令时，等待 I/O 的时间百分比非常小; 然而，当运行`iostat`时，我们可以看到 cpu 实际上花了很多时间等待 I/O。 虽然 I/O 等待并不一定意味着等待磁盘，但该输出的其余部分似乎表明存在相当多的磁盘活动。

```sh
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.10    13.67  764.97  808.68 71929.34 78534.73 191.23    62.32   39.75    0.74   76.65   0.42  65.91
dm-0              0.00     0.00    0.00    0.10     0.00     0.40 8.00     0.01   70.00    0.00   70.00  70.00   0.70
dm-1              0.00     0.00  765.27  769.76 71954.89 78713.17 196.31    64.65   42.25    0.74   83.51   0.43  66.46

```

扩展的统计数据输出有许多列，为了使这个输出更容易理解，让我们分解这些列告诉我们的内容。

*   **rrqm/s**:每秒合并排队的读请求数
*   **wrqm/s**:每秒合并并排队的写请求数
*   **r/s**:每秒完成的读请求数
*   **w/s**:每秒完成的写请求数
*   **rkB/s**:每秒读操作数，单位为千字节
*   **wkB/s**:每秒写操作数，单位为千字节
*   **avgr-sz**:向设备发出请求的平均大小(扇区)
*   **avgque -sz**:发送到设备的请求的平均队列长度
*   **await**:请求等待被服务的平均时间(以毫秒为单位)
*   **r_await**:读请求等待服务的平均时间(以毫秒为单位)
*   **w_await**:写请求等待服务的平均时间(以毫秒为单位)
*   **svctm**:该字段无效，拟删除; 它不应该被信任或使用
*   **%util**:设备处理 I/O 请求时花费的 CPU 时间百分比。 一个设备最多只能被 100%的利用

对于我们的示例，我们将只关注`r/s`、`w/s`、`await`和`%util`值，因为这些值将在保持示例简单的同时告诉我们关于该系统的磁盘利用率的相当多信息。

在回顾了`iostat`输出之后，我们可以看到`sda`和`dm-1`器件都有最高的`%util`值，这意味着它们最接近于处于容量状态。

```sh
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.10    13.67  764.97  808.68 71929.34 78534.73 191.23    62.32   39.75    0.74   76.65   0.42  65.91
dm-1              0.00     0.00  765.27  769.76 71954.89 78713.17 196.31    64.65   42.25    0.74   83.51   0.43  66.46

```

从这个报告中，我们可以看到，`sda`设备平均每秒完成 764 次读取(`r/s`)和 808 次写入(`w/s`)。 我们还可以确定这些请求平均需要 39 毫秒(等待)才能完成。 虽然这些数字很有趣，但它们并不一定意味着系统处于异常状态。 因为我们不熟悉这个系统，所以我们不一定知道读写的级别是否超出了这个系统的预期。 但是，收集这些信息非常重要，因为这些统计信息是故障排除过程的数据收集阶段的重要数据。

从`iostat`中我们可以看到另一个有趣的数据，`sda`和`dm-1`设备的`%util`值都约为 66%。 这意味着在第一次报告生成和第二次报告生成之间的 10 秒内，66%的 CPU 时间花费在等待`sda`或`dm-1`设备上。

#### 标识设备

磁盘设备的使用率达到 66%通常被认为是很高的，虽然这是非常有用的信息，但它不能告诉我们是谁或什么在使用磁盘。 为了回答这些问题，我们需要弄清楚`sda`和`dm-1`到底是用来做什么的。

由于`iostat`命令输出的设备通常是磁盘设备，因此识别这些设备的第一步是运行`mount`命令。

```sh
$ mount
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime,seclabel)
devtmpfs on /dev type devtmpfs (rw,nosuid,seclabel,size=244828k,nr_inodes=61207,mode=755)
securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev,seclabel)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,seclabel,gid=5,mode=620,ptmxmode=000)
tmpfs on /run type tmpfs (rw,nosuid,nodev,seclabel,mode=755)
tmpfs on /sys/fs/cgroup type tmpfs (rw,nosuid,nodev,noexec,seclabel,mode=755)
configfs on /sys/kernel/config type configfs (rw,relatime)
/dev/mapper/root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,seclabel)
mqueue on /dev/mqueue type mqueue (rw,relatime,seclabel)
debugfs on /sys/kernel/debug type debugfs (rw,relatime)
/dev/sda1 on /boot type xfs (rw,relatime,seclabel,attr2,inode64,noquota)

```

当不带任何选项运行`mount`命令时，将显示当前挂载的所有文件系统。 `mount`输出中的第一列是已安装的设备。 在上面的输出中，我们可以看到`sda`设备实际上是一个磁盘设备，并且它有一个名为`sda1`的分区，该分区被挂载为`/boot`。

然而，我们没有看到的是`dm-1`设备。 由于该设备没有以其他方式列在`mount`命令的输出中，我们可以通过查看`/dev`文件夹来识别`dm-1`设备。

系统上的所有设备都以`/dev`文件夹结构中的文件形式呈现。 `dm-1`设备也不例外。

```sh
$ ls -la /dev/dm-1
brw-rw----. 1 root disk 253, 1 Feb  1 18:47 /dev/dm-1

```

虽然我们已经找到了`dm-1`设备的位置，但我们还没有确定它的用途。 然而，这款设备最引人注目的一点是它的名字`dm-1`。 当设备以`dm`开始时，这表明该设备是由设备映射器创建的逻辑设备。

设备映射器是一个 Linux 内核框架，它允许系统创建“映射”回物理设备的虚拟磁盘设备。 此功能用于许多特性，包括软件 raid、磁盘加密和逻辑卷。

设备映射器框架中的一个常见做法是为这些特性创建符号链接，这些符号链接回单个逻辑设备。 由于通过`ls`命令可以通过第一列的输出(`brw-rw----.`)中的“b”值看到`dm-1`是一个块设备，因此我们知道`dm-1`不是符号链接。 我们可以使用此信息和 find 命令来标识链接回`dm-1`块设备的任何符号链接。

```sh
# find -L /dev -samefile /dev/dm-1
/dev/dm-1
/dev/rhel/root
/dev/disk/by-uuid/beb5220d-5cab-4c43-85d7-8045f870ba7d
/dev/disk/by-id/dm-uuid-LVM-qj3iMeektIlL3Z0g4WMPMJRbzacnpS9IVOCzB60GSHCEgbRKYW9ZKXR5prUPEE1e
/dev/disk/by-id/dm-name-root
/dev/block/253:1
/dev/mapper/root

```

在前面的章节中，我们使用 find 命令来标识配置文件和日志文件。 在上面的示例中,我们使用`-L`(链接)国旗,紧随其后的是`/dev`路径和`--samefile`国旗告诉找到搜索`/dev`文件夹结构,搜索任何文件的符号链接文件夹来识别“相同的文件”`/dev/dm-1`。

`--samefile`标志标识具有相同`inode`编号的文件。 当`-L`标志包含在命令中时，输出包含符号链接，并且这个示例似乎已经返回了几个结果。 最突出的符号链接文件是`/dev/mapper/root`; 这个文件突出的原因是它也出现在 mount 命令的输出中。

```sh
/dev/mapper/root on / type xfs (rw,relatime,seclabel,attr2,inode64,noquota)

```

似乎`/dev/mapper/root`是一个逻辑卷。 Linux 中的逻辑卷本质上是存储虚拟化。 该功能允许您创建伪设备(作为设备映射器的一部分)，将其映射到一个或多个物理设备。

例如，可以使用四个不同的硬盘并将这些硬盘组合成一个逻辑卷。 然后可以将逻辑卷用作单个文件系统的磁盘。 甚至可以使用逻辑卷在稍后的时间添加另一个硬盘。

要确认`/dev/mapper/root`设备实际上是一个逻辑卷，可以执行`lvdisplay`命令，该命令用于显示系统上的逻辑卷。

```sh
# lvdisplay
 --- Logical volume ---
 LV Path                /dev/rhel/swap
 LV Name                swap
 VG Name                rhel
 LV UUID                y1ICUQ-l3uA-Mxfc-JupS-c6PN-7jvw-W8wMV6
 LV Write Access        read/write
 LV Creation host, time localhost, 2014-07-21 23:35:55 +0000
 LV Status              available
 # open                 2
 LV Size                1.03 GiB
 Current LE             264
 Segments               1
 Allocation             inherit
 Read ahead sectors     auto
 - currently set to     256
 Block device           253:0

 --- Logical volume ---
 LV Path                /dev/rhel/root
 LV Name                root
 VG Name                rhel
 LV UUID                VOCzB6-0GSH-CEgb-RKYW-9ZKX-R5pr-UPEE1e
 LV Write Access        read/write
 LV Creation host, time localhost, 2014-07-21 23:35:55 +0000
 LV Status              available
 # open                 1
 LV Size                38.48 GiB
 Current LE             9850
 Segments               1
 Allocation             inherit
 Read ahead sectors     auto
 - currently set to     256
 Block device           253:1

```

从`lvdisplay`的输出中，我们可以看到一个有趣的路径`/dev/rhel/root`，它也存在于`find`命令的输出中。 让我们看看使用`ls`命令的这个设备。

```sh
# ls -la /dev/rhel/root
lrwxrwxrwx. 1 root root 7 Aug  3 16:27 /dev/rhel/root -> ../dm-1

```

这里，我们可以看到，`/dev/rhel/root`是到`/dev/dm-1`的符号链接; 这证实了`/dev/rhel/root`与`/dev/dm-1`是相同的，并且这些实际上是逻辑卷设备，这意味着它们不是真正的物理设备。

要显示这些逻辑卷后面的物理设备，可以使用`pvdisplay`命令。

```sh
# pvdisplay
 --- Physical volume ---
 PV Name               /dev/sda2
 VG Name               rhel
 PV Size               39.51 GiB / not usable 3.00 MiB
 Allocatable           yes (but full)
 PE Size               4.00 MiB
 Total PE              10114
 Free PE               0
 Allocated PE          10114
 PV UUID               n5xoxm-kvyI-Z7rR-MMcH-1iJI-D68w-NODMaJ

```

我们可以看到从输出的`pvdisplay``dm-1`设备映射到`sda2`,这也解释了为什么磁盘利用率为`dm-1`和`sda`非常接近,任何活动`dm-1`是`sda`执行。

### 谁正在给这些设备写信?

现在我们已经找到了使用 I/O 的地方，我们需要找出谁在使用这个 I/O。 找出哪个进程向磁盘写入最多的最简单方法是使用`iotop`命令。 这个工具是一个相对较新的命令，现在默认包含在 Red Hat Enterprise Linux 7 中。 但是，这个命令在以前的 RHEL 版本中并不总是可用的。

在采用`iotop`之前，查找正在使用 I/O 的顶级进程的方法包括使用`ps`命令和查看`/proc`文件系统。

#### ps -使用 ps 来识别使用 I/O 的进程

在收集与 CPU 相关的数据时，我们在`ps`命令的输出中讨论了 state 字段。 我们没有涉及的是过程可以处于的各种状态。 下面的列表包含了`ps`命令将显示的七种可能的状态:

*   **Uninterruptible sleep**(`D`):进程在等待 I/O 时通常处于睡眠状态
*   **运行或可运行**(`R`):运行队列上的进程
*   **可中断睡眠**(`S`):进程等待事件完成，但不会阻塞 CPU 或 I/O
*   **Stopped**(`T`):被作业控制系统(如 jobs 命令)停止的进程
*   **Paging**(`P`):当前正在分页的进程; 然而，这与更新的内核不太相关
*   **Dead**(`X`):死了的进程，这应该不会被看到，因为在运行`ps`时死了的进程不会出现
*   **Defunct**(`Z`):僵尸进程被终止但处于不死状态

在调查 I/O 利用率时，重要的是要确定一个列在`D`**Uninterruptible Sleep**中的状态。 由于这些进程通常都在等待 I/O，它们是最有可能过度利用磁盘 I/O 的进程。

为此，我们将使用带有`–e`(所有内容)、`-l`(长格式)和`-f`(全格式)标志的`ps`命令。 我们还将再次使用管道将输出重定向到`grep`命令，并过滤输出，只显示具有`D`状态的进程。

```sh
# ps -elf | grep " D "
1 D root     13185     2  2  80   0 -     0 get_re 00:21 ? 00:01:32 [kworker/u4:1]
4 D root     15639 15638 30  80   0 -  4233 balanc 01:26 pts/2 00:00:02 bonnie++ -n 0 -u 0 -r 239 -s 478 -f -b -d /tmp

```

通过上面的输出，我们可以看到目前有两个进程处于不可中断的睡眠状态。 一个是`kworker`，是内核系统进程，另一个是`bonnie++`，是根用户启动的进程。 由于`kworker`进程是一个通用的内核进程，我们将首先关注`bonnie++`进程。

为了更好地理解这个过程，我们将再次运行`ps`命令，但这一次使用`--forest`选项。

```sh
# ps -elf –forest
4 S root      1007     1  0  80   0 - 20739 poll_s Feb07 ? 00:00:00 /usr/sbin/sshd -D
4 S root     11239  1007  0  80   0 - 32881 poll_s Feb08 ? 00:00:00  \_ sshd: vagrant [priv]
5 S vagrant  11242 11239  0  80   0 - 32881 poll_s Feb08 ? 00:00:02      \_ sshd: vagrant@pts/2
0 S vagrant  11243 11242  0  80   0 - 28838 wait   Feb08 pts/2 00:00:01          \_ -bash
4 S root     16052 11243  0  80   0 - 47343 poll_s 01:39 pts/2 00:00:00              \_ sudo bonnie++ -n 0 -u 0 -r 239 -s 478 -f -b -d /tmp
4 S root     16053 16052 32  80   0 - 96398 hrtime 01:39 pts/2 00:00:03                  \_ bonnie++ -n 0 -u 0 -r 239 -s 478 -f -b -d /tmp

```

通过查看上面的输出，我们可以看到`bonnie++`进程实际上是`16052`进程的一个子进程，而`16052`进程是`vagrant`用户的 bash shell`11243`的另一个子进程。

前面的`ps`命令显示了进程 id 为`16053`的`bonnie++`进程正在等待 I/O 任务。 然而，这并没有告诉我们这个进程使用了多少 I/O; 要确定这一点，我们可以在`/proc`文件系统中读取一个名为`io`的特殊文件。

```sh
# cat /proc/16053/io
rchar: 1002448848
wchar: 1002438751
syscr: 122383
syscw: 122375
read_bytes: 1002704896
write_bytes: 1002438656
cancelled_write_bytes: 0

```

每个正在运行的进程在`/proc`中都有一个与进程`id`同名的子文件夹; 对于我们的例子，这是`/proc/16053`。 这个文件夹由内核为每个正在运行的进程维护，在这些文件夹中存在许多文件，其中包含有关正在运行的进程的信息。

这些文件非常有用，它们实际上是`ps`命令信息的来源。 其中一个有用的文件名为`io`; `io`文件包含进程的读写次数统计信息。

从 cat 命令的输出中，我们可以看到这个进程已经读写了大约 1gb 的数据。 虽然这看起来很多，但可能需要很长一段时间。 为了了解这个进程正在向磁盘写入多少内容，我们可以再次读取这个文件以捕获差异。

```sh
# cat /proc/16053/io
cat: /proc/16053/io: No such file or directory

```

然而，似乎当我们第二次执行 cat 命令时，我们收到一个错误，即`io`文件不再存在。 如果我们再次运行`ps`命令并使用`grep`来搜索 bonnie++进程的输出，我们可以看到一个`bonnie++`进程正在运行; 然而，它是一个新工艺加上一个新工艺`ID`。

```sh
# ps -elf | grep bonnie
4 S root     17891 11243  0  80   0 - 47343 poll_s 02:34 pts/2 00:00:00 sudo bonnie++ -n 0 -u 0 -r 239 -s 478 -f -b -d /tmp
4 D root     17892 17891 33  80   0 -  4233 sleep_ 02:34 pts/2 00:00:02 bonnie++ -n 0 -u 0 -r 239 -s 478 -f -b -d /tmp

```

由于子进程`bonnie++`似乎是短暂的进程，通过读取`io`文件来跟踪 I/O 统计信息可能相当困难。

### iotop -一个类似于 top 的磁盘 i/o 命令

由于这些进程的启动和停止非常频繁，我们可以使用`iotop`命令来确定哪些进程使用 I/O 最多。

```sh
# iotop
Total DISK READ :     102.60 M/s | Total DISK WRITE :      26.96 M/s
Actual DISK READ:     102.60 M/s | Actual DISK WRITE:      42.04 M/s
 TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO> COMMAND
16395 be/4 root        0.00 B/s    0.00 B/s  0.00 % 45.59 % [kworker/u4:0]
18250 be/4 root      101.95 M/s   26.96 M/s  0.00 % 42.59 % bonnie++ -n 0 -u 0 -r 239 -s 478 -f -b -d /tmp

```

在前面来自`iotop`的输出中，我们可以看到一些有趣的 I/O 统计信息。 使用`iotop`，我们不仅可以看到系统范围的统计信息，如**总磁盘读取次数**每秒和**总磁盘写入次数**每秒，还可以看到单个进程的相当多的统计信息。

从每个进程的角度来看，我们可以看到`bonnie++`进程正以 101.96 MBps 的速率从磁盘读取数据，并以 26.96 MBps 的速率写入磁盘。

```sh
16395 be/4 root        0.00 B/s    0.00 B/s  0.00 % 45.59 % [kworker/u4:0]
18250 be/4 root      101.95 M/s   26.96 M/s  0.00 % 42.59 % bonnie++ -n 0 -u 0 -r 239 -s 478 -f -b -d /tmp

```

`iotop`命令与 top 命令非常相似，因为它将每隔几秒刷新报告的结果。 这具有“实时”显示 I/O 统计数据的效果。

### 提示

像`top`和`iotop`这样的命令很难以书本格式显示。 我强烈建议在拥有这些命令的系统上执行这些命令，以了解它们是如何工作的。

### 把它们放在一起

既然我们已经介绍了一些用于诊断磁盘性能和利用率的工具，那么在诊断报告的慢速时，让我们把它们放在一起。

#### 使用 iostat 来确定是否存在 I/O 带宽问题

我们将运行的第一个命令是`iostat`，因为这将首先为我们验证是否实际上存在问题。

```sh
# iostat -x 10 3
Linux 3.10.0-123.el7.x86_64 (blog.example.com)   02/09/2015 _x86_64_  (2 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 38.58    0.00    3.22    5.46    0.00   52.75

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda              10.86     4.25  122.46  118.15 11968.97 12065.60 199.78    13.27   55.18    0.67  111.67   0.51  12.21
dm-0              0.00     0.00   14.03    3.44    56.14    13.74 8.00     0.42   24.24    0.51  121.15   0.46   0.80
dm-1              0.00     0.00  119.32  112.35 11912.79 12051.98 206.89    13.52   58.33    0.68  119.55   0.52  12.16

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 7.96    0.00   14.60   29.31    0.00   48.12

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.70     0.80  804.49  776.85 79041.12 76999.20 197.35    64.26   41.41    0.54   83.73   0.42  66.38
dm-0              0.00     0.00    0.90    0.80     3.59     3.19 8.00     0.08   50.00    0.00  106.25  19.00   3.22
dm-1              0.00     0.00  804.29  726.35 79037.52 76893.81 203.75    64.68   43.03    0.53   90.08   0.44  66.75

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
 5.22    0.00   11.21   36.21    0.00   47.36

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               1.10     0.30  749.40  429.70 84589.20 43619.80 217.47    76.31   66.49    0.43  181.69   0.58  68.32
dm-0              0.00     0.00    1.30    0.10     5.20     0.40 8.00     0.00    2.21    1.00   18.00   1.43   0.20
dm-1              0.00     0.00  749.00  391.20 84558.40 41891.80 221.80    76.85   69.23    0.43  200.95   0.60  68.97

```

从`iostat`的输出可以得出以下结论:

*   该系统的 CPU 目前花费了相当多的时间等待 I/O，占 30%-40%
*   看起来，`dm-1`和`sda`设备是使用最多的设备
*   从`iostat`来看，这些设备的利用率为 68%，这个数字似乎相当高

根据这些数据点，我们可以确定存在一个潜在的 I/O 利用率问题，除非预期利用率为 68%。

#### 使用 iotop 来确定哪些进程正在消耗磁盘带宽

现在我们已经确定了相当多的 CPU 时间都花在了等待 I/O 上，现在我们应该关注哪些进程最频繁地使用磁盘。 为此，我们将使用`iotop`命令。

```sh
# iotop
Total DISK READ :     100.64 M/s | Total DISK WRITE :      23.91 M/s
Actual DISK READ:     100.67 M/s | Actual DISK WRITE:      38.04 M/s
 TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO> COMMAND
19358 be/4 root        0.00 B/s    0.00 B/s  0.00 % 40.38 % [kworker/u4:1]
20262 be/4 root      100.35 M/s   23.91 M/s  0.00 % 33.65 % bonnie++ -n 0 -u 0 -r 239 -s 478 -f -b -d /tmp
 363 be/4 root        0.00 B/s    0.00 B/s  0.00 %  2.51 % [xfsaild/dm-1]
 32 be/4 root        0.00 B/s    0.00 B/s  0.00 %  1.74 % [kswapd0]

```

从`iotop`命令中，我们可以看到正在运行`bonnie++`命令的进程`20262`具有较高的利用率和较大的磁盘读写值。

从`iotop`可以确定:

*   系统的总磁盘每秒读是 100.64 MBps
*   系统的总磁盘每秒写入是 23.91 MBps
*   运行`bonnie++`命令的进程`20262`正在读取 100.35 MBps，写入 23.91 MBps
*   比较总数，我们发现进程`20262`是磁盘读写的主要贡献者

综上所述，我们似乎需要识别更多关于过程`20262`的信息。

#### 使用 ps 来更多地了解进程

现在我们已经确定了使用大量 I/O 的进程，我们可以使用`ps`命令研究该进程的详细信息。 我们将再次使用带有`--forest`标志的`ps`命令来显示父进程和子进程的关系。

```sh
# ps -elf --forest
1007  0  80   0 - 32881 poll_s Feb08 ?        00:00:00  \_ sshd: vagrant [priv]
5 S vagrant  11242 11239  0  80   0 - 32881 poll_s Feb08 ? 00:00:05      \_ sshd: vagrant@pts/2
0 S vagrant  11243 11242  0  80   0 - 28838 wait   Feb08 pts/2 00:00:02          \_ -bash
4 S root     20753 11243  0  80   0 - 47343 poll_s 03:52 pts/2 00:00:00              \_ sudo bonnie++ -n 0 -u 0 -r 239 -s 478 -f -b -d /tmp
4 D root     20754 20753 52  80   0 -  4233 sleep_ 03:52 pts/2 00:00:01                  \_ bonnie++ -n 0 -u 0 -r 239 -s 478 -f -b -d /tmp

```

使用`ps`命令，我们可以确定以下内容:

*   与`iotop`鉴定的`bonnie++`过程`20262`缺失; 然而，还存在其他`bonnie++`进程
*   日志含义`vagrant`用户通过`sudo`命令启动了父`bonnie++`进程
*   `vagrant`用户与前面在 CPU 和内存部分讨论的观察结果中的用户相同

从上面的细节可以看出，流浪用户很可能是性能问题的罪魁祸首。

## 网络

解决性能问题的最后一个共同资源是网络。 有很多工具可以解决网络问题; 然而，这些命令中很少有专门针对网络性能的。 这些工具中的大多数设计用于深入的网络故障排除。

由于[第 5 章](05.html#UGI01-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 5. Network Troubleshooting")、*网络故障排除*专门用于故障排除网络问题，因此本节将特别关注性能。

### ifstat -查看接口统计信息

当涉及到网络时，大约有四个指标可以用来度量吞吐量。

*   **Received Packets**:接口接收到的报文数
*   **Sent Packets**:接口发送的报文数
*   **Received Data**:接口接收的数据量
*   **Sent Data**:接口发送的数据量

有许多命令可以提供这些指标，从`ifconfig`或`ip`到`netstat`。 专门输出这些度量的非常有用的实用程序是`ifstat`命令。

```sh
# ifstat
#21506.1804289383 sampling_interval=5 time_const=60
Interface   RX Pkts/Rate   TX Pkts/Rate   RX Data/Rate   TX Data/Rate
 RX Errs/Drop   TX Errs/Drop   RX Over/Rate   TX Coll/Rate
lo              47 0            47 0         4560 0          4560 0
 0 0             0 0            0 0             0 0
enp0s3       70579 1         50636 0      17797K 65        5520K 96
 0 0             0 0            0 0             0 0
enp0s8       23034 0            43 0       2951K 18          7035 0
 0 0             0 0            0 0             0 0

```

与`vmstat`或`iostat`非常相似，`ifstat`生成的第一个报告基于自服务器上次重新启动以来的统计数据。 这意味着上面的报告表明`enp0s3`接口自上次重启以来已经收到 70,579 个数据包。

当第二次执行`ifstat`时，结果将显示与第一次报告的差异非常大。 原因是第二次报告是基于第一次报告之后的时间。

```sh
# ifstat
#21506.1804289383 sampling_interval=5 time_const=60
Interface   RX Pkts/Rate    TX Pkts/Rate   RX Data/Rate  TX Data/Rate
 RX Errs/Drop    TX Errs/Drop   RX Over/Rate  TX Coll/Rate
lo                0 0             0 0             0 0             0 0
 0 0             0 0             0 0             0 0
enp0s3           23 0            18 0         1530 59         1780 80
 0 0             0 0             0 0             0 0
enp0s8            1 0             0 0           86 10             0 0
 0 0             0 0             0 0             0 0

```

在上面的例子中，我们可以看到我们的系统通过`enp0s3`接口接收了 23 个数据包(RX Pkts)，并传输了 18 个数据包(`TX Pkts`)。

通过`ifstat`命令，我们可以确定以下关于系统的信息:

*   目前的网络利用率相当小，不太可能对整个系统造成影响
*   前面显示的来自`vagrant`用户的进程不太可能占用大量的网络资源

根据通过`ifstat`看到的统计数据，该系统上的网络流量最小，不太可能导致所感知的慢速。

## 快速回顾一下我们已经确定的内容

在深入讨论之前，让我们回顾一下我们从迄今为止收集到的性能统计信息中了解到的内容:

### 注意事项

`vagrant`用户已经启动运行`bonnie++`和`lookbusy`应用的进程。

`lookbusy`应用似乎占用了整个系统 CPU 的 20%-30%。

该服务器有两个 CPU，而`lookbusy`似乎始终使用大约 60%的 CPU。

`lookbusy`应用似乎也一直使用大约 200 MB 的内存; 然而，在故障排除期间，我们确实看到这些进程使用了几乎所有的系统内存，导致系统交换。

当`vagrant`用户启动`bonnie++`进程时，系统正在经历一个高 I/O 等待时间。

在运行时，`bonnie++`进程占用了大约 60%-70%的磁盘吞吐量。

由`vagrant`用户执行的活动似乎对网络利用率几乎没有影响。

# 比较历史指标

看着所有的事实,我们学过这个系统到目前为止,似乎我们的下一个最佳行动将推荐联系`vagrant`用户确定是否`lookbusy`和`bonnie++`应用应该使用如此高的资源利用率。

虽然前面的观察结果显示了较高的资源利用率，但在此环境中可能会出现这种水平的利用率。 在开始联系用户之前，我们应该首先查看该服务器的历史性能指标。 在大多数环境中，都存在某种类型的服务器性能监视软件，如 Munin、Cacti 或许多云 SaaS 提供商中的一个，它们收集和存储系统统计信息。

如果您的环境利用了其中的一个服务，那么您可以使用收集到的性能数据将以前的性能统计数据与我们刚刚收集到的信息进行比较。 例如，如果在过去 30 天内，CPU 性能从未高于 10%，那么`lookbusy`进程可能在那个时候没有运行，这是有道理的。

即使您的环境没有使用这些工具之一，您仍然可以执行历史比较。 为此，我们将使用在大多数 Red Hat Enterprise Linux 系统上默认安装的工具; 这个工具叫做`sar`。

## sar - System 活动报告

在第二章、*故障诊断命令和有用信息来源*中，我们简要讨论了`sar`命令的用法，以查看历史性能统计数据。

当安装部署`sar`实用程序的`sysstat`包时，它将部署`/etc/cron.d/sysstat`文件。 在这个文件中有两个运行`sysstat`命令的作业，其唯一目的是收集系统性能统计数据和生成收集信息的报告。

```sh
$ cat /etc/cron.d/sysstat
# Run system activity accounting tool every 10 minutes
*/2 * * * * root /usr/lib64/sa/sa1 1 1
# 0 * * * * root /usr/lib64/sa/sa1 600 6 &
# Generate a daily summary of process accounting at 23:53
53 23 * * * root /usr/lib64/sa/sa2 -A

```

执行这些命令后，收集到的信息保存在“`/var/log/sa/`”文件夹中。

```sh
# ls -la /var/log/sa/
total 1280
drwxr-xr-x. 2 root root   4096 Feb  9 00:00 .
drwxr-xr-x. 9 root root   4096 Feb  9 03:17 ..
-rw-r--r--. 1 root root  68508 Feb  1 23:20 sa01
-rw-r--r--. 1 root root  40180 Feb  2 16:00 sa02
-rw-r--r--. 1 root root  28868 Feb  3 05:30 sa03
-rw-r--r--. 1 root root  91084 Feb  4 20:00 sa04
-rw-r--r--. 1 root root  57148 Feb  5 23:50 sa05
-rw-r--r--. 1 root root  34524 Feb  6 23:50 sa06
-rw-r--r--. 1 root root 105224 Feb  7 23:50 sa07
-rw-r--r--. 1 root root 235312 Feb  8 23:50 sa08
-rw-r--r--. 1 root root 105224 Feb  9 06:00 sa09
-rw-r--r--. 1 root root  56616 Jan 23 23:00 sa23
-rw-r--r--. 1 root root  56616 Jan 24 20:10 sa24
-rw-r--r--. 1 root root  24648 Jan 30 23:30 sa30
-rw-r--r--. 1 root root  11948 Jan 31 23:20 sa31
-rw-r--r--. 1 root root  44476 Feb  5 23:53 sar05
-rw-r--r--. 1 root root  27244 Feb  6 23:53 sar06
-rw-r--r--. 1 root root  81094 Feb  7 23:53 sar07
-rw-r--r--. 1 root root 180299 Feb  8 23:53 sar08

```

`sysstat`包生成的数据文件使用“`sa<two digit day>`”格式的文件名。 例如，在上面的输出中，我们可以看到“`sa24`”文件是在 1 月 24 日生成的。 我们也可以看到这个系统有 1 月 23 日到 2 月 9 日的文件。

`sar`命令允许我们读取这些捕获的性能指标。 本节将向您展示如何使用`sar`命令查看与前面通过`iostat`、`top`和`vmstat`等命令查看相同的统计信息。 然而，这一次，`sar`命令将提供最近和历史信息。

### CPU

要使用`sar`命令查看 CPU 统计信息，我们可以简单地使用`–u`(CPU 利用率)标志。

```sh
# sar -u
Linux 3.10.0-123.el7.x86_64 (blog.example.com)   02/09/2015   _x86_64_  (2 CPU)

12:00:01 AM     CPU     %user     %nice   %system   %iowait    %steal %idle
12:10:02 AM     all      7.42      0.00     13.46     37.51      0.00 41.61
12:20:01 AM     all      7.59      0.00     13.61     38.55      0.00 40.25
12:30:01 AM     all      7.44      0.00     13.46     38.50      0.00 40.60
12:40:02 AM     all      8.62      0.00     15.71     31.42      0.00 44.24
12:50:02 AM     all      8.77      0.00     16.13     29.66      0.00 45.44
01:00:01 AM     all      8.88      0.00     16.20     29.43      0.00 45.49
01:10:01 AM     all      7.46      0.00     13.64     37.29      0.00 41.61
01:20:02 AM     all      7.35      0.00     13.52     37.79      0.00 41.34
01:30:01 AM     all      7.40      0.00     13.36     38.60      0.00 40.64
01:40:01 AM     all      7.42      0.00     13.53     37.86      0.00 41.19
01:50:01 AM     all      7.44      0.00     13.58     38.38      0.00 40.60
04:20:02 AM     all      7.51      0.00     13.72     37.56      0.00 41.22
04:30:01 AM     all      7.34      0.00     13.36     38.56      0.00 40.74
04:40:02 AM     all      7.40      0.00     13.41     37.94      0.00 41.25
04:50:01 AM     all      7.45      0.00     13.81     37.73      0.00 41.01
05:00:02 AM     all      7.49      0.00     13.75     37.72      0.00 41.04
05:10:01 AM     all      7.43      0.00     13.30     39.28      0.00 39.99
05:20:02 AM     all      7.24      0.00     13.17     38.52      0.00 41.07
05:30:02 AM     all     13.47      0.00     11.10     31.12      0.00 44.30
05:40:01 AM     all     67.05      0.00      1.92      0.00      0.00 31.03
05:50:01 AM     all     68.32      0.00      1.85      0.00      0.00 29.82
06:00:01 AM     all     69.36      0.00      1.76      0.01      0.00 28.88
06:10:01 AM     all     70.53      0.00      1.71      0.01      0.00 27.76
Average:        all     14.43      0.00     12.36     33.14      0.00 40.07

```

如果我们查看上面的头信息，我们可以看到带有`-u`标志的`sar`命令与`iostat`和顶级 CPU 细节相匹配。

```sh
12:00:01 AM     CPU     %user     %nice   %system   %iowait    %steal %idle

```

从`sar -u`输出中，我们可以发现一个有趣的趋势:从 00:00 到 05:30，有一个固定的 CPU I/O 等待时间为 30%-40%。 但是，在 05:40 时，I/O 等待减少，但是用户级 CPU 利用率增加到 65%-70%。

虽然这两个度量没有特别指向任何进程，但它们确实显示了最近 I/O 等待时间减少了，而用户 CPU 时间增加了。

为了更好地了解历史统计数据，我们需要查看前一天的 CPU 利用率。 幸运的是，我们可以通过`–f`(filename)标志做到这一点。 `–f`标志允许我们为`sar`命令指定一个历史文件。 这将允许我们有选择地查看前一天的统计数据。

```sh
# sar -f /var/log/sa/sa07 -u
Linux 3.10.0-123.el7.x86_64 (blog.example.com)   02/07/2015 _x86_64_  (2 CPU)

12:00:01 AM     CPU     %user     %nice   %system   %iowait    %steal %idle
12:10:01 AM     all     24.63      0.00      0.71      0.00      0.00 74.66
12:20:01 AM     all     25.31      0.00      0.70      0.00      0.00 73.99
01:00:01 AM     all     27.59      0.00      0.68      0.00      0.00 71.73
01:10:01 AM     all     29.64      0.00      0.71      0.00      0.00 69.65
05:10:01 AM     all     44.09      0.00      0.63      0.00      0.00 55.28
05:20:01 AM     all     60.94      0.00      0.58      0.00      0.00 38.48
05:30:01 AM     all     62.32      0.00      0.56      0.00      0.00 37.12
05:40:01 AM     all     63.74      0.00      0.56      0.00      0.00 35.70
05:50:01 AM     all     65.08      0.00      0.56      0.00      0.00 34.35
0.00     76.07
Average:        all     37.98      0.00      0.65      0.00      0.00 61.38

```

在 2 月 7 日的报告中，我们可以看到 CPU 利用率与之前的故障排除期间发现的情况有很大的不同。 值得注意的是，在第 7 期的报告中，没有在 I/O 等待状态中花费 CPU 时间。

然而，我们看到用户 CPU 时间根据一天中的时间从 20%波动到 65%。 这可能表明预期会有更高的用户 CPU 时间利用率。

### 内存

要显示内存的统计信息，可以使用`–r`(内存)标志执行`sar`命令。

```sh
# sar -r
Linux 3.10.0-123.el7.x86_64 (blog.example.com)   02/09/2015 _x86_64_  (2 CPU)

12:00:01 AM kbmemfree kbmemused  %memused kbbuffers  kbcached kbcommit   %commit  kbactive   kbinact   kbdirty
12:10:02 AM     38228    463832     92.39         0    387152 446108     28.17    196156    201128         0
12:20:01 AM     38724    463336     92.29         0    378440 405128     25.59    194336    193216     73360
12:30:01 AM     38212    463848     92.39         0    377848 405128     25.59      9108    379348     58996
12:40:02 AM     37748    464312     92.48         0    387500 446108     28.17    196252    201684         0
12:50:02 AM     33028    469032     93.42         0    392240 446108     28.17    196872    205884         0
01:00:01 AM     34716    467344     93.09         0    380616 405128     25.59    195900    195676     69332
01:10:01 AM     31452    470608     93.74         0    384092 396660     25.05    199100    196928     74372
05:20:02 AM     38756    463304     92.28         0    387120 399996     25.26    197184    198456         4
05:30:02 AM    187652    314408     62.62         0     19988 617000     38.97    222900     22524         0
05:40:01 AM    186896    315164     62.77         0     20116 617064     38.97    223512     22300         0
05:50:01 AM    186824    315236     62.79         0     20148 617064     38.97    223788     22220         0
06:00:01 AM    182956    319104     63.56         0     24652 615888     38.90    226744     23288         0
06:10:01 AM    176992    325068     64.75         0     29232 615880     38.90    229356     26500         0
06:20:01 AM    176756    325304     64.79         0     29480 615884     38.90    229448     26588         0
06:30:01 AM    176636    325424     64.82         0     29616 615888     38.90    229516     26820         0
Average:        77860    424200     84.49         0    303730 450102     28.43    170545    182617     29888

```

同样，如果我们查看`sar`的内存报告的头文件，我们可以看到一些熟悉的值。

```sh
12:00:01 AM kbmemfree kbmemused  %memused kbbuffers  kbcached kbcommit   %commit  kbactive   kbinact   kbdirty

```

从这个报告中，我们可以从**kbmemused**列中看到，在 05:40 时，系统突然释放了 150 MB 的物理内存。 从`kbcached`列可以看出，这 150 MB 的内存被分配给了磁盘缓存。 这是基于这样一个事实:在 05:40，缓存内存从 196 MB 增加到 22 MB。

有趣的是，与的 CPU 利用率变化一致，也发生在 05:40。 如果希望查看历史内存利用率，还可以将`-f`(文件名)标志与`-r`(内存)标志一起使用。 然而，由于我们可以在 05:40 看到一个相当明显的趋势，所以我们现在将关注这个时间。

### 磁盘

要显示当前的磁盘统计信息，我们可以使用`–d`(块设备)标志。

```sh
# sar -d
Linux 3.10.0-123.el7.x86_64 (blog.example.com)   02/09/2015 _x86_64_  (2 CPU)

12:00:01 AM       DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util
12:10:02 AM    dev8-0   1442.64 150584.15 146120.49    205.67 82.17     56.98      0.51     74.17
12:10:02 AM  dev253-0      1.63     11.11      1.96      8.00 0.06     34.87     19.72      3.22
12:10:02 AM  dev253-1   1402.67 150572.19 146051.96    211.47 82.73     58.98      0.53     74.68
04:20:02 AM    dev8-0   1479.72 152799.09 150240.77    204.80 81.27     54.89      0.50     73.86
04:20:02 AM  dev253-0      1.74     10.98      2.96      8.00 0.06     31.81     14.60      2.54
04:20:02 AM  dev253-1   1438.57 152788.11 150298.01    210.69 81.84     56.83      0.52     74.38
05:30:02 AM  dev253-0      1.00      7.83      0.17      8.00 0.00      3.81      2.76      0.28
05:30:02 AM  dev253-1   1170.61 123647.27 122655.72    210.41 69.12     59.04      0.53     62.20
05:40:01 AM    dev8-0      0.08      1.00      0.34     16.10 0.00      1.88      1.00      0.01
05:40:01 AM  dev253-0      0.11      0.89      0.00      8.00 0.00      1.57      0.25      0.00
05:40:01 AM  dev253-1      0.05      0.11      0.34      8.97 0.00      2.77      1.17      0.01
05:50:01 AM    dev8-0      0.07      0.49      0.28     11.10 0.00      1.71      1.02      0.01
05:50:01 AM  dev253-0      0.06      0.49      0.00      8.00 0.00      2.54      0.46      0.00
05:50:01 AM  dev253-1      0.05      0.00      0.28      6.07 0.00      1.96      0.96      0.00

Average:          DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz avgqu-sz     await     svctm     %util
Average:       dev8-0   1215.88 125807.06 123583.62    205.11 66.86     55.01      0.50     60.82
Average:     dev253-0      2.13     12.48      4.53      8.00 0.10     44.92     17.18      3.65
Average:     dev253-1   1181.94 125794.56 123577.42    210.99 67.31     56.94      0.52     61.17

```

默认情况下，`sar`命令将打印设备名称为“`dev<major>-<minor>`”，这可能有点令人困惑。 如果添加了`-p`(持久名称)标志，则设备名称将使用持久名称，它与挂载命令中的设备相匹配。

```sh
# sar -d -p
Linux 3.10.0-123.el7.x86_64 (blog.example.com)   08/16/2015 _x86_64_  (4 CPU)

01:46:42 AM       DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util
01:48:01 AM       sda      0.37      0.00      3.50      9.55 0.00      1.86      0.48      0.02
01:48:01 AM rhel-swap      0.00      0.00      0.00      0.00 0.00      0.00      0.00      0.00
01:48:01 AM rhel-root      0.37      0.00      3.50      9.55 0.00      2.07      0.48      0.02

```

即使使用无法识别的格式的名称，我们也可以看到，在 05:40 之前，`dev253-1`似乎有相当多的活动，其中磁盘`tps`(每秒事务数)从 1170 减少到 0.11。 磁盘 I/O 利用率的大幅下降似乎表明今天在`05:40`发生了相当大的变化。

### 网络

要显示网络统计信息，我们需要执行带有`–n DEV`标志的`sar`命令。

```sh
# sar -n DEV
Linux 3.10.0-123.el7.x86_64 (blog.example.com)   02/09/2015 _x86_64_  (2 CPU)

12:00:01 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s rxcmp/s   txcmp/s  rxmcst/s
12:10:02 AM    enp0s3      1.51      1.18      0.10      0.12 0.00      0.00      0.00
12:10:02 AM    enp0s8      0.14      0.00      0.02      0.00 0.00      0.00      0.07
12:10:02 AM        lo      0.00      0.00      0.00      0.00 0.00      0.00      0.00
12:20:01 AM    enp0s3      0.85      0.85      0.05      0.08 0.00      0.00      0.00
12:20:01 AM    enp0s8      0.18      0.00      0.02      0.00 0.00      0.00      0.08
12:20:01 AM        lo      0.00      0.00      0.00      0.00 0.00      0.00      0.00
12:30:01 AM    enp0s3      1.45      1.16      0.10      0.11 0.00      0.00      0.00
12:30:01 AM    enp0s8      0.18      0.00      0.03      0.00 0.00      0.00      0.08
12:30:01 AM        lo      0.00      0.00      0.00      0.00 0.00      0.00      0.00
05:20:02 AM        lo      0.00      0.00      0.00      0.00 0.00      0.00      0.00
05:30:02 AM    enp0s3      1.23      1.02      0.08      0.11 0.00      0.00      0.00
05:30:02 AM    enp0s8      0.15      0.00      0.02      0.00 0.00      0.00      0.04
05:30:02 AM        lo      0.00      0.00      0.00      0.00 0.00      0.00      0.00
05:40:01 AM    enp0s3      0.79      0.78      0.05      0.14 0.00      0.00      0.00
05:40:01 AM    enp0s8      0.18      0.00      0.02      0.00 0.00      0.00      0.08
05:40:01 AM        lo      0.00      0.00      0.00      0.00 0.00      0.00      0.00
05:50:01 AM    enp0s3      0.76      0.75      0.05      0.13 0.00      0.00      0.00
05:50:01 AM    enp0s8      0.16      0.00      0.02      0.00 0.00      0.00      0.07
05:50:01 AM        lo      0.00      0.00      0.00      0.00 0.00      0.00      0.00
06:00:01 AM    enp0s3      0.67      0.60      0.04      0.10 0.00      0.00      0.00

```

在网络统计报告中，我们看到全天无变化。 这表明，总的来说，这台服务器从未出现过任何网络性能瓶颈。

## 通过比较历史统计来复习我们所学的内容

在查看带有`sar`的历史统计数据和使用`ps`、`iostat`、`vmstat`和`top`等命令的近期统计数据之后，我们可以得出以下关于“性能缓慢”的结论。

由于我们的一个同行要求我们调查这个问题，我们的结论将以电子邮件的形式回复这个同行。

嗨，鲍勃!

*我查看了一个用户说服务器“慢”的服务器。 似乎名为 vagrant 的用户一直在运行两个主程序的多个实例。 第一个是看起来很忙的应用，它似乎总是在使用大约 20%-40%的 CPU。 但是，至少在一个实例中，lookbusy 应用还使用了大量内存，耗尽了系统的物理内存，并迫使系统进行大量交换。 然而，这个过程并没有持续很长时间。*

*第二个程序是 bonnie++应用，它似乎利用了大量的磁盘 I/O 资源。 当流浪用户运行邦尼++应用时，它利用了大约 60%的 dm-1 和 sda 磁盘带宽，导致大约 30%的高 I/O 等待。 通常，该系统的 I/O 等待为 0%(通过 sar 确认)。*

*似乎游移用户正在运行的应用使用的资源超过了预期的水平，导致其他用户的性能下降。*

# 总结

在本章中，我们开始使用在[第二章](02.html#I3QM2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 2. Troubleshooting Commands and Sources of Useful Information")、*故障诊断命令和有用信息来源*中探讨的一些高级 Linux 命令，如`iostat`和`vmstat`。 在诊断一个模糊的性能问题时，我们还非常熟悉 Linux 中的一个基本实用程序`ps`命令。

在第 3 章,*故障排除一个 Web 应用*我们可以遵循整个故障诊断过程从数据收集到的试验和错误,在这一章,我们的行动主要是集中在数据收集和建立一个假设阶段。 发现自己只是排除问题而不执行纠正措施是很常见的。 有许多问题应该由系统的用户而不是系统管理员来解决，但是确定问题的根源仍然是管理员的角色。

在[第五章](05.html#UGI01-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 5. Network Troubleshooting")、*网络故障排除*中，我们将排除一些非常有趣的网络问题。 网络对任何系统都至关重要; 问题有时很简单，有时又很复杂。 在下一章中，我们将探讨网络以及如何使用`netstat`和`tcpdump`等工具来排除网络问题。*****